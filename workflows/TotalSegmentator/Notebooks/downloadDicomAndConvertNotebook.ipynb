{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/CloudSegmentator/blob/main/workflows/TotalSegmentator/Notebooks/downloadDicomAndConvertNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh8wdZYXnGby"
      },
      "source": [
        "# **This Notebook can download CT data from Imaging Data Commons and convert to NIfTI with dcm2niix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/Docs/images/download_convert.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DICOM files are downloaded from IDC and converted to NIFTI files with dcm2niix. Whenever a non CT series or if there are multiple NIFTI files generated for a series, such series are prohibited from continuing to Inference. A CSV file is created with a list of such series.\n",
        "\n",
        "Please cite:\n",
        "\n",
        "Li X, Morgan PS, Ashburner J, Smith J, Rorden C. (2016) The first step for neuroimaging data analysis: DICOM to NIfTI conversion. J Neurosci Methods. 264:47-56.\n",
        "\n",
        "Fedorov A, Longabaugh WJR, Pot D, Clunie DA, Pieper SD, Gibbs DL, Bridge C, Herrmann MD, Homeyer A, Lewis R, Aerts HJWL, Krishnaswamy D, Thiriveedhi VK, Ciausu C, Schacherer DP, Bontempi D, Pihl T, Wagner U, Farahani K, Kim E, Kikinis R. National Cancer Institute Imaging Data Commons: Toward Transparency, Reproducibility, and Scalability in Imaging Artificial Intelligence. Radiographics. 2023 Dec;43(12):e230180. doi: 10.1148/rg.230180. PMID: 37999984; PMCID: PMC10716669.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a_FPoOpH_I"
      },
      "source": [
        "## **Ways to utilize this notebook**\n",
        "\n",
        "\n",
        "*   **Colab**\n",
        "*   **DockerContainer/Terra/SB-CGC**\n",
        "\n",
        "\n",
        "#### **Colab**\n",
        "*  This notebook was initally developed and tested on Colab, and a working version is saved on github, however reproducibility may not be guaranteed as the run time environment changes with colab updates\n",
        "*  To run this notebook with Colab, Click 'Open In Colab' icon on top left\n",
        "*  In 'interactive' mode, a list of seriesInstanceUIDs are chosen by default but a user may modify them to their use case\n",
        "*  Run each cell to install the packages and to download the data from IDC, convert to NIfTI saved in lz4 compressed format\n",
        "\n",
        "#### **Docker**\n",
        "* This notebook is primarly developed to be used on Terra/SB-CGC platforms using Docker\n",
        "* Running this notebook in a docker container ensures reproduciblity, as we lock the run environment beginning from the base docker image to pip packages in the docker image\n",
        "* Docker images can be found @ https://hub.docker.com/repository/docker/imagingdatacommons/download_convert/tags\n",
        "* The link to dockerfile along with git commit hash used for building the docker image can be found in one of the layers called 'LABEL'\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/Docs/images/download_convert_docker.png\">\n",
        "\n",
        "* We use a python package called Papermill, that can run the notebook with out having to convert it to a python script. This allows us maintain one copy of code instead of two.\n",
        "* A sample papermill command is\n",
        "    <pre>\n",
        "    papermill downloadDicomAndConvertNotebook.ipynb outputdownloadDicomAndConvertNotebook.ipynb -y SeriesInstanceUIDs yamlListOfSeriesInstanceUIDs\n",
        "    </pre>\n",
        "    * refer to https://papermill.readthedocs.io/en/latest/usage-execute.html#note-about-using-yaml on how to pass the yaml list\n",
        "    \n",
        "    Instead of passing the yaml list as a string, a yaml file can also be passed, as we do for SB-CGC\n",
        "    [an example can be found here](!https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/Docs/sampleManifests/batch_1.yaml)\n",
        "    <pre>\n",
        "    papermill downloadDicomAndConvertNotebook.ipynb outputdownloadDicomAndConvertNotebook.ipynb -f path_to_yamlListOfSeriesInstanceUIDs.yaml\n",
        "    </pre>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVSZRlpTrXTe"
      },
      "source": [
        "### **Installing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT_MQVJ_NbZU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  #Install apt packages\n",
        "  !apt-get update \\\n",
        "    && apt-get install -y --no-install-recommends \\\n",
        "      dcm2niix\\\n",
        "      lz4\\\n",
        "      pigz\\\n",
        "      #plastimatch\\\n",
        "      wget\\\n",
        "      zip\\\n",
        "    && rm -rf /var/lib/apt/lists/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqEuXW3vr2Go"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in sys.modules:\n",
        "  !pip install idc-index==0.2.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMi94BlAQrb5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in sys.modules:\n",
        "  #install s5cmd\n",
        "  !wget \"https://github.com/peak/s5cmd/releases/download/v2.2.2/s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "    && tar -xvzf \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "    && rm \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "    && mv s5cmd /usr/local/bin/s5cmd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ETtcQleyLck"
      },
      "source": [
        "### **Importing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWLvDAwIQcg1"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from datetime import datetime\n",
        "from idc_index import index\n",
        "from pathlib import Path\n",
        "from time import sleep\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcQfK2XB9wl5"
      },
      "source": [
        "### **Current Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpjsFVP79z-S"
      },
      "outputs": [],
      "source": [
        "curr_dir   = Path().absolute()\n",
        "\n",
        "print(time.asctime(time.localtime()))\n",
        "print(\"\\nCurrent directory :{}\".format( curr_dir))\n",
        "print(\"Python version    :\", sys.version.split('\\n')[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WI3gyi59Lbu"
      },
      "source": [
        "### **Initialize IDC Client**\n",
        "\n",
        "we use idc-index pypi package to handle downloading data from IDC. \n",
        "In this notebook, we are using version 0.2.8 which contains the index from idc version 17\n",
        "\n",
        "Learn more about idc-index at https://github.com/ImagingDataCommons/idc-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOvR8fXF9LJ6"
      },
      "outputs": [],
      "source": [
        "idc_client=index.IDCClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u3YBlVdw0_i"
      },
      "source": [
        "### **For local testing**\n",
        "\n",
        "By default a list of seriesInstanceUIDs are chosen  here. However, you can modify them to your usecase.\n",
        "\n",
        "Below cell is also tagged as `parameters`, so that when running this notebook in non interactive mode on Terra or Seven Bridges Genomics- Cancer Genomics Cloud platforms, papermill will inject a cell to pass the yaml list of SeriesInstanceUIDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPptwqXbw0TD",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  SeriesInstanceUIDs=['1.3.6.1.4.1.14519.5.2.1.7009.9004.100143549999116733615345241533',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100148350742920339334061834697',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100241427395754063917290539621',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100266844261017577841946689119',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100554983367710060268444607770',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100609092457306482866656779396',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100766949446324115207263771273',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100807683971079396484581858934',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100898243941555041874482639283',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100962349324934756610763981593',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100993923831797845405183790593',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.101126369092366339550409924127']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNhldQv7tEj-"
      },
      "source": [
        "### **Defining Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx_U7rRjtRYk"
      },
      "outputs": [],
      "source": [
        "#Creating Directories\n",
        "try:\n",
        "  shutil.rmtree('dcm2niix')\n",
        "except OSError:\n",
        "  pass\n",
        "os.mkdir('dcm2niix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFG0KcZ_QPyG"
      },
      "outputs": [],
      "source": [
        "def download_dicom_data(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Downloads raw DICOM data\n",
        "\n",
        "    Args:\n",
        "    series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "    download_directory = f\"idc_data/{series_id}\"\n",
        "    # Attempt to remove the directory for the series if it exists\n",
        "    try:\n",
        "        shutil.rmtree(download_directory)\n",
        "    except OSError:\n",
        "        pass\n",
        "    print(f'\\n Downloading DICOM files from IDC Storage Buckets \\n')\n",
        "    idc_client.download_dicom_series(seriesInstanceUID= series_id, downloadDir=download_directory, quiet=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n80QUoJ1VaBd"
      },
      "outputs": [],
      "source": [
        "def is_series_CT(series_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Gets the image modality for the corresponding seriesInstanceUID from idc-index\n",
        "    Refer to this query for additional columns available in idc-index!\n",
        "\n",
        "    https://github.com/ImagingDataCommons/idc-index/blob/main/queries/idc_index.sql\n",
        "\n",
        "    Args:\n",
        "    series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be processed.\n",
        "    \"\"\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "    Modality\n",
        "    FROM index\n",
        "    WHERE SeriesInstanceUID = '{series_id}'\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        modality_df = idc_client.sql_query(query)\n",
        "        if not modality_df.empty:\n",
        "            modality = modality_df['Modality'][0]\n",
        "            if modality=='CT':\n",
        "              return True\n",
        "            else:\n",
        "              log_modality_errors(series_id)\n",
        "              return False\n",
        "        else:\n",
        "            log_modality_errors(series_id)\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        log_modality_errors(series_id)\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWrNiNOgVaBd"
      },
      "outputs": [],
      "source": [
        "def log_modality_errors(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Logs an error when the modality is not CT for a given series.\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "    \"\"\"\n",
        "    # Open the log file in append mode\n",
        "    with open(\"modality_error_file.txt\", \"a\") as f:\n",
        "        # Write the error message to the file\n",
        "        f.write(f\"Error: Modality is not CT for series {series_id}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0SPIZ5RTxDb"
      },
      "outputs": [],
      "source": [
        "def convert_dicom_to_nifti(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Converts a DICOM series to a NIfTI file.\n",
        "\n",
        "    Args:\n",
        "      series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "\n",
        "    # Attempt to remove the directory for the series if it exists\n",
        "    try:\n",
        "        shutil.rmtree(f\"dcm2niix/{series_id}\")\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    # Create a new directory for the series\n",
        "    os.mkdir(f\"dcm2niix/{series_id}\")\n",
        "\n",
        "    print(\"\\n Converting DICOM files to NIfTI \\n\")\n",
        "\n",
        "    # Run the appropriate converter command and capture the output\n",
        "\n",
        "    result = subprocess.run(\n",
        "        f\"dcm2niix -z y -f %j_%p_%t_%s -b n -m y -o dcm2niix/{series_id} idc_data/{series_id}\",\n",
        "        shell=True,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "    )\n",
        "    print(result.stdout)\n",
        "    print(\"\\n Conversion successful \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKATbtgwPjX9"
      },
      "outputs": [],
      "source": [
        "def download_and_process_series(series_id: str, runtime_stats: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Downloads and processes a DICOM series.\n",
        "\n",
        "  Args:\n",
        "    series_id: The identifier of the DICOM series to be processed.\n",
        "    runtime_stats: DataFrame to store runtime statistics.\n",
        "\n",
        "  Returns:\n",
        "    Updated DataFrame with runtime statistics.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a DataFrame to track the processing times.\n",
        "  log = pd.DataFrame({'SeriesInstanceUID': [series_id]})\n",
        "\n",
        "  print(f\"\\n Processing series: {series_id}\")\n",
        "\n",
        "  if is_series_CT(series_id):\n",
        "\n",
        "      # Start the timer for downloading the DICOM series.\n",
        "      start_time = time.time()\n",
        "      download_dicom_data(series_id)\n",
        "      download_time = time.time() - start_time\n",
        "\n",
        "      print(f'Downloaded in %g seconds'% download_time)\n",
        "\n",
        "      # Add the download time to the DataFrame.\n",
        "      log['download_time'] = download_time\n",
        "\n",
        "      # Start the timer for converting the DICOM series to NIfTI.\n",
        "      start_time = time.time()\n",
        "      convert_dicom_to_nifti(series_id)\n",
        "      convert_dicom_to_nifti_time = time.time() - start_time\n",
        "\n",
        "      # Add the conversion time to the DataFrame.\n",
        "      log['NiftiConverter_time'] = convert_dicom_to_nifti_time\n",
        "\n",
        "  else:\n",
        "     log['download_time'] = 0\n",
        "     log['NiftiConverter_time'] = 0\n",
        "\n",
        "  # Update the runtime statistics DataFrame.\n",
        "  runtime_stats = pd.concat([runtime_stats, log], ignore_index=True, axis=0)\n",
        "\n",
        "  return runtime_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAQcXTtAGUfZ"
      },
      "outputs": [],
      "source": [
        "class MemoryMonitor:\n",
        "    def __init__(self):\n",
        "        # Flag to control the measurement loop\n",
        "        self.keep_measuring = True\n",
        "        # Get the path of the working disk\n",
        "        self.working_disk_path = self.get_working_disk_path()\n",
        "\n",
        "    def get_working_disk_path(self):\n",
        "        # This code is specific to Terra/SB-CGC as multiple disks are mounted on the platforms\n",
        "\n",
        "        # Get all disk partitions\n",
        "        partitions = psutil.disk_partitions()\n",
        "        for partition in partitions:\n",
        "            # If root partition, return root path\n",
        "            if partition.mountpoint == '/':\n",
        "                return '/'\n",
        "            # If cromwell_root is in mountpoint, return cromwell_root path\n",
        "            elif '/cromwell_root' in partition.mountpoint:\n",
        "                return '/cromwell_root'\n",
        "        # Default to root directory if no specific path is found\n",
        "        return '/'\n",
        "\n",
        "    def measure_usage(self):\n",
        "        # Initialize lists to store measurements\n",
        "        cpu_usage = []\n",
        "        ram_usage_mb = []\n",
        "        disk_usage_all = []\n",
        "        time_stamps = []\n",
        "\n",
        "        # Record start time\n",
        "        start_time = time.time()\n",
        "\n",
        "        while self.keep_measuring:\n",
        "            # Measure CPU usage\n",
        "            cpu = psutil.cpu_percent()\n",
        "\n",
        "            # Measure RAM usage\n",
        "            ram = psutil.virtual_memory()\n",
        "\n",
        "            # Measure disk usage\n",
        "            disk_usage = psutil.disk_usage(self.working_disk_path)\n",
        "\n",
        "            # Calculate used and total disk space in GB\n",
        "            disk_used = disk_usage.used / 1000 / 1000 / 1000\n",
        "            disk_total = disk_usage.total / 1000 / 1000 / 1000\n",
        "\n",
        "            # Calculate total and used RAM in MB\n",
        "            ram_total_mb = ram.total / 1000 / 1000\n",
        "            ram_mb = (ram.total - ram.available) / 1000 / 1000\n",
        "\n",
        "            # Append measurements to lists\n",
        "            cpu_usage.append(cpu)\n",
        "            ram_usage_mb.append(ram_mb)\n",
        "            disk_usage_all.append(disk_used)\n",
        "\n",
        "            # Record timestamp relative to start time\n",
        "            time_stamps.append(time.time() - start_time)\n",
        "\n",
        "            # Wait for a second before next measurement\n",
        "            sleep(1)\n",
        "\n",
        "        # Return all measurements and totals\n",
        "        return cpu_usage, ram_usage_mb, time_stamps, ram_total_mb, disk_usage_all, disk_total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYcb5F-2sFSO"
      },
      "source": [
        "### **Downloading and Converting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTtOJ7CtQYkX"
      },
      "outputs": [],
      "source": [
        "# Initialize a DataFrame to store runtime statistics\n",
        "runtime_stats = pd.DataFrame(columns=['SeriesInstanceUID','download_time',\n",
        "                                      'NiftiConverter_time', 'cpu_usage','ram_usage_mb', 'ram_total_mb', 'disk_usage_all', 'disk_total'\n",
        "                                      ])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Loop over all series IDs\n",
        "    for series_id in SeriesInstanceUIDs:\n",
        "        # Create a ThreadPoolExecutor\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            # Initialize a MemoryMonitor instance\n",
        "            monitor = MemoryMonitor()\n",
        "            # Start a new thread to measure memory usage\n",
        "            mem_thread = executor.submit(monitor.measure_usage)\n",
        "            try:\n",
        "                # Start a new thread to download and process the series\n",
        "                proc_thread = executor.submit(download_and_process_series, series_id, runtime_stats)\n",
        "                # Wait for the processing thread to finish\n",
        "                runtime_stats = proc_thread.result()\n",
        "            finally:\n",
        "                # Stop the memory monitor thread\n",
        "                monitor.keep_measuring = False\n",
        "                # Get the results from the memory monitor thread\n",
        "                cpu_usage, ram_usage_mb, time_stamps, ram_total_mb, disk_usage_all, disk_total= mem_thread.result()\n",
        "\n",
        "                # Update the runtime statistics DataFrame with the results\n",
        "                cpu_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[cpu_idx, runtime_stats.columns.get_loc('cpu_usage')] = [[cpu_usage]]\n",
        "\n",
        "                ram_usage_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_usage_mb_idx, runtime_stats.columns.get_loc('ram_usage_mb')] = [[ram_usage_mb]]\n",
        "\n",
        "                ram_total_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_total_mb_idx, runtime_stats.columns.get_loc('ram_total_mb')] = [[ram_total_mb]]\n",
        "\n",
        "                disk_usage_gb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[disk_usage_gb_idx, runtime_stats.columns.get_loc('disk_usage_all')] = [[disk_usage_all]]\n",
        "\n",
        "                # Update total disk space for all rows (assuming it's the same for all series)\n",
        "                runtime_stats['disk_total']=disk_total\n",
        "\n",
        "                # Plot CPU usage, memory usage and disk usage over time\n",
        "                fig, ((ax1,ax2, ax3)) = plt.subplots(1,3, figsize=(12, 4))\n",
        "\n",
        "                ax1.plot(time_stamps, cpu_usage)\n",
        "                ax1.set_ylim(0, 100)\n",
        "                ax1.set_xlabel('Time (s)')\n",
        "                ax1.set_ylabel('CPU usage (%)')\n",
        "\n",
        "                ax2.plot(time_stamps, ram_usage_mb)\n",
        "                ax2.set_ylim(0, ram_total_mb)\n",
        "                ax2.set_xlabel('Time (s)')\n",
        "                ax2.set_ylabel('Memory usage (MB)')\n",
        "\n",
        "                ax3.plot(time_stamps, disk_usage_all)\n",
        "                ax3.set_ylim(0, disk_total)\n",
        "                ax3.set_xlabel('Time (s)')\n",
        "                ax3.set_ylabel('Disk usage (GB)')\n",
        "\n",
        "                plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su4z4ettsTaO"
      },
      "source": [
        "### **Monitoring for dcm2niix Errors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tba4tK6pvja9"
      },
      "outputs": [],
      "source": [
        "def check_dcm2niix_errors(path: str) -> None:\n",
        "    \"\"\"\n",
        "    Check for errors in the conversion of DICOM to NIfTI files.\n",
        "\n",
        "    Args:\n",
        "    path: The path to the directory containing the series directories.\n",
        "    \"\"\"\n",
        "    # Loop over all series directories in the path\n",
        "    for series_id in os.listdir(path):\n",
        "        series_id_path = os.path.join(path, series_id)\n",
        "\n",
        "        # Check if the path is a directory\n",
        "        if os.path.isdir(series_id_path):\n",
        "            # Count the number of files in the directory\n",
        "            num_files = len([f for f in os.listdir(series_id_path) if os.path.isfile(os.path.join(series_id_path, f))])\n",
        "\n",
        "            # If no files or more than one file found, log an error and remove the directory\n",
        "            if num_files == 0 or num_files > 1:\n",
        "                print(f'{\"No\" if num_files == 0 else \"More than one\"} NIfTI file{\"s\" if num_files > 1 else \"\"} found for {series_id}')\n",
        "\n",
        "                with open('dcm2niix_errors.csv', 'a') as csvfile:\n",
        "                    writer = csv.writer(csvfile)\n",
        "                    writer.writerow([series_id])\n",
        "\n",
        "                shutil.rmtree(os.path.join('dcm2niix', series_id))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP7_k4msvl2E"
      },
      "outputs": [],
      "source": [
        "check_dcm2niix_errors(f'dcm2niix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-EDu3IFsxdq"
      },
      "source": [
        "### **Compressing Output Files**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q3J_d00v0gz"
      },
      "outputs": [],
      "source": [
        "# Attempt to remove the archive file if it exists\n",
        "try:\n",
        "  os.remove('downloadDicomAndConvertNiftiFiles.tar.lz4')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "# Record the start time of the archiving process\n",
        "start_time = time.time()\n",
        "\n",
        "# Create a tar archive of the converterType directory, compress it with lz4, and save it as downloadDicomAndConvertNiftiFiles.tar.lz4\n",
        "!tar cvf - -C {curr_dir} dcm2niix | lz4 > downloadDicomAndConvertNiftiFiles.tar.lz4\n",
        "\n",
        "# Calculate and record the time taken for the archiving process\n",
        "archiving_time = time.time() - start_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVsFcPebsoro"
      },
      "source": [
        "### **Utilization Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9n17x4vhDQp"
      },
      "outputs": [],
      "source": [
        "# Save the runtime statistics DataFrame to a CSV file\n",
        "runtime_stats.to_csv('runtime.csv')\n",
        "\n",
        "# Add the csv_read_time and archiving_time to the DataFrame as new columns\n",
        "runtime_stats['archiving_time'] = archiving_time\n",
        "\n",
        "# Attempt to remove the lz4 file if it exists\n",
        "try:\n",
        "  os.remove('downloadDicomAndConvertUsageMetrics.lz4')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "# Compress the runtime.csv file using lz4 and save it as downloadDicomAndConvertUsageMetrics.lz4\n",
        "!lz4 {curr_dir}/runtime.csv downloadDicomAndConvertUsageMetrics.lz4\n",
        "\n",
        "# Print the runtime statistics DataFrame\n",
        "runtime_stats\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
