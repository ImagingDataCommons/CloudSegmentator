{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/CloudSegmentator/blob/main/workflows/TotalSegmentator/Notebooks/endToEndTotalSegmentatorNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHS1DmzV54av"
      },
      "source": [
        "# **This Notebook can download CT data from Imaging Data Commons, perform Inference with TotalSegmentator, calculate the radiomics features and produces DICOM SEG and DICOM Structured Reports**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBAlZ8yhUOIa"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/Docs/images/endtoend.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBuvJLVl6C4g"
      },
      "source": [
        "Please cite:\n",
        "\n",
        "Jakob Wasserthal, Manfred Meyer, Hanns-Christian Breit, Joshy Cyriac, Shan Yang, & Martin Segeroth. (2022). TotalSegmentator: robust segmentation of 104 anatomical structures in CT images. https://doi.org/10.48550/arXiv.2208.05868\n",
        "\n",
        "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat Methods 18, 203â€“211 (2021). https://doi.org/10.1038/s41592-020-01008-z\n",
        "\n",
        "Herz C, Fillion-Robin JC, Onken M, Riesmeier J, Lasso A, Pinter C, Fichtinger G, Pieper S, Clunie D, Kikinis R, Fedorov A. dcmqi: An Open Source Library for Standardized Communication of Quantitative Image Analysis Results Using DICOM. Cancer Res. 2017 Nov 1;77(21):e87-e90. doi: 10.1158/0008-5472.CAN-17-0336. PMID: 29092948; PMCID: PMC5675033.\n",
        "\n",
        "Li X, Morgan PS, Ashburner J, Smith J, Rorden C. (2016) The first step for neuroimaging data analysis: DICOM to NIfTI conversion. J Neurosci Methods. 264:47-56.\n",
        "\n",
        "Fedorov A, Longabaugh WJR, Pot D, Clunie DA, Pieper SD, Gibbs DL, Bridge C, Herrmann MD, Homeyer A, Lewis R, Aerts HJWL, Krishnaswamy D, Thiriveedhi VK, Ciausu C, Schacherer DP, Bontempi D, Pihl T, Wagner U, Farahani K, Kim E, Kikinis R. National Cancer Institute Imaging Data Commons: Toward Transparency, Reproducibility, and Scalability in Imaging Artificial Intelligence. Radiographics. 2023 Dec;43(12):e230180. doi: 10.1148/rg.230180. PMID: 37999984; PMCID: PMC10716669."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcw0nbD82q31"
      },
      "source": [
        "## **Ways to utilize this notebook**\n",
        "\n",
        "\n",
        "*   **Colab**\n",
        "*   **DockerContainer/Terra/SB-CGC**\n",
        "\n",
        "\n",
        "#### **Colab/Jupyter Notebook/Lab**\n",
        "*  This notebook was initally developed and tested on Colab, and a working version is saved on github\n",
        "*  To run this notebook with Colab, Click 'Open In Colab' icon on top left\n",
        "*  In 'interactive' mode, a list of seriesInstanceUIDs are chosen by default but a user may modify them to their use case\n",
        "*  Run each cell to install the packages and to download the data from IDC, convert to NIfTI saved in lz4 compressed format\n",
        "\n",
        "#### **Docker**\n",
        "* This notebook is saved by default in a way that's amenable to be used on Terra/SB-CGC platforms using Docker\n",
        "* Running this notebook in a docker container ensures reproduciblity, as we lock the run environment beginning from the base docker image to pip packages in the docker image\n",
        "* Docker images can be found @ https://hub.docker.com/repository/docker/imagingdatacommons/download_convert_inference_totalseg/tags\n",
        "* The link to dockerfile along with git commit hash used for building the docker image can be found in one of the layers called 'LABEL'\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/Docs/images/endtoend_docker.png\">\n",
        "\n",
        "* We use a python package called Papermill, that can run the notebook with out having to convert it to python script. This allows us maintain one copy of code instead of two.\n",
        "* A sample papermill command is\n",
        "    <pre>\n",
        "    papermill endToEndTotalSegmentatorNotebook.ipynb outputendToEndTotalSegmentatorNotebook.ipynb -y SeriesInstanceUIDs yamlListOfSeriesInstanceUIDs\n",
        "    </pre>\n",
        "    * refer to https://papermill.readthedocs.io/en/latest/usage-execute.html#note-about-using-yaml on how to pass the yaml list\n",
        "\n",
        "    Instead of passing the yaml list as a string, a yaml file can also be passed, as we do for SB-CGC\n",
        "    [an example can be found here](!https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/Docs/sampleManifests/batch_1.yaml)\n",
        "    <pre>\n",
        "    papermill endToEndTotalSegmentatorNotebook.ipynb outputendToEndTotalSegmentatorNotebook.ipynb -f path_to_yamlListOfSeriesInstanceUIDs.yaml\n",
        "    </pre>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfrNp2OmC942"
      },
      "source": [
        "### **Installing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOhW6CUz1qIm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !sudo apt-get update && apt-get install -y --no-install-recommends\\\n",
        "        build-essential\\\n",
        "        ffmpeg\\\n",
        "        lz4\\\n",
        "        pigz\\\n",
        "        python3-dev\\\n",
        "        python3-pip\\\n",
        "        unzip\\\n",
        "        wget\\\n",
        "        xvfb\\\n",
        "        zip\\\n",
        "        && rm -rf /var/lib/apt/lists/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrdPt-v_mr9U"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in sys.modules:\n",
        "    !sudo pip install --no-cache-dir \\\n",
        "        idc_index==0.2.8\\\n",
        "        ipykernel==6.22.0\\\n",
        "        ipython==8.12.0\\\n",
        "        ipywidgets==8.0.6 \\\n",
        "        jupyter==1.0.0\\\n",
        "        matplotlib==3.7.1\\\n",
        "        nibabel==5.1.0\\\n",
        "        nvidia-ml-py3==7.352.0\\\n",
        "        papermill==2.4.0\\\n",
        "        p_tqdm==1.4.0\\\n",
        "        pydicom==2.3.1\\\n",
        "        \"scipy>=1.8,<1.9.2; python_version <= '3.9'\"\\\n",
        "        scikit-learn==1.2.2\\\n",
        "        scikit-image==0.20.0\\\n",
        "        TotalSegmentator==1.5.6\\\n",
        "        tqdm==4.65.0\\\n",
        "    && pip install --no-cache-dir\\\n",
        "        pyradiomics==3.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxH7dpgiDsUN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in sys.modules:\n",
        "    dcmqi_release_url = \"https://github.com/QIICR/dcmqi/releases/download/v1.3.0/dcmqi-1.3.0-linux.tar.gz\"\n",
        "    dcmqi_download_path = f\"dcmqi-1.3.0-linux.tar.gz\"\n",
        "    dcmqi_path = f\"dcmqi-1.3.0-linux\"\n",
        "    !wget -O $dcmqi_download_path $dcmqi_release_url\\\n",
        "    && tar -xvf $dcmqi_download_path\\\n",
        "    && mv $dcmqi_path/bin/* /bin\\\n",
        "    && rm -r $dcmqi_download_path $dcmqi_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRDMnjsF5_JS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in sys.modules:\n",
        "    !wget \"https://github.com/peak/s5cmd/releases/download/v2.2.2/s5cmd_2.2.2_Linux-64bit.tar.gz\"\n",
        "    !tar -xvzf \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "    && rm \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "    && mv s5cmd /usr/local/bin/s5cmd\\\n",
        "    && rm CHANGELOG.md LICENSE README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUvUSnwyuYAV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in sys.modules:\n",
        "    !wget \"https://github.com/rordenlab/dcm2niix/releases/download/v1.0.20230411/dcm2niix_lnx.zip\" \\\n",
        "    && unzip \"dcm2niix_lnx.zip\" \\\n",
        "    && rm \"dcm2niix_lnx.zip\" \\\n",
        "    && mv dcm2niix /usr/local/bin/dcm2niix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8OTxhaXmODa"
      },
      "source": [
        "### **Importing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoKIOAToFu2p"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "from idc_index import index\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import nvidia_smi\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import pydicom\n",
        "from pydicom.filereader import dcmread\n",
        "from pydicom.sr.codedict import codes\n",
        "from pydicom.uid import generate_uid\n",
        "import radiomics\n",
        "from radiomics import featureextractor, generalinfo\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import SimpleITK as sitk\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "from time import sleep, asctime, localtime\n",
        "from tqdm import tqdm\n",
        "from tqdm.contrib.concurrent import process_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JYrV3NQQS7F"
      },
      "source": [
        "### **Current Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-6oQeNoQUqb"
      },
      "outputs": [],
      "source": [
        "curr_dir   = Path().absolute()\n",
        "print(time.asctime(time.localtime()))\n",
        "print(\"\\nCurrent directory :{}\".format( curr_dir))\n",
        "print(\"Python version    :\", sys.version.split('\\n')[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSrRXal7UOIo"
      },
      "source": [
        "### **Initialize IDC Client**\n",
        "\n",
        "we use idc-client pypi package to handle downloading data from IDC.\n",
        "In this notebook, we are using version 0.2.8 which contains the index from idc version 17\n",
        "\n",
        "Learn more about idc-index at ttps://github.com/ImagingDataCommons/idc-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSVqKKmkUOIo"
      },
      "outputs": [],
      "source": [
        "idc_client=index.IDCClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "masmaHkeC1Al"
      },
      "source": [
        "### **Local testing**\n",
        "By default, in interactive mode, a list of seriesInstanceUIDs are chosen  here. However, you can modify them to your usecase.\n",
        "\n",
        "Below cell is also tagged as `parameters`, so that when running this notebook in non interactive mode on Terra or Seven Bridges Genomics- Cancer Genomics Cloud platforms, papermill will inject a cell to pass the yaml list of SeriesInstanceUIDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smIAQvIiaPTO",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  SeriesInstanceUIDs=[\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.2932.1975.256749960592074016578641330855'\n",
        "                      ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2J92QT-uGYc"
      },
      "source": [
        "### **Downloading Configs**\n",
        "- Config for DICOM_SEG conversion\n",
        "- Label maps from TotalSegmentator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM6p-iVxUjIY"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.remove(f'dicomseg_metadata_whole_slicerAsRef.json')\n",
        "  os.remove(f'map_to_binary.py')\n",
        "except OSError:\n",
        "  pass\n",
        "!wget -q https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/resources/dicomseg_metadata_whole_slicerAsRef.json\n",
        "!wget -q https://raw.githubusercontent.com/wasserth/TotalSegmentator/v2.0.5/totalsegmentator/map_to_binary.py\n",
        "import map_to_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E0y_02qWRU6"
      },
      "outputs": [],
      "source": [
        "totalsegmentator_segments_code_mapping_df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/wasserth/TotalSegmentator/1691bb8cd27a9ab78c2da3acef4dddf677c7dd24/resources/totalsegmentator_snomed_mapping.csv\",\n",
        "    dtype={\"SegmentedPropertyTypeModifierCodeSequence.CodeValue\": str},\n",
        ")\n",
        "totalsegmentator_radiomics_features_code_mapping_df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/resources/radiomicsFeaturesMaps.csv\",\n",
        "    index_col=[0]\n",
        ")\n",
        "totalsegmentator_radiomics_features_code_mapping_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU99FJ8_SNub"
      },
      "source": [
        "### **Prepare Intial Directories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByNK-uLo5tfh"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  shutil.rmtree('Inference')\n",
        "  shutil.rmtree('metadata')\n",
        "  shutil.rmtree(f'itkimage2segimage')\n",
        "  shutil.rmtree(f'radiomics')\n",
        "  shutil.rmtree(f'dcm2niix')\n",
        "  shutil.rmtree(f'structuredReportsDICOM')\n",
        "  shutil.rmtree(f'structuredReportsJSON')\n",
        "  shutil.rmtree(f'jsonConfigs')\n",
        "except OSError:\n",
        "  pass\n",
        "os.mkdir('Inference')\n",
        "os.mkdir('metadata')\n",
        "os.mkdir(f'itkimage2segimage')\n",
        "os.mkdir(f'radiomics')\n",
        "os.mkdir(f'dcm2niix')\n",
        "os.mkdir(f'structuredReportsDICOM')\n",
        "os.mkdir(f'structuredReportsJSON')\n",
        "os.mkdir(f'jsonConfigs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpY3JnL9N7zI"
      },
      "source": [
        "### **Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0ufgeVFnlCm"
      },
      "outputs": [],
      "source": [
        "def download_dicom_data(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Downloads raw DICOM data\n",
        "\n",
        "    Args:\n",
        "    series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "    download_directory = f\"idc_data/{series_id}\"\n",
        "    # Attempt to remove the directory for the series if it exists\n",
        "    try:\n",
        "        shutil.rmtree(download_directory)\n",
        "    except OSError:\n",
        "        pass\n",
        "    print(f'\\n Downloading DICOM files from IDC Storage Buckets \\n')\n",
        "    idc_client.download_dicom_series(seriesInstanceUID= series_id, downloadDir=download_directory, quiet=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEOZoiAaUNg3"
      },
      "outputs": [],
      "source": [
        "def is_series_CT(series_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Gets the image modality for the corresponding seriesInstanceUID from idc-index\n",
        "    Refer to this query for additional columns available in idc-index!\n",
        "\n",
        "    https://github.com/ImagingDataCommons/idc-index/blob/main/queries/idc_index.sql\n",
        "\n",
        "    Args:\n",
        "    series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be processed.\n",
        "    \"\"\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "    Modality\n",
        "    FROM index\n",
        "    WHERE SeriesInstanceUID = '{series_id}'\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        modality_df = idc_client.sql_query(query)\n",
        "        if not modality_df.empty:\n",
        "            modality = modality_df['Modality'][0]\n",
        "            if modality=='CT':\n",
        "              return True\n",
        "            else:\n",
        "              log_modality_errors(series_id)\n",
        "              return False\n",
        "        else:\n",
        "            log_modality_errors(series_id)\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        log_modality_errors(series_id)\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrqdTkCg9Jqg"
      },
      "outputs": [],
      "source": [
        "def is_series_greater_than_800_slices(series_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Gets the SOPInstance count of the corresponding seriesInstanceUID from idc-index\n",
        "    Refer to this query for additional columns available in idc-index!\n",
        "    The reason why we are checking for 800 slices is whether to extract radiomics features\n",
        "    using multiprocessor or sequentially. We validated that series with slices upto 800 slices\n",
        "    work with out any issues with multiprocessor. However, above 800 we did find erratic\n",
        "    behavior. While it is inefficient to do extract radiomics features sequentially, we\n",
        "    expect it to work.\n",
        "\n",
        "    https://github.com/ImagingDataCommons/idc-index/blob/main/queries/idc_index.sql\n",
        "\n",
        "    Args:\n",
        "    series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be processed.\n",
        "    \"\"\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "    instanceCount\n",
        "    FROM index\n",
        "    WHERE SeriesInstanceUID = '{series_id}'\n",
        "    \"\"\"\n",
        "    sopInstanceCount_df = idc_client.sql_query(query)\n",
        "    if int(sopInstanceCount_df[\"instanceCount\"][0]) > 800:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvHzwKH8UNg3"
      },
      "outputs": [],
      "source": [
        "def log_modality_errors(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Logs an error when the modality is not CT for a given series.\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "    \"\"\"\n",
        "    # Open the log file in append mode\n",
        "    with open(\"modality_error_file.txt\", \"a\") as f:\n",
        "        # Write the error message to the file\n",
        "        f.write(f\"Error: Modality is not CT for series {series_id}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqWVlGEJZMSI"
      },
      "outputs": [],
      "source": [
        "def get_series_number(series_id: str) -> int:\n",
        "    \"\"\"\n",
        "    Gets the series number idc-index\n",
        "    Refer to this query for additional columns available!\n",
        "\n",
        "    https://github.com/ImagingDataCommons/idc-index/blob/main/queries/idc_index.sql\n",
        "\n",
        "    Args:\n",
        "    series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be processed.\n",
        "    \"\"\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "    SeriesNumber\n",
        "    FROM index\n",
        "    WHERE SeriesInstanceUID = '{series_id}'\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        series_number_df = idc_client.sql_query(query)\n",
        "        if not series_number_df.empty:\n",
        "            series_number = series_number_df['SeriesNumber'][0]\n",
        "            return series_number\n",
        "        else:\n",
        "            print(\"No results found for the given series_id.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmvzX809tGxM"
      },
      "outputs": [],
      "source": [
        "def convert_dicom_to_nifti(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Converts a DICOM series to a NIfTI file.\n",
        "\n",
        "    Args:\n",
        "      series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "\n",
        "    # Attempt to remove the directory for the series if it exists\n",
        "    try:\n",
        "        shutil.rmtree(f\"dcm2niix/{series_id}\")\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    # Create a new directory for the series\n",
        "    os.mkdir(f\"dcm2niix/{series_id}\")\n",
        "\n",
        "    print(\"\\n Converting DICOM files to NIfTI \\n\")\n",
        "\n",
        "    # Run the appropriate converter command and capture the output\n",
        "\n",
        "    result = subprocess.run(\n",
        "        f\"dcm2niix -z y -f %j_%p_%t_%s -b n -m y -o dcm2niix/{series_id} idc_data/{series_id}/\",\n",
        "        shell=True,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "    )\n",
        "    print(result.stdout)\n",
        "    print(\"\\n Conversion successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uultH-8naOht"
      },
      "outputs": [],
      "source": [
        "def check_dicom_conversion_errors(series_id_folder_path):\n",
        "    \"\"\"\n",
        "    This function checks if the conversion from DICOM to NIfTI format was successful.\n",
        "    It does this by checking the number of files in the specified folder.\n",
        "    The conversion is considered successful if there is exactly one file in the folder.\n",
        "\n",
        "    Args:\n",
        "    series_id_folder_path (str): The path of the folder containing the converted NIfTI files.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if there was an error in the conversion\n",
        "    (i.e., no files or more than one file in the folder), False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get a list of all files in the specified folder\n",
        "    nifti_files = os.listdir(series_id_folder_path)\n",
        "\n",
        "    # Check if the folder is empty\n",
        "    if len(nifti_files) == 0:\n",
        "        # If the folder is empty, log an error message in 'error_file.txt'\n",
        "        # This indicates that no file was created during the conversion, which means an error occurred\n",
        "        with open('error_file.txt', 'a') as f:\n",
        "            f.write(f\"Error: No files in {series_id_folder_path}\\n\")\n",
        "        # Return True to indicate an error\n",
        "        return True\n",
        "\n",
        "    # Check if the folder contains more than one file\n",
        "    elif len(nifti_files) > 1:\n",
        "        # If the folder contains more than one file, log an error message in 'error_file.txt'\n",
        "        # This indicates that more than one file was created during the conversion, which should not happen and thus means an error occurred\n",
        "        with open('error_file.txt', 'a') as f:\n",
        "            f.write(f\"Error: More than one file in {series_id_folder_path}\\n\")\n",
        "        # Return True to indicate an error\n",
        "        return True\n",
        "\n",
        "    # If there is exactly one file in the folder, no error occurred during the conversion\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yymi1fnid_fH"
      },
      "outputs": [],
      "source": [
        "def check_total_segmentator_errors(series_id: str):\n",
        "  \"\"\"\n",
        "  This function checks if the output files from TotalSegmentator exist.\n",
        "\n",
        "  Args:\n",
        "  series_id (str): The DICOM Tag SeriesInstanceUID of the DICOM series to be checked.\n",
        "\n",
        "  Returns:\n",
        "  bool: True if any of the output files do not exist, False otherwise.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define the output files from TotalSegmentator\n",
        "  output_files = [f\"Inference/{series_id}/segmentations.nii\"]\n",
        "\n",
        "  # Check if all output files exist\n",
        "  if not all(os.path.exists(file) for file in output_files):\n",
        "      # If any of the output files do not exist, log an error\n",
        "      with open('totalsegmentator_errors.txt', 'a') as f:\n",
        "          f.write(f\"Error: TotalSegmentator failed for series {series_id}\\n\")\n",
        "      return True\n",
        "\n",
        "  return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PndAg3dFTFtb"
      },
      "outputs": [],
      "source": [
        "def dicom_seg_config(series_id: str, series_number: str) -> None:\n",
        "    \"\"\"\n",
        "    Creates JSON config file required for DICOM SEG creation with dcmqi\n",
        "\n",
        "    Args:\n",
        "        series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "        series_number: The DICOM Tag SeriesNumber of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "    # Open the JSON file and load the data\n",
        "    with open(\"dicomseg_metadata_whole_slicerAsRef.json\", \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Update the SeriesNumber and SeriesDescription fields\n",
        "    if series_number:\n",
        "        data[\"SeriesNumber\"] = str(int(series_number) * 100)\n",
        "        data[\"SeriesDescription\"] = f\"TotalSegmentator(v1.5.6) Segmentation of Series {series_number}\"\n",
        "    else:\n",
        "        data[\"SeriesDescription\"] = \"TotalSegmentator(v1.5.6) Segmentation\"\n",
        "\n",
        "    # Attempt to remove the directory for the series if it exists\n",
        "    try:\n",
        "        shutil.rmtree(f\"jsonConfigs/{series_id}\")\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    # Create a new directory for the series\n",
        "    os.mkdir(f\"jsonConfigs/{series_id}\")\n",
        "\n",
        "    print(\"\\n Creating JSON Config file dcmqi itkimage2image\")\n",
        "\n",
        "    # Write the updated data back to the JSON file\n",
        "    with open(f\"jsonConfigs/{series_id}/{series_id}_dcmqi_config.json\", \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "        print(\"\\n JSON Config creation successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS_XC3wnTISO"
      },
      "outputs": [],
      "source": [
        "def create_dicom_seg_dcmqi(series_id: str):\n",
        "    \"\"\"\n",
        "    Creates a DICOM SEG file using dcmqi.\n",
        "\n",
        "    Args:\n",
        "        series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n Generating DICOM SEG File\")\n",
        "\n",
        "    inference_nifti_filename = os.path.join(\n",
        "        \"Inference\", series_id, series_id + \".nii\"\n",
        "    )\n",
        "    start_time = time.time()\n",
        "\n",
        "    !itkimage2segimage --inputImageList {inference_nifti_filename} \\\n",
        "        --inputDICOMDirectory idc_data/{series_id}/ \\\n",
        "        --outputDICOM itkimage2segimage/{series_id}/{series_id}.dcm \\\n",
        "        --inputMetadata jsonConfigs/{series_id}/{series_id}_dcmqi_config.json \\\n",
        "        --skip >> /dev/null\n",
        "\n",
        "    itkimage2segimage_time = time.time() - start_time\n",
        "\n",
        "    print(\"\\n DICOM SEG created in %g seconds.\\n\" % itkimage2segimage_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpLGA-W-TO8Q"
      },
      "outputs": [],
      "source": [
        "def dicom_seg_creation_errors(series_id: str):\n",
        "    \"\"\"\n",
        "    Verify if the DICOM segmentation was successful for a given series.\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "\n",
        "    Returns:\n",
        "       None. Creates an error file if applicable\n",
        "    \"\"\"\n",
        "    # Define the path to the output file\n",
        "    output_file_path = f\"itkimage2segimage/{series_id}/{series_id}.dcm\"\n",
        "\n",
        "    try:\n",
        "        # Check if the output file exists\n",
        "        assert os.path.exists(output_file_path)\n",
        "        return False\n",
        "    except AssertionError:\n",
        "        # If the output file does not exist, log an error\n",
        "        with open(\"itkimage2segimage_error_file.txt\", \"a\") as f:\n",
        "            f.write(f\"Error: itkimage2segimage failed for series {series_id}\\n\")\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyXZxviTTPp0"
      },
      "outputs": [],
      "source": [
        "def ndarray_to_list(obj):\n",
        "\n",
        "    \"\"\"Convert a numpy array to a list.\n",
        "       Helps for saving raw radiomics in a json file\n",
        "\n",
        "    Args:\n",
        "      obj: A numpy array.\n",
        "\n",
        "    Returns:\n",
        "      A list representation of the numpy array.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    return obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiDObu4bwmXl"
      },
      "outputs": [],
      "source": [
        "# set radiomics verbosity\n",
        "logger = radiomics.logger\n",
        "logger.setLevel(logging.WARNING)\n",
        "\n",
        "def extract_radiomics_features_from_one_label(\n",
        "    segmentation_file, image_file, label_id_body_part_df, label=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Extract radiomics features from a given ct nifti image and segmentation file.\n",
        "\n",
        "    Args:\n",
        "        segmentation_file: The path to the segmentation file.\n",
        "        image_file: The path to the ct nifti image file.\n",
        "        label: The label of the region of interest in the segmentation file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the extracted radiomics features.\n",
        "    \"\"\"\n",
        "    body_part = label_id_body_part_df.loc[label_id_body_part_df[\"label_id\"] == label][\n",
        "        \"body_part\"\n",
        "    ].values[0]\n",
        "    try:\n",
        "        # Define the settings for the feature extractor\n",
        "\n",
        "        \"\"\"the tolerance value is taken from the totalsegmentator repo\n",
        "        # https://github.com/wasserth/TotalSegmentator/blob/master/totalsegmentator/statistics.py#L31\n",
        "        \"\"\"\n",
        "        settings = {\"geometryTolerance\": 1e-3}\n",
        "\n",
        "        # Create the feature extractor\n",
        "        extractor = featureextractor.RadiomicsFeatureExtractor(**settings)\n",
        "\n",
        "        # Get the list of shape and firstorder features\n",
        "        shape_features = totalsegmentator_radiomics_features_code_mapping_df[\n",
        "            totalsegmentator_radiomics_features_code_mapping_df[\n",
        "                \"pyradiomics_feature_class\"\n",
        "            ]\n",
        "            == \"shape\"\n",
        "        ][\"feature\"].tolist()\n",
        "        firstorder_features = totalsegmentator_radiomics_features_code_mapping_df[\n",
        "            totalsegmentator_radiomics_features_code_mapping_df[\n",
        "                \"pyradiomics_feature_class\"\n",
        "            ]\n",
        "            == \"firstorder\"\n",
        "        ][\"feature\"].tolist()\n",
        "\n",
        "        # Enable the shape and firstorder features\n",
        "        extractor.disableAllFeatures()\n",
        "        extractor.enableFeaturesByName(\n",
        "            shape=shape_features, firstorder=firstorder_features\n",
        "        )\n",
        "        # Extract the features\n",
        "        raw_features = extractor.execute(\n",
        "            str(image_file), str(segmentation_file), label=label\n",
        "        )\n",
        "\n",
        "        # Clean the feature names and round the values\n",
        "        cleaned_features = {\n",
        "            name.replace(\"original_\", \"\"): round(float(value), 4)\n",
        "            for name, value in raw_features.items()\n",
        "            if name.startswith(\"original_\")\n",
        "        }\n",
        "        mask_stats = {\n",
        "            k: v.tolist() if isinstance(v, np.ndarray) else v\n",
        "            for k, v in cleaned_features.items()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"WARNING: radiomics raised an exception (setting all features to 0): {e}\"\n",
        "        )\n",
        "        cleaned_features = {feature: 0 for feature in shape_features}\n",
        "        raw_features= {feature: 0 for feature in shape_features}\n",
        "        mask_stats = {\n",
        "            k: v.tolist() if isinstance(v, np.ndarray) else v\n",
        "            for k, v in cleaned_features.items()\n",
        "        }\n",
        "    return body_part, mask_stats, raw_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hVYZDLW9Jqk"
      },
      "outputs": [],
      "source": [
        "def log_failed_to_save_raw_radiomics_features(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Log an error message when the raw radiomics features for a given series fail to save.\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "\n",
        "    Returns:\n",
        "        None. The error message is written to a log file.\n",
        "    \"\"\"\n",
        "    # Define the path to the log file\n",
        "    log_file_path = 'radiomics_error_file.txt'\n",
        "\n",
        "    # Define the error message\n",
        "    error_message = f\"Error: Failed to save raw radiomics features for series {series_id}\\n\"\n",
        "\n",
        "    # Append the error message to the log file\n",
        "    with open(log_file_path, 'a') as f:\n",
        "        f.write(error_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LCqiBFcfZqT"
      },
      "outputs": [],
      "source": [
        "def extract_features_for_all_labels(ct_file: Path, seg_file: Path, output_file: Path):\n",
        "    \"\"\"\n",
        "    Extract radiomics features from a given image and segmentation file for all labels, and map the label numbers to names.\n",
        "\n",
        "    Args:\n",
        "        ct_file: The path to the CT Nifti image file.\n",
        "        seg_file: The path to the segmentation file.\n",
        "        output_file: The path to the output file where the results will be saved.\n",
        "\n",
        "    Returns:\n",
        "        None. The results are saved to the output file.\n",
        "    \"\"\"\n",
        "    # Map the label IDs to body part names\n",
        "    label_id_body_part_data = map_to_binary.class_map[\"total_v1\"].items()\n",
        "    label_id_body_part_df = pd.DataFrame(\n",
        "        label_id_body_part_data, columns=[\"label_id\", \"body_part\"]\n",
        "    )\n",
        "\n",
        "    # Initialize an empty dictionary to store the results\n",
        "    stats = {}\n",
        "    raw_stats = {}\n",
        "\n",
        "    # Get the list of unique labels in the segmentation file\n",
        "    labels = [\n",
        "        int(x) for x in np.unique(nib.load(seg_file).get_fdata()).tolist() if x != 0\n",
        "    ]\n",
        "\n",
        "    # Define the function to be applied to each label\n",
        "    func = partial(\n",
        "        extract_radiomics_features_from_one_label,\n",
        "        seg_file,\n",
        "        ct_file,\n",
        "        label_id_body_part_df,\n",
        "    )\n",
        "\n",
        "    if not is_series_greater_than_800_slices(series_id):\n",
        "        # Use a multiprocessing pool to apply the function to all labels\n",
        "        with multiprocessing.Pool() as pool:\n",
        "            results = list(tqdm(pool.imap(func, labels), total=len(labels)))\n",
        "    else:\n",
        "        # Apply the function to all labels sequentially\n",
        "        results = [func(label) for label in tqdm(labels)]\n",
        "\n",
        "    # Process the results\n",
        "    for body_part, mask_stats, raw_features in results:\n",
        "        if any(v != 0 for v in mask_stats.values()):\n",
        "            stats[body_part] = mask_stats\n",
        "            raw_stats[body_part] = raw_features\n",
        "\n",
        "    # Save the results to the output file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(stats, f, indent=4)\n",
        "    try:\n",
        "        # Save the raw features to a separate output file\n",
        "        with open(output_file.rsplit(\".\", 1)[0] + \"_raw.json\", \"w\") as f:\n",
        "            json.dump(raw_stats, f, indent=4, default=ndarray_to_list)\n",
        "    except:\n",
        "        log_failed_to_save_raw_radiomics_features(series_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1qOjofmTb7Y"
      },
      "outputs": [],
      "source": [
        "def setup_and_trigger_extraction_of_radiomics_features_all_labels(series_id: str):\n",
        "    \"\"\"\n",
        "    Compute radiomics features for all labels in a given series.\n",
        "    This function calls `extract_features_for_all_labels` with\n",
        "    the expected file paths of ct nifti and segmentation nifti\n",
        "    files.\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "\n",
        "    Returns:\n",
        "        None. The results are saved to a file.\n",
        "    \"\"\"\n",
        "    # Define the paths\n",
        "    inference_nifti_path = os.path.join(\n",
        "        \"Inference\", series_id, f\"{series_id}.nii\"\n",
        "    )\n",
        "\n",
        "    # Get the full path of the CT Nifti file\n",
        "    ct_series_id_folder_path = os.path.join(\"dcm2niix\", series_id)\n",
        "    # Get the list of files in series_id_path\n",
        "    ct_nifti_files = os.listdir(ct_series_id_folder_path)\n",
        "    # Get the first (and only) file in the list\n",
        "    ct_nifti_filename = ct_nifti_files[0]\n",
        "    # Get the full path of the file\n",
        "    ct_nifti_filename_path = os.path.join(ct_series_id_folder_path, ct_nifti_filename)\n",
        "\n",
        "    # Define the output filename\n",
        "    output_filename = os.path.join(\n",
        "        \"radiomics\", series_id, f\"{series_id}_radiomics.json\"\n",
        "    )\n",
        "\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Compute the radiomics features for all labels\n",
        "    extract_features_for_all_labels(\n",
        "        ct_nifti_filename_path, inference_nifti_path, output_filename\n",
        "    )\n",
        "\n",
        "    # Calculate the elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Radiomics Features Calculation Done in {elapsed_time} seconds.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXoUjAJRTeig"
      },
      "outputs": [],
      "source": [
        "def post_process_radiomics_features(series_id: str):\n",
        "    \"\"\"\n",
        "    Post-process the computed radiomics features for a given series.\n",
        "\n",
        "    In dicom seg objects, segment numbers should be continuous,\n",
        "    so label numbers cannot be used as we may not always see all\n",
        "    labels\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame containing the processed radiomics features.\n",
        "    \"\"\"\n",
        "    # Define the path to the JSON file containing the radiomics features\n",
        "    json_path = os.path.join('radiomics', series_id, f'{series_id}_radiomics.json')\n",
        "\n",
        "    # Load the JSON file into a DataFrame\n",
        "    radiomics_df = pd.read_json(json_path, orient='index')\n",
        "\n",
        "    # Rename the columns by splitting on underscores and taking the last part\n",
        "    radiomics_df = radiomics_df.rename(columns=lambda x: x.split('_')[-1])\n",
        "\n",
        "    # Reset the index and rename the index column to 'body_part'\n",
        "    radiomics_df = radiomics_df.reset_index().rename(columns={'index': 'body_part'})\n",
        "\n",
        "    # Create a DataFrame from the class map\n",
        "    class_map_df = pd.DataFrame(map_to_binary.class_map['total_v1'].items(), columns=['label_id', 'body_part'])\n",
        "\n",
        "    # Merge the radiomics DataFrame with the class map DataFrame\n",
        "    radiomics_df = pd.merge(radiomics_df, class_map_df, on='body_part')\n",
        "\n",
        "    # Sort the 'label_id' column and create a 'seg_segment_number' column\n",
        "    radiomics_df = radiomics_df.assign(\n",
        "        label_id=radiomics_df['label_id'].astype(int).sort_values(),\n",
        "        seg_segment_number=range(1, len(radiomics_df) + 1)\n",
        "    )\n",
        "\n",
        "    # Reorder the columns\n",
        "    columns = ['body_part', 'seg_segment_number', 'label_id'] + [col for col in radiomics_df.columns if col not in ['body_part', 'seg_segment_number', 'label_id']]\n",
        "    radiomics_df = radiomics_df[columns]\n",
        "    '''\n",
        "    in IBSI kurtosis is subracted by 3 units\n",
        "    https://pubs.rsna.org/doi/suppl/10.1148/radiol.2020191145/suppl_file/IBSI_Reference_Manual.pdf#.3.4%20(Excess)%20intensity%20kurtosis\n",
        "    '''\n",
        "    radiomics_df['Kurtosis']=radiomics_df['Kurtosis']-3\n",
        "\n",
        "    return radiomics_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1npWAGf-UMXf"
      },
      "outputs": [],
      "source": [
        "def verify_radiomics_extraction(series_id: str):\n",
        "    \"\"\"\n",
        "    Verify if the radiomics feature extraction was successful for a given series.\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "\n",
        "    Returns:\n",
        "        A boolean indicating whether the radiomics feature extraction failed.\n",
        "    \"\"\"\n",
        "    # Define the path to the output file\n",
        "    output_file_path = f\"radiomics/{series_id}/{series_id}_radiomics.json\"\n",
        "\n",
        "    try:\n",
        "        # Check if the output file exists\n",
        "        assert os.path.exists(output_file_path)\n",
        "    except AssertionError:\n",
        "        # If the output file does not exist, log an error and return True\n",
        "        with open('radiomics_error_file.txt', 'a') as f:\n",
        "            f.write(f\"Error: Radiomics Feature extraction failed for series {series_id}\\n\")\n",
        "        return True\n",
        "\n",
        "    # If the output file exists, return False\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_xmbQMgNN5H"
      },
      "outputs": [],
      "source": [
        "def delete_directories_lacking_nifti_files(path: str):\n",
        "    \"\"\"\n",
        "    Remove directories from a given path that do not contain any NIfTI files.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the directory to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        None. Directories are removed in-place.\n",
        "    \"\"\"\n",
        "    # Walk through the directory tree\n",
        "    for dirpath, dirnames, filenames in os.walk(path, topdown=False):\n",
        "        # If the directory is not the root directory and does not contain any NIfTI files\n",
        "        if dirpath != path and not any(filename.endswith('.nii.lz4') for filename in filenames):\n",
        "            try:\n",
        "                # Try to remove the directory\n",
        "                os.rmdir(dirpath)\n",
        "            except OSError as e:\n",
        "                # Print an error message if the directory removal fails\n",
        "                print(f\"Error: Failed to remove directory {dirpath}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH6G9RDcPjzs"
      },
      "outputs": [],
      "source": [
        "def create_structured_report_metajson_for_features(\n",
        "    SeriesInstanceUID,\n",
        "    series_number,\n",
        "    SOPInstanceUID_seg,\n",
        "    seg_file,\n",
        "    dcm_directory,\n",
        "    segments_code_mapping_df,\n",
        "    features_code_mapping_df,\n",
        "    radiomics_features,\n",
        "):\n",
        "\n",
        "    \"\"\"Function that creates the metajson necessary for the creation of a\n",
        "    structured report from a pandas dataframe of label names and features for\n",
        "    each.\n",
        "\n",
        "    Inputs:\n",
        "      SeriesInstanceUID               : SeriesInstanceUID of the corresponding CT\n",
        "                                        file\n",
        "      series_number                    : SeriesNumber of the corresponding CT file\n",
        "      SOPInstanceUID_seg              : SOPInstanceUID of the corresponding SEG file\n",
        "      seg_file                        : filename of SEG DCM file\n",
        "      dcm_directory                   : ct directory\n",
        "      segments_code_mapping_df        : dataframe that holds the names of the\n",
        "                                        segments and the associated code values etc.\n",
        "      features_code_mapping_df        : dataframe that holds the names of the\n",
        "                                        features and the associated code values etc.\n",
        "      df_features                     : a pandas dataframe holding the segments and a\n",
        "                                        set of 3D shape features for each\n",
        "\n",
        "    Outputs:\n",
        "      Returns the metajson for the structured report that will then be used by\n",
        "      dcmqi tid1500writer to create a structured report\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Get the version number for the pyradiomics package --- #\n",
        "\n",
        "    pyradiomics_version_number = str(radiomics.__version__)\n",
        "\n",
        "    image_library= [os.path.basename(f) for f in os.listdir(dcm_directory)]\n",
        "\n",
        "    # --- Create the header for the json --- #\n",
        "\n",
        "    inputMetadata = {}\n",
        "    inputMetadata[\n",
        "        \"@schema\"\n",
        "    ] = \"https://raw.githubusercontent.com/qiicr/dcmqi/master/doc/schemas/sr-tid1500-schema.json#\"\n",
        "    inputMetadata[\"SeriesDescription\"] = (\n",
        "        \"TotalSegmentator(v1.5.6) \"\n",
        "        + features_code_mapping_df[\"pyradiomics_feature_class\"].iloc[0]\n",
        "        + \" Measurements of series \"\n",
        "        + str(series_number)\n",
        "    )\n",
        "    if series_number:\n",
        "        inputMetadata[\"SeriesNumber\"] = str(1 + int(series_number) * 100)\n",
        "    inputMetadata[\"InstanceNumber\"] = \"1\"\n",
        "\n",
        "    inputMetadata[\"compositeContext\"] = [seg_file]\n",
        "\n",
        "    inputMetadata[\"imageLibrary\"] = image_library\n",
        "    inputMetadata[\"observerContext\"] = {\n",
        "        \"ObserverType\": \"DEVICE\",\n",
        "        \"DeviceObserverName\": \"pyradiomics\",\n",
        "        \"DeviceObserverModelName\": pyradiomics_version_number,\n",
        "    }\n",
        "\n",
        "    inputMetadata[\"VerificationFlag\"] = \"UNVERIFIED\"\n",
        "    inputMetadata[\"CompletionFlag\"] = \"COMPLETE\"\n",
        "    inputMetadata[\"activitySession\"] = \"1\"\n",
        "    inputMetadata[\"timePoint\"] = \"1\"\n",
        "\n",
        "    # ------------------------------------------------------------------------- #\n",
        "    # --- Create the measurement_dict for each segment - holds all features --- #\n",
        "\n",
        "    measurement = []\n",
        "\n",
        "    # --- Now create the dict for all features and all segments --- #\n",
        "\n",
        "    # --- Loop over the number of segments --- #\n",
        "\n",
        "    # number of rows in the df_features\n",
        "    num_segments = radiomics_features.shape[0]\n",
        "    print(num_segments)\n",
        "\n",
        "    # Array of dictionaries - one dictionary for each segment\n",
        "    measurement_across_segments_combined = []\n",
        "\n",
        "    for segment_id in range(0, num_segments):\n",
        "\n",
        "        ReferencedSegment = int(\n",
        "            radiomics_features[\"seg_segment_number\"].values[segment_id]\n",
        "        )  # referencedsegment must be an integer according to the schema.\n",
        "        FindingSite = radiomics_features[\"body_part\"].values[segment_id]\n",
        "\n",
        "        # --- Create the dict for the Measurements group --- #\n",
        "        TrackingIdentifier = \"Measurements group \" + str(ReferencedSegment)\n",
        "\n",
        "        segment_row = segments_code_mapping_df[\n",
        "            segments_code_mapping_df[\"Structure\"] == FindingSite\n",
        "        ]\n",
        "\n",
        "        # Inside the loop for each segment\n",
        "        my_dict = {\n",
        "            \"TrackingIdentifier\": str(TrackingIdentifier),\n",
        "            \"ReferencedSegment\": int(ReferencedSegment),\n",
        "            \"SourceSeriesForImageSegmentation\": str(SeriesInstanceUID),\n",
        "            \"segmentationSOPInstanceUID\": str(SOPInstanceUID_seg),\n",
        "            \"Finding\": {\n",
        "                \"CodeValue\": str(\n",
        "                    segment_row[\"SegmentedPropertyCategoryCodeSequence.CodeValue\"].iloc[\n",
        "                        0\n",
        "                    ]\n",
        "                ),\n",
        "                \"CodingSchemeDesignator\": str(\n",
        "                    segment_row[\n",
        "                        \"SegmentedPropertyCategoryCodeSequence.CodingSchemeDesignator\"\n",
        "                    ].iloc[0]\n",
        "                ),\n",
        "                \"CodeMeaning\": str(\n",
        "                    segment_row[\n",
        "                        \"SegmentedPropertyCategoryCodeSequence.CodeMeaning\"\n",
        "                    ].iloc[0]\n",
        "                ),\n",
        "            },\n",
        "            \"FindingSite\": {\n",
        "                \"CodeValue\": str(\n",
        "                    segment_row[\"SegmentedPropertyTypeCodeSequence.CodeValue\"].iloc[0]\n",
        "                ),\n",
        "                \"CodingSchemeDesignator\": str(\n",
        "                    segment_row[\n",
        "                        \"SegmentedPropertyTypeCodeSequence.CodingSchemeDesignator\"\n",
        "                    ].iloc[0]\n",
        "                ),\n",
        "                \"CodeMeaning\": str(\n",
        "                    segment_row[\"SegmentedPropertyTypeCodeSequence.CodeMeaning\"].iloc[0]\n",
        "                ),\n",
        "            },\n",
        "        }\n",
        "\n",
        "        laterality_dict = {\n",
        "            \"CodeValue\": str(\n",
        "                segment_row[\"SegmentedPropertyTypeModifierCodeSequence.CodeValue\"].iloc[\n",
        "                    0\n",
        "                ]\n",
        "            ),\n",
        "            \"CodingSchemeDesignator\": str(\n",
        "                segment_row[\n",
        "                    \"SegmentedPropertyTypeModifierCodeSequence.CodingSchemeDesignator\"\n",
        "                ].iloc[0]\n",
        "            ),\n",
        "            \"CodeMeaning\": str(\n",
        "                segment_row[\n",
        "                    \"SegmentedPropertyTypeModifierCodeSequence.CodeMeaning\"\n",
        "                ].iloc[0]\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        # Append the remaining code after creating the measurement_across_segments_combined array\n",
        "        # Check if the laterality dictionary is empty or contains NaN values\n",
        "        if laterality_dict and not any(\n",
        "            value == \"nan\" or pd.isna(value) for value in laterality_dict.values()\n",
        "        ):\n",
        "            my_dict[\"Laterality\"] = laterality_dict\n",
        "\n",
        "        measurement = []\n",
        "        # number of features - number of columns in df_features - 2 (label_name and ReferencedSegment)\n",
        "        num_values = len(radiomics_features.columns) - 2\n",
        "\n",
        "        # feature_list = radiomics_features.columns[2:] # remove first two\n",
        "        feature_list = features_code_mapping_df.feature.to_list()\n",
        "\n",
        "        # For each measurement per region segment\n",
        "        for n in range(0, len(feature_list)):\n",
        "            measurement_dict = {}\n",
        "            row = radiomics_features.loc[radiomics_features[\"body_part\"] == FindingSite]\n",
        "            feature_row = features_code_mapping_df.loc[\n",
        "                features_code_mapping_df[\"feature\"] == feature_list[n]\n",
        "            ]\n",
        "            value = str(np.round(row[feature_list[n]].values[0], 3))\n",
        "            measurement_dict[\"value\"] = value\n",
        "            measurement_dict[\"quantity\"] = {}\n",
        "            measurement_dict[\"quantity\"][\"CodeValue\"] = str(\n",
        "                feature_row[\"quantity_CodeValue\"].values[0]\n",
        "            )\n",
        "            measurement_dict[\"quantity\"][\"CodingSchemeDesignator\"] = str(\n",
        "                feature_row[\"quantity_CodingSchemeDesignator\"].values[0]\n",
        "            )\n",
        "            measurement_dict[\"quantity\"][\"CodeMeaning\"] = str(\n",
        "                feature_row[\"quantity_CodeMeaning\"].values[0]\n",
        "            )\n",
        "            measurement_dict[\"units\"] = {}\n",
        "            measurement_dict[\"units\"][\"CodeValue\"] = str(\n",
        "                feature_row[\"units_CodeValue\"].values[0]\n",
        "            )\n",
        "            measurement_dict[\"units\"][\"CodingSchemeDesignator\"] = str(\n",
        "                feature_row[\"units_CodingSchemeDesignator\"].values[0]\n",
        "            )\n",
        "            measurement_dict[\"units\"][\"CodeMeaning\"] = str(\n",
        "                feature_row[\"units_CodeMeaning\"].values[0]\n",
        "            )\n",
        "            measurement_dict[\"measurementAlgorithmIdentification\"] = {}\n",
        "            measurement_dict[\"measurementAlgorithmIdentification\"][\n",
        "                \"AlgorithmName\"\n",
        "            ] = \"pyradiomics\"\n",
        "            measurement_dict[\"measurementAlgorithmIdentification\"][\n",
        "                \"AlgorithmVersion\"\n",
        "            ] = str(pyradiomics_version_number)\n",
        "            measurement.append(measurement_dict)\n",
        "\n",
        "        measurement_combined_dict = {}\n",
        "        measurement_combined_dict[\n",
        "            \"measurementItems\"\n",
        "        ] = measurement  # measurement is an array of dictionaries\n",
        "\n",
        "        output_dict_one_segment = {**my_dict, **measurement_combined_dict}\n",
        "\n",
        "        # append to array for all segments\n",
        "\n",
        "        measurement_across_segments_combined.append(output_dict_one_segment)\n",
        "\n",
        "    # --- Add the measurement data --- #\n",
        "\n",
        "    inputMetadata[\"Measurements\"] = {}\n",
        "    inputMetadata[\"Measurements\"] = measurement_across_segments_combined\n",
        "    feature_type = features_code_mapping_df[\"pyradiomics_feature_class\"].iloc[0]\n",
        "    sr_json_path = f\"structuredReportsJSON/{series_id}/{series_id}_{feature_type}_sr.json\"\n",
        "    sr_path = f\"structuredReportsDICOM/{series_id}/{series_id}_{feature_type}_sr.dcm\"\n",
        "    pred_dicomseg_path = f\"itkimage2segimage/{series_id}\"\n",
        "\n",
        "    with open(sr_json_path, \"w\") as f:\n",
        "        json.dump(inputMetadata, f, indent=2)\n",
        "        print(f\"wrote out json for {feature_type} features\")\n",
        "\n",
        "    inputImageLibraryDirectory = dcm_directory\n",
        "    # outputDICOM = sr_json_path\n",
        "    outputDICOM = sr_path\n",
        "    # the name of the folder where the seg files are located\n",
        "    inputCompositeContextDirectory = pred_dicomseg_path\n",
        "    inputMetadata_json = sr_json_path\n",
        "\n",
        "    print(\"inputImageLibraryDirectory: \" + str(inputImageLibraryDirectory))\n",
        "    print(\"outputDICOM: \" + str(outputDICOM))\n",
        "    print(\"inputCompositeContextDirectory: \" + str(inputCompositeContextDirectory))\n",
        "    print(\"inputMetadata_json: \" + str(inputMetadata_json))\n",
        "    !tid1500writer --inputImageLibraryDirectory $inputImageLibraryDirectory \\\n",
        "                  --outputDICOM $outputDICOM  \\\n",
        "                  --inputCompositeContextDirectory $inputCompositeContextDirectory \\\n",
        "                  --inputMetadata $sr_json_path\n",
        "    print(f\"wrote out SR for {feature_type} radiomics features\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlUeNSudU06V"
      },
      "outputs": [],
      "source": [
        "def initialize_directories(series_id: str):\n",
        "    \"\"\"\n",
        "    Initialize directories for itkimage2segimage and Radiomics processing.\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "\n",
        "    Returns:\n",
        "        None. Directories are created in-place.\n",
        "    \"\"\"\n",
        "    # Define the directories to be created\n",
        "    directories = [\n",
        "        f'Inference/{series_id}',\n",
        "        f'itkimage2segimage/{series_id}',\n",
        "        f'radiomics/{series_id}',\n",
        "        f'structuredReportsDICOM/{series_id}',\n",
        "        f'structuredReportsJSON/{series_id}'\n",
        "    ]\n",
        "\n",
        "    for directory in directories:\n",
        "        try:\n",
        "            # Try to remove the directory if it exists\n",
        "            shutil.rmtree(directory)\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "        # Create the directory\n",
        "        os.mkdir(directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iK8HeKrVKym"
      },
      "outputs": [],
      "source": [
        "def compress_files(series_id: str):\n",
        "    \"\"\"\n",
        "    Compress files using LZ4.\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "\n",
        "    Returns:\n",
        "        None. Files are compressed in-place.\n",
        "    \"\"\"\n",
        "    # Define the files to be compressed\n",
        "    files = [\n",
        "        f'itkimage2segimage/{series_id}/{series_id}.dcm',\n",
        "        f'radiomics/{series_id}/{series_id}_radiomics.json',\n",
        "        f'radiomics/{series_id}/{series_id}_radiomics_raw.json',\n",
        "        f'structuredReportsDICOM/{series_id}/{series_id}_shape_sr.dcm',\n",
        "        f'structuredReportsDICOM/{series_id}/{series_id}_firstorder_sr.dcm',\n",
        "        f'structuredReportsJSON/{series_id}/{series_id}_shape_sr.json',\n",
        "        f'structuredReportsJSON/{series_id}/{series_id}_firstorder_sr.json'\n",
        "    ]\n",
        "\n",
        "    for file in files:\n",
        "        # Compress the file using LZ4\n",
        "        os.system(f'lz4 --rm {file} {file}.lz4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwRE4F8AmscK"
      },
      "outputs": [],
      "source": [
        "def endToEndTotalSegmentator(series_id:str, runtime_stats: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download DICOM files, convert to NIfTI, perform Inference\n",
        "    encode segmentations in DICOM SEG objects,\n",
        "    extract radiomics features and create SR objects\n",
        "    encoding shape and first order features\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "\n",
        "    Returns:\n",
        "        None. Files are compressed in-place.\n",
        "    \"\"\"\n",
        "    initialize_directories(series_id)\n",
        "\n",
        "    print(\"Processing series: \" + series_id)\n",
        "\n",
        "    if is_series_CT(series_id):\n",
        "\n",
        "        start_time = time.time()\n",
        "        download_dicom_data(series_id)\n",
        "        dicom_download_time = time.time() - start_time\n",
        "\n",
        "        series_number= get_series_number(series_id)\n",
        "\n",
        "        start_time = time.time()\n",
        "        convert_dicom_to_nifti(series_id)\n",
        "        dicom_conversion_time = time.time() - start_time\n",
        "\n",
        "        series_id_folder_path = os.path.join(\"dcm2niix\", series_id)\n",
        "\n",
        "        if not check_dicom_conversion_errors(series_id_folder_path):\n",
        "            # Get the list of files in series_id_path\n",
        "            nifti_files = os.listdir(series_id_folder_path)\n",
        "            # Get the first (and only) file in the list\n",
        "            nifti_filename = nifti_files[0]\n",
        "            # Get the full path of the file\n",
        "            nifti_filename_path = os.path.join(series_id_folder_path, nifti_filename)\n",
        "\n",
        "            start_time = time.time()\n",
        "            #!TotalSegmentator -i {nifti_filename_path} -o segmentations --ml --fast --preview  --radiomics\n",
        "            result = subprocess.run(\n",
        "                [\n",
        "                    \"TotalSegmentator\",\n",
        "                    \"-i\",\n",
        "                    nifti_filename_path,\n",
        "                    \"-o\",\n",
        "                    \"segmentations\",\n",
        "                    \"--ml\",\n",
        "                ],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                universal_newlines=True,\n",
        "            )\n",
        "            print(result.stdout)\n",
        "            total_segmentator_time = time.time() - start_time\n",
        "\n",
        "            try:\n",
        "                subprocess.run(\n",
        "                    [\n",
        "                        \"mv\",\n",
        "                        f\"segmentations.nii\",\n",
        "                        f\"Inference/{series_id}/\",\n",
        "                    ],\n",
        "                    check=True,\n",
        "                )\n",
        "                print(\"Files moved successfully using the first command\")\n",
        "            except subprocess.CalledProcessError:\n",
        "                try:\n",
        "                    subprocess.run(\n",
        "                        [\n",
        "                            \"mv\",\n",
        "                            \"segmentations/segmentations.nii\",\n",
        "                            f\"Inference/{series_id}/\",\n",
        "                        ],\n",
        "                        check=True,\n",
        "                    )\n",
        "                    print(\"Files moved successfully using the second command\")\n",
        "                except subprocess.CalledProcessError:\n",
        "                    print(\"Error: Failed to move files using both commands\")\n",
        "\n",
        "            if not check_total_segmentator_errors(series_id):\n",
        "\n",
        "                !mv Inference/{series_id}/segmentations.nii Inference/{series_id}/{series_id}.nii\n",
        "\n",
        "                #dicomseg\n",
        "                start_time = time.time()\n",
        "                dicom_seg_config(series_id, series_number)\n",
        "                create_dicom_seg_dcmqi(series_id)\n",
        "                itkimage2segimage_time = time.time() - start_time\n",
        "\n",
        "                if not dicom_seg_creation_errors(series_id):\n",
        "\n",
        "                    #radiomics features\n",
        "                    start_time = time.time()\n",
        "                    setup_and_trigger_extraction_of_radiomics_features_all_labels(series_id)\n",
        "                    verify_radiomics_extraction(series_id)\n",
        "                    radiomics_features = post_process_radiomics_features(series_id)\n",
        "                    radiomics_time = time.time() - start_time\n",
        "\n",
        "                    #structured reports\n",
        "                    start_time = time.time()\n",
        "                    seg_dcm = dcmread(f\"itkimage2segimage/{series_id}/{series_id}.dcm\")\n",
        "                    SOPInstanceUID_seg = seg_dcm.SOPInstanceUID\n",
        "                    seg_file = f\"{series_id}.dcm\"\n",
        "                    dicom_directory = f\"idc_data/{series_id}\"\n",
        "                    segments_code_mapping_df = totalsegmentator_segments_code_mapping_df\n",
        "                    shape_features_code_mapping_df = (\n",
        "                        totalsegmentator_radiomics_features_code_mapping_df[\n",
        "                            totalsegmentator_radiomics_features_code_mapping_df[\n",
        "                                \"pyradiomics_feature_class\"\n",
        "                            ]\n",
        "                            == \"shape\"\n",
        "                        ]\n",
        "                    )\n",
        "                    first_order_features_code_mapping_df = (\n",
        "                        totalsegmentator_radiomics_features_code_mapping_df[\n",
        "                            totalsegmentator_radiomics_features_code_mapping_df[\n",
        "                                \"pyradiomics_feature_class\"\n",
        "                            ]\n",
        "                            == \"firstorder\"\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    # Create structured report meta-JSON for shape features\n",
        "                    create_structured_report_metajson_for_features(\n",
        "                        series_id,\n",
        "                        series_number,\n",
        "                        SOPInstanceUID_seg,\n",
        "                        seg_file,\n",
        "                        dicom_directory,\n",
        "                        segments_code_mapping_df,\n",
        "                        shape_features_code_mapping_df,\n",
        "                        radiomics_features,\n",
        "                    )\n",
        "\n",
        "                    # Create structured report meta-JSON for first order features\n",
        "                    create_structured_report_metajson_for_features(\n",
        "                        series_id,\n",
        "                        series_number,\n",
        "                        SOPInstanceUID_seg,\n",
        "                        seg_file,\n",
        "                        dicom_directory,\n",
        "                        segments_code_mapping_df,\n",
        "                        first_order_features_code_mapping_df,\n",
        "                        radiomics_features,\n",
        "                    )\n",
        "\n",
        "                    # Calculate the time taken to generate structured reports\n",
        "                    structuredReportsGenerationTime = time.time() - start_time\n",
        "                    print(\n",
        "                        f\"Structured Reports Generated in {structuredReportsGenerationTime} seconds.\\n\"\n",
        "                    )\n",
        "\n",
        "                    # Record the start time\n",
        "                    start_time = time.time()\n",
        "                    # Compress the files\n",
        "                    compress_files(series_id)\n",
        "                    # Calculate the time taken to archive the files\n",
        "                    archiving_time = time.time() - start_time\n",
        "                else:\n",
        "                    archiving_time = 0\n",
        "                    radiomics_time = 0\n",
        "                    structuredReportsGenerationTime = 0\n",
        "            else:\n",
        "                archiving_time = 0\n",
        "                itkimage2segimage_time = 0\n",
        "                radiomics_time = 0\n",
        "                structuredReportsGenerationTime = 0\n",
        "        else:\n",
        "            total_segmentator_time = 0\n",
        "            archiving_time = 0\n",
        "            itkimage2segimage_time = 0\n",
        "            radiomics_time = 0\n",
        "            structuredReportsGenerationTime = 0\n",
        "    else:\n",
        "        dicom_download_time=0\n",
        "        dicom_conversion_time=0\n",
        "        total_segmentator_time = 0\n",
        "        archiving_time = 0\n",
        "        itkimage2segimage_time = 0\n",
        "        radiomics_time = 0\n",
        "        structuredReportsGenerationTime = 0\n",
        "\n",
        "    log = pd.DataFrame({\"SeriesInstanceUID\": [series_id]})\n",
        "\n",
        "    log[\"dicom_download_time\"] = dicom_download_time\n",
        "    log[\"dicom_conversion_time\"] = dicom_conversion_time\n",
        "    log[\"total_segmentator_time\"] = total_segmentator_time\n",
        "    log[\"itkimage2segimage_time\"] = itkimage2segimage_time\n",
        "    log[\"radiomics_time\"] = radiomics_time\n",
        "    log[\"structuredReportsGenerationTime\"] = structuredReportsGenerationTime\n",
        "    log[\"archiving_time\"] = archiving_time\n",
        "\n",
        "    !rm -r Inference/{series_id}\n",
        "    !rm -r idc_data\n",
        "    !rm -r dcm2niix/{series_id}\n",
        "\n",
        "    runtime_stats = pd.concat([runtime_stats, log], ignore_index=True, axis=0)\n",
        "\n",
        "    return runtime_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rMtxR77N_RH"
      },
      "outputs": [],
      "source": [
        "class MemoryMonitor:\n",
        "    def __init__(self):\n",
        "        self.keep_measuring = True\n",
        "        self.working_disk_path = self.get_working_disk_path()\n",
        "\n",
        "    def get_working_disk_path(self):\n",
        "        partitions = psutil.disk_partitions()\n",
        "        for partition in partitions:\n",
        "            if partition.mountpoint == '/':\n",
        "                return '/'\n",
        "            elif '/cromwell_root' in partition.mountpoint:\n",
        "                return '/cromwell_root'\n",
        "        return '/'  # Default to root directory if no specific path is found\n",
        "    def measure_usage(self):\n",
        "        cpu_usage = []\n",
        "        ram_usage_mb=[]\n",
        "        gpu_usage_mb=[]\n",
        "        disk_usage_all=[]\n",
        "        time_stamps = []\n",
        "        start_time = time.time()\n",
        "        while self.keep_measuring:\n",
        "            cpu = psutil.cpu_percent()\n",
        "            ram = psutil.virtual_memory()\n",
        "            disk_usage = psutil.disk_usage(self.working_disk_path)\n",
        "            disk_used = disk_usage.used / 1000 / 1000 / 1000\n",
        "            disk_total = disk_usage.total / 1000 / 1000 / 1000\n",
        "            ram_total_mb = psutil.virtual_memory().total / 1000 / 1000\n",
        "            ram_mb = (ram.total - ram.available) / 1000 / 1000\n",
        "            try:\n",
        "                nvidia_smi.nvmlInit()\n",
        "                handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "                info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "                gpu_type = nvidia_smi.nvmlDeviceGetName(handle)\n",
        "                gpu_total_mb = info.total/1000/1000\n",
        "                gpu_mb = info.used/1000/1000\n",
        "                nvidia_smi.nvmlShutdown()\n",
        "            except:\n",
        "                gpu_type = ''\n",
        "                gpu_total_mb = 0\n",
        "                gpu_mb = 0\n",
        "\n",
        "            cpu_usage.append(cpu)\n",
        "            ram_usage_mb.append(ram_mb)\n",
        "            disk_usage_all.append(disk_used)\n",
        "            gpu_usage_mb.append(gpu_mb)\n",
        "            time_stamps.append(time.time()- start_time)\n",
        "            sleep(1)\n",
        "\n",
        "        return cpu_usage, ram_usage_mb, time_stamps, ram_total_mb, gpu_usage_mb, gpu_total_mb, gpu_type, disk_usage_all, disk_total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTPLJHOeNN5I"
      },
      "outputs": [],
      "source": [
        "#removing empty directories\n",
        "delete_directories_lacking_nifti_files(os.path.join('Inference/'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqxRLpdSQAQ5"
      },
      "source": [
        "### **Convert Inference NIFTI file to DICOM_SEG Object**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_-jEmCgKJBe"
      },
      "outputs": [],
      "source": [
        "runtime_stats = pd.DataFrame(columns=['SeriesInstanceUID','dicom_download_time', 'dicom_conversion_time','total_segmentator_time', 'itkimage2segimage_time','radiomics_time',\n",
        "                                      'archiving_time','structuredReportsGenerationTime', 'cpu_usage','ram_usage_mb', 'ram_total_mb', 'gpu_usage_mb','gpu_total_mb','gpu_type',\n",
        "                                      'disk_usage_all', 'disk_total'\n",
        "                                      ])\n",
        "if __name__ == \"__main__\":\n",
        "    for series_id in SeriesInstanceUIDs:\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            monitor = MemoryMonitor()\n",
        "            mem_thread = executor.submit(monitor.measure_usage)\n",
        "            try:\n",
        "                proc_thread = executor.submit(endToEndTotalSegmentator, series_id, runtime_stats)\n",
        "                runtime_stats = proc_thread.result()\n",
        "            finally:\n",
        "                monitor.keep_measuring = False\n",
        "                cpu_usage, ram_usage_mb, time_stamps, ram_total_mb, gpu_usage_mb, gpu_total_mb, gpu_type, disk_usage_all, disk_total = mem_thread.result()\n",
        "\n",
        "                cpu_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[cpu_idx, runtime_stats.columns.get_loc('cpu_usage')] = [[cpu_usage]]\n",
        "\n",
        "                ram_usage_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_usage_mb_idx, runtime_stats.columns.get_loc('ram_usage_mb')] = [[ram_usage_mb]]\n",
        "\n",
        "                ram_total_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_total_mb_idx, runtime_stats.columns.get_loc('ram_total_mb')] = [[ram_total_mb]]\n",
        "\n",
        "                gpu_total_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[gpu_total_mb_idx, runtime_stats.columns.get_loc('gpu_total_mb')] = [[gpu_total_mb]]\n",
        "\n",
        "                gpu_usage_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[gpu_usage_mb_idx, runtime_stats.columns.get_loc('gpu_usage_mb')] = [[gpu_usage_mb]]\n",
        "\n",
        "\n",
        "                disk_usage_gb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[disk_usage_gb_idx, runtime_stats.columns.get_loc('disk_usage_all')] = [[disk_usage_all]]\n",
        "\n",
        "                runtime_stats['gpu_type']=gpu_type\n",
        "                runtime_stats['disk_total']=disk_total\n",
        "\n",
        "                fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8, 6))\n",
        "\n",
        "                ax1.plot(time_stamps, cpu_usage)\n",
        "                ax1.set_ylim(0, 100)\n",
        "                ax1.set_xlabel('Time (s)')\n",
        "                ax1.set_ylabel('CPU usage (%)')\n",
        "\n",
        "                ax2.plot(time_stamps, ram_usage_mb)\n",
        "                ax2.set_ylim(0, ram_total_mb)\n",
        "                ax2.set_xlabel('Time (s)')\n",
        "                ax2.set_ylabel('Memory usage (MB)')\n",
        "\n",
        "                ax3.plot(time_stamps, gpu_usage_mb)\n",
        "                ax3.set_ylim(0, gpu_total_mb)\n",
        "                ax3.set_xlabel('Time (s)')\n",
        "                ax3.set_ylabel('GPU Memory usage (MB)')\n",
        "\n",
        "                ax4.plot(time_stamps, disk_usage_all)\n",
        "                ax4.set_ylim(0, disk_total)\n",
        "                ax4.set_xlabel('Time (s)')\n",
        "                ax4.set_ylabel('Disk usage (GB)')\n",
        "                plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFqgbLWuQs9f"
      },
      "source": [
        "### **Compressing Output Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4xcpEa8pzZH"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "try:\n",
        "  os.remove('dicomsegAndRadiomicsSR_DICOMsegFiles.tar.lz4')\n",
        "  os.remove('pyradiomicsRadiomicsFeatures.tar.lz4')\n",
        "  os.remove('structuredReportsDICOM.tar.lz4')\n",
        "  os.remove('structuredReportsJSON.tar.lz4')\n",
        "\n",
        "except OSError:\n",
        "  pass\n",
        "!tar cvf - itkimage2segimage | lz4 > dicomsegAndRadiomicsSR_DICOMsegFiles.tar.lz4\n",
        "!tar cvf - radiomics | lz4 > pyradiomicsRadiomicsFeatures.tar.lz4\n",
        "!tar cvf - structuredReportsDICOM | lz4 > structuredReportsDICOM.tar.lz4\n",
        "!tar cvf - structuredReportsJSON | lz4 > structuredReportsJSON.tar.lz4\n",
        "\n",
        "output_archiving_time = time.time() - start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYXE4SlxQxXy"
      },
      "source": [
        "### **Utilization Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sSM4RRfpxeC"
      },
      "outputs": [],
      "source": [
        "runtime_stats.to_csv('runtime.csv')\n",
        "runtime_stats['output_archiving_time']=output_archiving_time\n",
        "try:\n",
        "  os.remove('endToEndTotalSegmentator_UsageMetrics.lz4')\n",
        "except OSError:\n",
        "  pass\n",
        "!lz4 runtime.csv endToEndTotalSegmentator_UsageMetrics.lz4\n",
        "runtime_stats"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
