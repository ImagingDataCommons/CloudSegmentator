{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/CloudSegmentator/blob/v1.3.0/workflows/TotalSegmentator/Notebooks/PostProcessingTerra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6wtCVV3rGS7"
      },
      "source": [
        "##**This notebook can process the data generated by the twoVmWorkflowOnTerra and push to Google Cloud storage buckets.**\n",
        "\n",
        "The steps are:\n",
        "- The lz4 compressed DICOM SEG and SR files are downloaded from the Terra Workspace bucket\n",
        "- They are decompressed by lz4 and moved to a temporary directory\n",
        "- The uncompressed DICOM SEG and SR files are then pushed to Google Cloud Storage buckets, which can then be imported by DICOM store.\n",
        "- The process is iterated for each batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azyde9c3u5m6"
      },
      "source": [
        "##**Install lz4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QjxKglcr8Bd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt-get update \\\n",
        "  && apt-get install -y --no-install-recommends \\\n",
        "    lz4\\\n",
        "  && rm -rf /var/lib/apt/lists/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xqqgm_cuS46"
      },
      "source": [
        "##**Authenticate gcloud**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDVwFs1kujv-"
      },
      "outputs": [],
      "source": [
        "project_id='my-test-project'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftl5mKOSqe0N"
      },
      "outputs": [],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGRyvQDQqm6M"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project $project_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V06BwDUu_Wu"
      },
      "source": [
        "##**Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXuCtyZ3qTNA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pshMjPAvGUG"
      },
      "source": [
        "##**Load the terra table containing links to artifacts generated by the twoVmWorkflow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl6rWwzxrTZf"
      },
      "outputs": [],
      "source": [
        "data= pd.read_table('twoVM_2024_01_12.tsv')\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IA9rJllwkSJ"
      },
      "source": [
        "##**Decompress DICOM SEG files and push to Cloud Storage buckets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZXWK9jJ57WT",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dicom_seg_download_urls=data['dicomsegAndRadiomicsSR_CompressedFiles'].to_list()\n",
        "dicom_seg_download_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XdS3vGuf_B4"
      },
      "outputs": [],
      "source": [
        "!wget \"https://github.com/peak/s5cmd/releases/download/v2.2.2/s5cmd_2.2.2_Linux-64bit.tar.gz\"\n",
        "!tar -xvzf \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "&& rm \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "&& mv s5cmd /usr/local/bin/s5cmd\\\n",
        "&& rm CHANGELOG.md LICENSE README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XVHctV1iCO3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if the .aws directory exists, and if not, create it\n",
        "aws_dir = os.path.expanduser('~/.aws')\n",
        "if not os.path.exists(aws_dir):\n",
        "    os.makedirs(aws_dir)\n",
        "\n",
        "# Now, try writing to the credentials file\n",
        "credentials_file = os.path.join(aws_dir, 'credentials')\n",
        "with open(credentials_file, 'w') as f:\n",
        "    f.write('[default]\\n')\n",
        "    f.write('aws_access_key_id=\\n')\n",
        "    f.write('aws_secret_access_key=')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "796xToVj0hfQ"
      },
      "outputs": [],
      "source": [
        "destination_bucket_name='total_segmentator_nlst_sample_011224'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffGIRZ1V8wNO",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for url in tqdm(dicom_seg_download_urls):\n",
        "    print('processing url:'+url)\n",
        "    try:\n",
        "      shutil.rmtree(f'dicom_seg_objects')\n",
        "      shutil.rmtree(f'itkimage2segimage')\n",
        "    except OSError:\n",
        "      pass\n",
        "    try:\n",
        "        os.mkdir(f'dicom_seg_objects')\n",
        "        s3_url = url.replace('gs://', 's3://')\n",
        "        !s5cmd --endpoint-url https://storage.googleapis.com cp --show-progress {s3_url} .\n",
        "        !lz4 -d --rm dicomsegAndRadiomicsSR_DICOMsegFiles.tar.lz4 -c | tar --strip-components=0  -xvf - > /dev/null 2>&1\n",
        "        !find ./itkimage2segimage -name '*.dcm.lz4' -exec mv -t dicom_seg_objects {} + > /dev/null 2>&1\n",
        "        !lz4 -d -m --rm \"dicom_seg_objects\"/*.lz4 > /dev/null 2>&1\n",
        "        #!gsutil -m cp -r dicom_seg_objects/* gs://$destination_bucket_name/DICOM_SEGS/ > /dev/null 2>&1\n",
        "        !s5cmd --endpoint-url https://storage.googleapis.com cp --show-progress \"dicom_seg_objects/*\" \"s3://$destination_bucket_name/DICOM_SEGS/\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing {url}: {e}')\n",
        "        traceback.print_exc()\n",
        "shutil.rmtree(f'dicom_seg_objects')\n",
        "shutil.rmtree(f'itkimage2segimage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9BxxuL3y4pB"
      },
      "source": [
        "##**Decompress DICOM SR files and push to Cloud Storage buckets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-XxLXORrDya",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "sr_download_urls=data['structuredReportsDICOM'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV3a-LPbrDyb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "series_df = pd.DataFrame(columns=['series_id'])\n",
        "\n",
        "batch_count = 1  # Counter for batch folders\n",
        "\n",
        "for url in tqdm(sr_download_urls):\n",
        "    print('processing url:' + url)\n",
        "    try:\n",
        "        shutil.rmtree(f'structuredReportsDICOM')\n",
        "        shutil.rmtree(f'decompressedStructuredReportsDICOM')\n",
        "    except OSError:\n",
        "        pass\n",
        "    os.mkdir(f'decompressedStructuredReportsDICOM')\n",
        "    try:\n",
        "        s3_url = url.replace('gs://', 's3://')\n",
        "        print(s3_url)\n",
        "        !s5cmd --endpoint-url https://storage.googleapis.com cp {s3_url} .\n",
        "        !lz4 -d --rm structuredReportsDICOM.tar.lz4 -c | tar --strip-components=0 -xvf - > /dev/null 2>&1\n",
        "        !find ./structuredReportsDICOM -name '*.dcm.lz4' -exec mv -t decompressedStructuredReportsDICOM {} + > /dev/null 2>&1\n",
        "        !lz4 -d -m --rm \"decompressedStructuredReportsDICOM\"/*.lz4 > /dev/null 2>&1\n",
        "        !s5cmd --endpoint-url https://storage.googleapis.com cp --show-progress \"decompressedStructuredReportsDICOM/*\" \"s3://$destination_bucket_name/decompressedStructuredReportsDICOM/batch_{batch_count}/\"\n",
        "\n",
        "        # Find all series IDs and add them to the DataFrame\n",
        "        series_ids = [filename.split('_')[0] for filename in os.listdir('decompressedStructuredReportsDICOM')]\n",
        "        url_series_df = pd.DataFrame({'series_id': series_ids})\n",
        "\n",
        "        # Append the current DataFrame to the main DataFrame\n",
        "        series_df = pd.concat([series_df, url_series_df], ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing {url}: {e}')\n",
        "        traceback.print_exc()\n",
        "\n",
        "#the below steps are for importing dicom files to dicom store from command line.\n",
        "    # try:\n",
        "    #     # Upload shape_sr.dcm files in the batch\n",
        "    #     gcs_uri = f\"gs://$destination_bucket_name/decompressedStructuredReportsDICOM/batch_{batch_count}/*.dcm\"\n",
        "    #     !gcloud healthcare dicom-stores import gcs 10k-series --dataset=total_segmentator_nlst_sample_061023 --location=us-central1 --gcs-uri={gcs_uri}\n",
        "    # except Exception as e:\n",
        "    #     print(f'Error processing {url}: {e}')\n",
        "    #     traceback.print_exc()\n",
        "\n",
        "    # Increment the batch counter\n",
        "    batch_count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYWOiib7rDyc"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(f'structuredReportsDICOM')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
