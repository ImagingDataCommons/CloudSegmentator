{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "view-in-github",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/CloudSegmentator/blob/v1.2.0/workflows/TotalSegmentator/Notebooks/preProccessing_of_postProcessingExtractPerframe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qWeL6RS4pVtu",
      "metadata": {
        "id": "qWeL6RS4pVtu"
      },
      "source": [
        "## **This notebook provides a step-by-step guide on how to generate a datatable for Terra. This datatable is essential for extracting the DICOM attribute, PerFrameFunctionalGroupsSequence, using the workflow linked below.**\n",
        "\n",
        "You can find the PerFrameFunctionalGroupsSequence Extraction workflow [here](https://dockstore.org/workflows/github.com/ImagingDataCommons/CloudSegmentator/perFrameFunctionalGroupSequenceExtractionOnTerra:main?tab=info).\n",
        "\n",
        "The workflow requires list of paths to the lz4 compressed DICOM SEG objects generated by our TotalSegmentator workflows like [here](https://github.com/ImagingDataCommons/CloudSegmentator/blob/v1.2.0/workflows/TotalSegmentator/Notebooks/dicomsegAndRadiomicsSR_Notebook.ipynb). While running the workflow on Cloud, each VM is assigned 10 (chosen arbitarily can be any number) batches of compressed DICOM SEGs, amounting to up to 120 DICOM SEG files.\n",
        "\n",
        "Once these steps are completed, a datatable is produced and is ready to be uploaded to Terra's data tables, that can be referenced for  PerFrameFunctionalGroupsSequence Extraction workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-LdyuHrrPigD",
      "metadata": {
        "id": "-LdyuHrrPigD"
      },
      "source": [
        "### **Installing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "B5KuCCFhOgpA",
      "metadata": {
        "id": "B5KuCCFhOgpA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt-get update \\\n",
        "  && apt-get install -y --no-install-recommends \\\n",
        "  lz4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "B0TrOX90OhV_",
      "metadata": {
        "id": "B0TrOX90OhV_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pydicom \\\n",
        "   google-cloud-bigquery \\\n",
        "   pyarrow \\\n",
        "   db_dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73LFRISZPn6Q",
      "metadata": {
        "id": "73LFRISZPn6Q"
      },
      "source": [
        "### **Importing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "db4585bb-9bfb-4de3-882a-c7454c7ec3af",
      "metadata": {
        "id": "db4585bb-9bfb-4de3-882a-c7454c7ec3af"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import traceback\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1mqxvpePhxA",
      "metadata": {
        "id": "d1mqxvpePhxA"
      },
      "source": [
        "### **Example Terra datatable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Bri49R5rsPsW",
      "metadata": {
        "id": "Bri49R5rsPsW"
      },
      "outputs": [],
      "source": [
        "segFilesCsv='https://github.com/ImagingDataCommons/CloudSegmentator/releases/download/v1.0.0/sample_manifest_for_perframe.tsv'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fU_QT-Dnsakj",
      "metadata": {
        "id": "fU_QT-Dnsakj"
      },
      "source": [
        "### **Read the tsv from twoVMworkflow datatable on terra**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kgt7OeFzP1BX",
      "metadata": {
        "id": "Kgt7OeFzP1BX"
      },
      "outputs": [],
      "source": [
        "data= pd.read_table(segFilesCsv)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de-zRHR4InP8",
      "metadata": {
        "id": "de-zRHR4InP8"
      },
      "source": [
        "### **Generate manifests for Terra datatable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N0dTxO-P6TQx",
      "metadata": {
        "id": "N0dTxO-P6TQx"
      },
      "outputs": [],
      "source": [
        "# Set the number of rows per file\n",
        "batches_per_row = 10\n",
        "\n",
        "# Sort the dataframe by the first non-index column (assuming it's 'batch_id')\n",
        "df = data.sort_values(by=data.columns[0])\n",
        "\n",
        "# Calculate the number of files needed\n",
        "num_files = math.ceil(len(df) / batches_per_row)\n",
        "\n",
        "# Split the dataframe into multiple dataframes\n",
        "dfs = [df[i*batches_per_row:(i+1)*batches_per_row] for i in range(num_files)]\n",
        "\n",
        "# Get the current date and time formatted with underscores up to minutes\n",
        "now = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
        "\n",
        "# Set the directory for the manifests\n",
        "manifests_dir = 'manifests'\n",
        "\n",
        "# Make sure the directory exists\n",
        "os.makedirs(manifests_dir, exist_ok=True)\n",
        "\n",
        "# Create a new column name for the batch_id column\n",
        "batch_id_column = f'entity:perFrameExtraction_{now}_id'\n",
        "\n",
        "# Create a new dataframe to store the batch information\n",
        "batch_df = pd.DataFrame(columns=[batch_id_column, 'dicomsegAndRadiomicsSR_CompressedFiles'])\n",
        "\n",
        "# Analyze each file and add a row to the batch dataframe\n",
        "for i, df_batch in enumerate(dfs):\n",
        "    # Create a list of segFiles for this batch\n",
        "    segFiles_list = df_batch['dicomsegAndRadiomicsSR_CompressedFiles'].tolist()\n",
        "\n",
        "    # Convert the list to a JSON string with double quotes\n",
        "    segFiles_json = json.dumps(segFiles_list)\n",
        "\n",
        "    # Create a new row with the batch information and the segFiles list\n",
        "    new_row = pd.DataFrame({\n",
        "        batch_id_column: [i+1],\n",
        "        'dicomsegAndRadiomicsSR_CompressedFiles': [segFiles_json],\n",
        "    })\n",
        "    # Add the new row to the batch dataframe\n",
        "    batch_df = pd.concat([batch_df, new_row], ignore_index=True)\n",
        "\n",
        "batch_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "xq6iyJqf1guv",
      "metadata": {
        "id": "xq6iyJqf1guv"
      },
      "outputs": [],
      "source": [
        "batch_df.to_csv(f'perframe_datatable_{now}.tsv',sep=\"\\t\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
