{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/CloudSegmentator/blob/main/workflows/TotalSegmentator/Notebooks/downloadDicomAndConvertAndInferenceTotalSegmentatorNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QhtMLIRPQsH"
      },
      "source": [
        "# **This Notebook can download CT data from Imaging Data Commons and convert to NIfTI with dcm2niix, perform Inference using TotalSegmentator(v1.5.6) and produces multilabel Segmentation Maps NIfTI file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vZY9hXWDEMT"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/Docs/images/download_convert_inference_totalseg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6wX5zMpPYn8"
      },
      "source": [
        "DICOM files are downloaded from IDC and converted to NIFTI files with dcm2niix. Whenever there are multiple NIFTI files generated for a series, such series are removed from final output. A CSV file is created with a list of such series. Inference is then performed using TotalSegmentator (v1.5.6) resulting in multilabeled Segmentation Map in NIfTI format, compressed by lz4\n",
        "\n",
        "\n",
        "Please cite:\n",
        "\n",
        "Jakob Wasserthal, Manfred Meyer, Hanns-Christian Breit, Joshy Cyriac, Shan Yang, & Martin Segeroth. (2022). TotalSegmentator: robust segmentation of 104 anatomical structures in CT images. https://doi.org/10.48550/arXiv.2208.05868\n",
        "\n",
        "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat Methods 18, 203â€“211 (2021). https://doi.org/10.1038/s41592-020-01008-z\n",
        "\n",
        "Li X, Morgan PS, Ashburner J, Smith J, Rorden C. (2016) The first step for neuroimaging data analysis: DICOM to NIfTI conversion. J Neurosci Methods. 264:47-56.\n",
        "\n",
        "Fedorov A, Longabaugh WJR, Pot D, Clunie DA, Pieper SD, Gibbs DL, Bridge C, Herrmann MD, Homeyer A, Lewis R, Aerts HJWL, Krishnaswamy D, Thiriveedhi VK, Ciausu C, Schacherer DP, Bontempi D, Pihl T, Wagner U, Farahani K, Kim E, Kikinis R. National Cancer Institute Imaging Data Commons: Toward Transparency, Reproducibility, and Scalability in Imaging Artificial Intelligence. Radiographics. 2023 Dec;43(12):e230180. doi: 10.1148/rg.230180. PMID: 37999984; PMCID: PMC10716669."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU_rtptJDEMT"
      },
      "source": [
        "### **Ways to utilize this notebook**\n",
        "\n",
        "\n",
        "*   **Colab**\n",
        "*   **DockerContainer/Terra/SB-CGC**\n",
        "\n",
        "\n",
        "#### **Colab**\n",
        "*  This notebook was initally developed and tested on Colab, and a working version is saved on github, however reproducibility may not be guaranteed as the run time environment changes with colab updates\n",
        "*  To run this notebook with Colab, Click 'Open In Colab' icon on top left\n",
        "*  In 'interactive' mode, a list of seriesInstanceUIDs are chosen by default but a user may modify them to their use case\n",
        "*  Run each cell to install the packages and to download the data from IDC, convert to NIfTI, perform inference using TotalSegmentator and resulting segmentation maps are saved in lz4 compressed format\n",
        "\n",
        "#### **Docker**\n",
        "* This notebook is saved by default in a way that's amenable to be used on Terra/SB-CGC platforms using Docker\n",
        "* Running this notebook in a docker container ensures reproduciblity, as we lock the run environment beginning from the base docker image to pip packages in the docker image\n",
        "* Docker images can be found @ https://hub.docker.com/repository/docker/imagingdatacommons/download_convert_inference_totalseg/tags\n",
        "* The link to dockerfile along with git commit hash used for building the docker image can be found in one of the layers called 'LABEL'\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/Docs/images/download_convert_inference_totalseg_docker.png\">\n",
        "\n",
        "* We use a python package called Papermill, that can run the notebook with out having to convert it to python script. This allows us maintain one copy of code instead of two.\n",
        "* A sample papermill command is\n",
        "    <pre>\n",
        "    papermill downloadDicomAndConvertAndInferenceTotalSegmentatorNotebook.ipynb outputdownloadDicomAndConvertAndInferenceTotalSegmentatorNotebook.ipynb -y SeriesInstanceUIDs yamlListOfSeriesInstanceUIDs\n",
        "    </pre>\n",
        "    * refer to https://papermill.readthedocs.io/en/latest/usage-execute.html#note-about-using-yaml on how to pass the yaml list\n",
        "\n",
        "    Instead of passing the yaml list as a string, a yaml file can also be passed, as we do for SB-CGC\n",
        "    [an example can be found here](!https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/Docs/sampleManifests/batch_1.yaml)\n",
        "    <pre>\n",
        "    papermill downloadDicomAndConvertAndInferenceTotalSegmentatorNotebook.ipynb outputdownloadDicomAndConvertAndInferenceTotalSegmentatorNotebook.ipynb -f path_to_yamlListOfSeriesInstanceUIDs.yaml\n",
        "    </pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jcGevLmr3JW"
      },
      "source": [
        "### **Installing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0BAKG7AOI9a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !sudo apt-get update && apt-get install -y --no-install-recommends \\\n",
        "        build-essential\\\n",
        "        dcm2niix\\\n",
        "        ffmpeg\\\n",
        "        lz4\\\n",
        "        pigz\\\n",
        "        #plastimatch=1.8.0+dfsg.1-2build1\\\n",
        "        python3-dev\\\n",
        "        python3-pip\\\n",
        "        unzip\\\n",
        "        wget\\\n",
        "        xvfb\\\n",
        "        zip\\\n",
        "        && rm -rf /var/lib/apt/lists/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R68Zs8VTrvKg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in sys.modules:\n",
        "    !sudo pip install --no-cache-dir\\\n",
        "        idc_index==0.2.8\\\n",
        "        ipykernel==6.22.0\\\n",
        "        ipython==8.12.0\\\n",
        "        ipywidgets==8.0.6\\\n",
        "        jupyter==1.0.0\\\n",
        "        nvidia-ml-py3==7.352.0\\\n",
        "        papermill==2.4.0 \\\n",
        "        requests==2.27.1\\\n",
        "        TotalSegmentator==1.5.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26pJX1K1UnBy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in sys.modules:\n",
        "    #install dcm2niix\n",
        "    !wget \"https://github.com/rordenlab/dcm2niix/releases/download/v1.0.20230411/dcm2niix_lnx.zip\" \\\n",
        "        && unzip \"dcm2niix_lnx.zip\" \\\n",
        "        && rm \"dcm2niix_lnx.zip\" \\\n",
        "        && mv dcm2niix /usr/local/bin/dcm2niix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_Ms6GwHHOr2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in sys.modules:\n",
        "    #install s5cmd\n",
        "    !wget \"https://github.com/peak/s5cmd/releases/download/v2.2.2/s5cmd_2.2.2_Linux-64bit.tar.gz\"\n",
        "    !tar -xvzf \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\n",
        "    !rm \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\n",
        "    !mv s5cmd /usr/local/bin/s5cmd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2t6X63jKDSE"
      },
      "source": [
        "### **Importing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16RKHQCnIM2V"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from datetime import datetime\n",
        "from idc_index import index\n",
        "from pathlib import Path\n",
        "from time import sleep\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import nvidia_smi\n",
        "import os\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MfHJ_KtXmus"
      },
      "source": [
        "### **Current Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zkBscf3pxcO"
      },
      "outputs": [],
      "source": [
        "curr_dir   = Path().absolute()\n",
        "\n",
        "os.environ['TZ'] = 'US/Eastern'\n",
        "time.tzset()\n",
        "current_time = time.strftime('%a %b %d %H:%M:%S %Y', time.localtime())\n",
        "print(current_time)\n",
        "print(\"\\nCurrent directory :{}\".format( curr_dir))\n",
        "print(\"Python version    :\", sys.version.split('\\n')[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV0u5M-LXmus"
      },
      "source": [
        "### **Initialize IDC Client**\n",
        "\n",
        "We use idc-client pypi package to handle downloading data from IDC. \n",
        "In this notebook, we are using version 0.2.8 which contains the index from idc version 17\n",
        "\n",
        "Learn more about idc-index at ttps://github.com/ImagingDataCommons/idc-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2D0HTAUXmus"
      },
      "outputs": [],
      "source": [
        "idc_client=index.IDCClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg6rEeM6tuQJ",
        "tags": []
      },
      "source": [
        "### **Local testing**\n",
        "By default, in interactive mode, a list of seriesInstanceUIDs are chosen  here. However, you can modify them to your usecase.\n",
        "\n",
        "Below cell is also tagged as `parameters`, so that when running this notebook in non interactive mode on Terra or Seven Bridges Genomics- Cancer Genomics Cloud platforms, papermill will inject a cell to pass the yaml list of SeriesInstanceUIDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHaIwv0stya4",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  SeriesInstanceUIDs=['1.3.6.1.4.1.14519.5.2.1.7009.9004.100143549999116733615345241533',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100148350742920339334061834697',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100241427395754063917290539621',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100266844261017577841946689119',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100554983367710060268444607770',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100609092457306482866656779396',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100766949446324115207263771273',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100807683971079396484581858934',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100898243941555041874482639283',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100962349324934756610763981593',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.100993923831797845405183790593',\n",
        "                      '1.3.6.1.4.1.14519.5.2.1.7009.9004.101126369092366339550409924127'\n",
        "                      ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGbSvle4RSpd"
      },
      "source": [
        "### **Defining Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqeESHc7ntX8"
      },
      "outputs": [],
      "source": [
        "#create directory for TotalSegmentator Output files\n",
        "try:\n",
        "  shutil.rmtree('dcm2niix')\n",
        "  shutil.rmtree('Inference')\n",
        "\n",
        "except OSError:\n",
        "  pass\n",
        "os.mkdir('dcm2niix')\n",
        "os.mkdir('Inference')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1nH_ABkKoP2"
      },
      "outputs": [],
      "source": [
        "def download_dicom_data(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Downloads raw DICOM data\n",
        "\n",
        "    Args:\n",
        "    series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "    download_directory = f\"idc_data/{series_id}\"\n",
        "    # Attempt to remove the directory for the series if it exists\n",
        "    try:\n",
        "        shutil.rmtree(download_directory)\n",
        "    except OSError:\n",
        "        pass\n",
        "    print(f'\\n Downloading DICOM files from IDC Storage Buckets \\n')\n",
        "    idc_client.download_dicom_series(seriesInstanceUID= series_id, downloadDir=download_directory, quiet=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_NSBTmVXmuu"
      },
      "outputs": [],
      "source": [
        "def is_series_CT(series_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Gets the image modality for the corresponding seriesInstanceUID from idc-index\n",
        "    Refer to this query for additional columns available in idc-index!\n",
        "\n",
        "    https://github.com/ImagingDataCommons/idc-index/blob/main/queries/idc_index.sql\n",
        "\n",
        "    Args:\n",
        "    series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be processed.\n",
        "    \"\"\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "    Modality\n",
        "    FROM index\n",
        "    WHERE SeriesInstanceUID = '{series_id}'\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        modality_df = idc_client.sql_query(query)\n",
        "        if not modality_df.empty:\n",
        "            modality = modality_df['Modality'][0]\n",
        "            if modality=='CT':\n",
        "              return True\n",
        "            else:\n",
        "              log_modality_errors(series_id)\n",
        "              return False\n",
        "        else:\n",
        "            log_modality_errors(series_id)\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        log_modality_errors(series_id)\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "177tERHBXmuu"
      },
      "outputs": [],
      "source": [
        "def log_modality_errors(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Logs an error when the modality is not CT for a given series.\n",
        "\n",
        "    Args:\n",
        "        series_id: The ID of the series.\n",
        "    \"\"\"\n",
        "    # Open the log file in append mode\n",
        "    with open(\"modality_error_file.txt\", \"a\") as f:\n",
        "        # Write the error message to the file\n",
        "        f.write(f\"Error: Modality is not CT for series {series_id}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2jkSpS2KvXU"
      },
      "outputs": [],
      "source": [
        "def convert_dicom_to_nifti(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Converts a DICOM series to a NIfTI file.\n",
        "\n",
        "    Args:\n",
        "      series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "\n",
        "    # Attempt to remove the directory for the series if it exists\n",
        "    try:\n",
        "        shutil.rmtree(f\"dcm2niix/{series_id}\")\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    # Create a new directory for the series\n",
        "    os.mkdir(f\"dcm2niix/{series_id}\")\n",
        "\n",
        "    print(\"\\n Converting DICOM files to NIfTI \\n\")\n",
        "\n",
        "    # Run the appropriate converter command and capture the output\n",
        "\n",
        "    result = subprocess.run(\n",
        "        f\"dcm2niix -z y -f %j_%p_%t_%s -b n -m y -o dcm2niix/{series_id} idc_data/{series_id}\",\n",
        "        shell=True,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "    )\n",
        "    print(result.stdout)\n",
        "    print(\"\\n Conversion successful\")\n",
        "    #cleaning up the dicom files directory as they are no longer needed\n",
        "    shutil.rmtree(f'idc_data/{series_id}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWU6l2i-V2Q7"
      },
      "outputs": [],
      "source": [
        "def check_dicom_conversion_errors(series_id_folder_path):\n",
        "    \"\"\"\n",
        "    This function checks if the conversion from DICOM to NIfTI format was successful.\n",
        "    It does this by checking the number of files in the specified folder.\n",
        "    The conversion is considered successful if there is exactly one file in the folder.\n",
        "\n",
        "    Args:\n",
        "    series_id_folder_path (str): The path of the folder containing the converted NIfTI files.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if there was an error in the conversion\n",
        "    (i.e., no files or more than one file in the folder), False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get a list of all files in the specified folder\n",
        "    nifti_files = os.listdir(series_id_folder_path)\n",
        "\n",
        "    # Check if the folder is empty\n",
        "    if len(nifti_files) == 0:\n",
        "        # If the folder is empty, log an error message in 'error_file.txt'\n",
        "        # This indicates that no file was created during the conversion, which means an error occurred\n",
        "        with open('error_file.txt', 'a') as f:\n",
        "            f.write(f\"Error: No files in {series_id_folder_path}\\n\")\n",
        "        # Return True to indicate an error\n",
        "        return True\n",
        "\n",
        "    # Check if the folder contains more than one file\n",
        "    elif len(nifti_files) > 1:\n",
        "        # If the folder contains more than one file, log an error message in 'error_file.txt'\n",
        "        # This indicates that more than one file was created during the conversion, which should not happen and thus means an error occurred\n",
        "        with open('error_file.txt', 'a') as f:\n",
        "            f.write(f\"Error: More than one file in {series_id_folder_path}\\n\")\n",
        "        # Return True to indicate an error\n",
        "        return True\n",
        "\n",
        "    # If there is exactly one file in the folder, no error occurred during the conversion\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZYXfp-UtWMs"
      },
      "outputs": [],
      "source": [
        "def check_total_segmentator_errors(series_id: str):\n",
        "  \"\"\"\n",
        "  This function checks if the output files from TotalSegmentator exist.\n",
        "\n",
        "  Args:\n",
        "  series_id (str): The DICOM Tag SeriesInstanceUID of the DICOM series to be checked.\n",
        "\n",
        "  Returns:\n",
        "  bool: True if any of the output files do not exist, False otherwise.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define the output files from TotalSegmentator\n",
        "  output_files = [f\"Inference/{series_id}/segmentations.nii\"]\n",
        "\n",
        "  # Check if all output files exist\n",
        "  if not all(os.path.exists(file) for file in output_files):\n",
        "      # If any of the output files do not exist, log an error\n",
        "      with open('totalsegmentator_errors.txt', 'a') as f:\n",
        "          f.write(f\"Error: TotalSegmentator failed for series {series_id}\\n\")\n",
        "      return True\n",
        "\n",
        "  return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10_jVVTElNZ5"
      },
      "outputs": [],
      "source": [
        "def inferenceTotalSegmentator(series_id: str,runtime_stats: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This function calls download_dicom_data and then performs inference using TotalSegmentator on a given series.\n",
        "\n",
        "    Args:\n",
        "    series_id (str): The DICOM Tag SeriesInstanceUID of the DICOM series to be processed.\n",
        "    runtime_stats: DataFrame to store runtime statistics.\n",
        "    \"\"\"\n",
        "    print(\"Processing series: \" + series_id)\n",
        "\n",
        "    if is_series_CT(series_id):\n",
        "\n",
        "        start_time = time.time()\n",
        "        download_dicom_data(series_id)\n",
        "        dicom_download_time = time.time() - start_time\n",
        "\n",
        "        start_time = time.time()\n",
        "        convert_dicom_to_nifti(series_id)\n",
        "        dicom_conversion_time = time.time() - start_time\n",
        "\n",
        "        # Remove existing directories and files if they exist\n",
        "        shutil.rmtree(f\"Inference/{series_id}\", ignore_errors=True)\n",
        "        shutil.rmtree(f\"metadata/{series_id}\", ignore_errors=True)\n",
        "        try:\n",
        "            os.remove(\"segmentations.nii.gz\")\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "        # Create a new directory for the series\n",
        "        os.makedirs(f\"Inference/{series_id}\", exist_ok=True)\n",
        "\n",
        "        series_id_folder_path = os.path.join(\"dcm2niix\", series_id)\n",
        "\n",
        "        if not check_dicom_conversion_errors(series_id_folder_path):\n",
        "            # Get the list of files in series_id_path\n",
        "            nifti_files = os.listdir(series_id_folder_path)\n",
        "            # Get the first (and only) file in the list\n",
        "            nifti_filename = nifti_files[0]\n",
        "            # Get the full path of the file\n",
        "            nifti_filename_path = os.path.join(series_id_folder_path, nifti_filename)\n",
        "\n",
        "            start_time = time.time()\n",
        "            result = subprocess.run(\n",
        "                [\n",
        "                    \"TotalSegmentator\",\n",
        "                    \"-i\",\n",
        "                    nifti_filename_path,\n",
        "                    \"-o\",\n",
        "                    \"segmentations\",\n",
        "                    \"--ml\",\n",
        "                ],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                universal_newlines=True,\n",
        "            )\n",
        "            print(result.stdout)\n",
        "            total_segmentator_time = time.time() - start_time\n",
        "\n",
        "            try:\n",
        "                subprocess.run(\n",
        "                    [\"mv\", \"segmentations.nii\", f\"Inference/{series_id}/\"], check=True\n",
        "                )\n",
        "                print(\"Files moved successfully using the first command\")\n",
        "            except subprocess.CalledProcessError:\n",
        "                try:\n",
        "                    subprocess.run(\n",
        "                        [\n",
        "                            \"mv\",\n",
        "                            \"segmentations/segmentations.nii\",\n",
        "                            f\"Inference/{series_id}/\",\n",
        "                        ],\n",
        "                        check=True,\n",
        "                    )\n",
        "                    print(\"Files moved successfully using the second command\")\n",
        "                except subprocess.CalledProcessError:\n",
        "                    print(\"Error: Failed to move files using both commands\")\n",
        "\n",
        "            if not check_total_segmentator_errors(series_id):\n",
        "\n",
        "                shutil.move(\n",
        "                    f\"Inference/{series_id}/segmentations.nii\",\n",
        "                    f\"Inference/{series_id}/{series_id}.nii\",\n",
        "                )\n",
        "\n",
        "                start_time = time.time()\n",
        "                subprocess.run(\n",
        "                    [\n",
        "                        \"lz4\",\n",
        "                        \"--rm\",\n",
        "                        f\"Inference/{series_id}/{series_id}.nii\",\n",
        "                        f\"Inference/{series_id}/{series_id}.nii.lz4\",\n",
        "                    ],\n",
        "                    check=True,\n",
        "                )\n",
        "\n",
        "                archiving_time = time.time() - start_time\n",
        "            else:\n",
        "                archiving_time = 0  \n",
        "        else:\n",
        "            total_segmentator_time = 0\n",
        "            archiving_time = 0\n",
        "\n",
        "    else:\n",
        "        dicom_download_time = 0\n",
        "        dicom_conversion_time = 0\n",
        "        total_segmentator_time = 0\n",
        "        archiving_time = 0\n",
        "\n",
        "    log = pd.DataFrame({\"SeriesInstanceUID\": [series_id]})\n",
        "    log[\"dicom_download_time\"] = dicom_download_time\n",
        "    log[\"dicom_conversion_time\"] = dicom_conversion_time\n",
        "    log[\"total_segmentator_time\"] = total_segmentator_time\n",
        "    log[\"archiving_time\"] = archiving_time\n",
        "\n",
        "    shutil.rmtree(f\"dcm2niix/{series_id}\", ignore_errors=True)\n",
        "\n",
        "    runtime_stats = pd.concat([runtime_stats, log], ignore_index=True, axis=0)\n",
        "\n",
        "    return runtime_stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGQ_URr1pQpj"
      },
      "outputs": [],
      "source": [
        "class MemoryMonitor:\n",
        "    def __init__(self):\n",
        "        self.keep_measuring = True\n",
        "        self.working_disk_path = self.get_working_disk_path()\n",
        "\n",
        "    def get_working_disk_path(self):\n",
        "        partitions = psutil.disk_partitions()\n",
        "        for partition in partitions:\n",
        "            if partition.mountpoint == '/':\n",
        "                return '/'\n",
        "            elif '/cromwell_root' in partition.mountpoint:\n",
        "                return '/cromwell_root'\n",
        "        return '/'  # Default to root directory if no specific path is found\n",
        "    def measure_usage(self):\n",
        "        cpu_usage = []\n",
        "        ram_usage_mb=[]\n",
        "        gpu_usage_mb=[]\n",
        "        disk_usage_all=[]\n",
        "        time_stamps = []\n",
        "        start_time = time.time()\n",
        "        while self.keep_measuring:\n",
        "            cpu = psutil.cpu_percent()\n",
        "            ram = psutil.virtual_memory()\n",
        "            disk_usage = psutil.disk_usage(self.working_disk_path)\n",
        "            disk_used = disk_usage.used / 1000 / 1000 / 1000\n",
        "            disk_total = disk_usage.total / 1000 / 1000 / 1000\n",
        "            ram_total_mb = psutil.virtual_memory().total / 1000 / 1000\n",
        "            ram_mb = (ram.total - ram.available) / 1000 / 1000\n",
        "            try:\n",
        "                nvidia_smi.nvmlInit()\n",
        "                handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "                info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "                gpu_type = nvidia_smi.nvmlDeviceGetName(handle)\n",
        "                gpu_total_mb = info.total/1000/1000\n",
        "                gpu_mb = info.used/1000/1000\n",
        "                nvidia_smi.nvmlShutdown()\n",
        "            except:\n",
        "                gpu_type = ''\n",
        "                gpu_total_mb = 0\n",
        "                gpu_mb = 0\n",
        "                \n",
        "            cpu_usage.append(cpu)\n",
        "            ram_usage_mb.append(ram_mb)\n",
        "            disk_usage_all.append(disk_used)\n",
        "            gpu_usage_mb.append(gpu_mb)\n",
        "            time_stamps.append(time.time()- start_time)\n",
        "            sleep(1)\n",
        "\n",
        "        return cpu_usage, ram_usage_mb, time_stamps, ram_total_mb, gpu_usage_mb, gpu_total_mb, gpu_type, disk_usage_all, disk_total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_L2B-sjRf_1"
      },
      "source": [
        "###Total Segmentator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZfAhcQopl59"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize a DataFrame to store runtime statistics\n",
        "    runtime_stats = pd.DataFrame(columns=['SeriesInstanceUID','total_segmentator_time','dicom_download_time','dicom_conversion_time',\n",
        "                                          'archiving_time', 'cpu_usage','ram_usage_mb', 'ram_total_mb',\n",
        "                                          'gpu_usage_mb', 'gpu_total_mb', 'gpu_type', 'disk_usage_all', 'disk_total'\n",
        "                                          ])\n",
        "\n",
        "    for series_id in SeriesInstanceUIDs:\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            monitor = MemoryMonitor()\n",
        "            mem_thread = executor.submit(monitor.measure_usage)\n",
        "            try:\n",
        "                proc_thread = executor.submit(inferenceTotalSegmentator, series_id, runtime_stats)\n",
        "                runtime_stats = proc_thread.result()\n",
        "            finally:\n",
        "                monitor.keep_measuring = False\n",
        "                cpu_usage, ram_usage_mb, time_stamps, ram_total_mb, gpu_usage_mb, gpu_total_mb, gpu_type, disk_usage_all, disk_total= mem_thread.result()\n",
        "\n",
        "                cpu_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[cpu_idx, runtime_stats.columns.get_loc('cpu_usage')] = [[cpu_usage]]\n",
        "\n",
        "                ram_usage_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_usage_mb_idx, runtime_stats.columns.get_loc('ram_usage_mb')] = [[ram_usage_mb]]\n",
        "\n",
        "                ram_total_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_total_mb_idx, runtime_stats.columns.get_loc('ram_total_mb')] = [[ram_total_mb]]\n",
        "\n",
        "                gpu_total_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[gpu_total_mb_idx, runtime_stats.columns.get_loc('gpu_total_mb')] = [[gpu_total_mb]]\n",
        "\n",
        "                gpu_usage_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[gpu_usage_mb_idx, runtime_stats.columns.get_loc('gpu_usage_mb')] = [[gpu_usage_mb]]\n",
        "\n",
        "                disk_usage_gb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[disk_usage_gb_idx, runtime_stats.columns.get_loc('disk_usage_all')] = [[disk_usage_all]]\n",
        "\n",
        "                runtime_stats['gpu_type']=gpu_type\n",
        "                runtime_stats['disk_total']=disk_total\n",
        "\n",
        "                fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8, 6))\n",
        "\n",
        "                ax1.plot(time_stamps, cpu_usage)\n",
        "                ax1.set_ylim(0, 100)\n",
        "                ax1.set_xlabel('Time (s)')\n",
        "                ax1.set_ylabel('CPU usage (%)')\n",
        "\n",
        "                ax2.plot(time_stamps, ram_usage_mb)\n",
        "                ax2.set_ylim(0, ram_total_mb)\n",
        "                ax2.set_xlabel('Time (s)')\n",
        "                ax2.set_ylabel('Memory usage (MB)')\n",
        "\n",
        "                ax3.plot(time_stamps, gpu_usage_mb)\n",
        "                ax3.set_ylim(0, gpu_total_mb)\n",
        "                ax3.set_xlabel('Time (s)')\n",
        "                ax3.set_ylabel('GPU Memory usage (MB)')\n",
        "\n",
        "                ax4.plot(time_stamps, disk_usage_all)\n",
        "                ax4.set_ylim(0, disk_total)\n",
        "                ax4.set_xlabel('Time (s)')\n",
        "                ax4.set_ylabel('Disk usage (GB)')\n",
        "                plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzCG2YATRtVH"
      },
      "source": [
        "### **Compressing Output Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxuuD6yqdmzF"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "try:\n",
        "  os.remove('inferenceNiftiFiles.tar.lz4')\n",
        "  os.remove('metadata.tar.lz4')\n",
        "except OSError:\n",
        "  pass\n",
        "!tar cvf - -C {curr_dir} Inference | lz4 > inferenceNiftiFiles.tar.lz4\n",
        "\n",
        "output_file_archiving_time = time.time() - start_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4pMpClKRzEe"
      },
      "source": [
        "### **Utilization Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc7Hep0lkJWd"
      },
      "outputs": [],
      "source": [
        "runtime_stats.to_csv('runtime.csv')\n",
        "runtime_stats['output_file_archiving_time']=output_file_archiving_time\n",
        "try:\n",
        "  os.remove('inferenceUsageMetrics.lz4')\n",
        "except OSError:\n",
        "  pass\n",
        "!lz4 {curr_dir}/runtime.csv inferenceUsageMetrics.lz4\n",
        "runtime_stats"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
