{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/CloudSegmentator/blob/main/workflows/TotalSegmentator/Notebooks/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgDs7gUHE-dh"
      },
      "source": [
        "##**How to Generate a Datatable for Terra to Run the TotalSegmentatortwoVmWorkflowOnTerra**\n",
        "\n",
        "This notebook provides a step-by-step guide on how to prepare a datatable for Terra that is compatible with the [TotalSegmentatortwoVmWorkflowOnTerra](https://dockstore.org/workflows/github.com/ImagingDataCommons/Cloud-Resources-Workflows/TotalSegmentatortwoVmWorkflowOnTerra:dev?tab=info) workflow. This workflow performs segmentation and feature extraction on DICOM images using two virtual machines (VMs) on Terra.\n",
        "\n",
        "The steps are:\n",
        "\n",
        "1. **Filter out localizer and inconsistent series**. Run an SQL query to exclude series that are localizer scans or have geometric inconsistencies from the cohort of interest.\n",
        "3. **Split the cohort into chunks**. Create batches of 12 series  (assigning 12 series per VM), so you can leverage Terra's parallel computing capabilities efficiently and run the workflow across thousands of VMs on Terra. Note: Rawls, the underlying engine of Terra, can run up to 3000 jobs and up to 28800 tasks (a job may contain multiple tasks) at a time.\n",
        "5. **Generate a Terra datatable**. Each row of datatable will have the batchid and list of seriesInstanceuids in a yaml form amenable to be passed to papermill\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tlXtbeCLeuY"
      },
      "source": [
        "##**Authenticate gcloud**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CakOUR9MAe3"
      },
      "outputs": [],
      "source": [
        "#project_id='my-test-project'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzVR2TF-KmRA"
      },
      "outputs": [],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1xYbz8CK5gZ"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project $project_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tNALARLLjp-"
      },
      "source": [
        "##**Download and run the sql query which removes localizer and geometrically inconsistent series**##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4t3HJSKLcQs"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/ImagingDataCommons/CloudSegmentator/main/workflows/TotalSegmentator/sqlQueries/nlstCohort.sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDaww2epJUef"
      },
      "outputs": [],
      "source": [
        "!cat nlstCohort.sql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuwCM_1ImNYI"
      },
      "source": [
        "###Run this command twice as the first time bq is run, it returns a initialization message.\n",
        "\n",
        "https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/issues/35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvFH7z5WKhzM"
      },
      "outputs": [],
      "source": [
        "!cat nlstCohort.sql | bq query --format=csv  --project_id=$project_id --max_rows=999999999 --use_legacy_sql=false > nlst_cohort.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV_Af9tVoeh5"
      },
      "source": [
        "##**Generate Batches of 12 series and a terra data table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-o79VfxlZfQ"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "df= pd.read_csv('nlst_cohort.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BD5fGfInGe6"
      },
      "outputs": [],
      "source": [
        "df['projected_batch_number']=np.ceil((df.index + 1) / 12)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xknI9qo3RqQz"
      },
      "outputs": [],
      "source": [
        "# Set the number of rows per file\n",
        "series_per_batch = 12\n",
        "\n",
        "# Calculate the number of files needed\n",
        "num_files = math.ceil(len(df) / series_per_batch)\n",
        "\n",
        "# Split the dataframe into multiple dataframes\n",
        "dfs = [df[i*series_per_batch:(i+1)*series_per_batch] for i in range(num_files)]\n",
        "\n",
        "# Get the current date and time formatted with underscores up to minutes\n",
        "now = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
        "\n",
        "# Create a new column name for the batch_id column\n",
        "batch_id_column = f'entity:twoVM_{now}_id'\n",
        "\n",
        "# Create a new dataframe to store the batch information\n",
        "batch_df = pd.DataFrame(columns=[batch_id_column, 'SeriesInstanceUIDs', 'idc-version', 'dicomSegAndSRcpu', 'dicomSegAndSRram'])\n",
        "\n",
        "# Analyze each file and add a row to the batch dataframe\n",
        "import json\n",
        "\n",
        "# Analyze each file and add a row to the batch dataframe\n",
        "for i, df in enumerate(dfs):\n",
        "    max_sopinstancecount = df['sopInstanceCount'].max()\n",
        "\n",
        "    if max_sopinstancecount >= 300:\n",
        "        cpu = 8\n",
        "        ram = 32\n",
        "    else:\n",
        "        cpu = 4\n",
        "        ram = 16\n",
        "\n",
        "    # Create a list of seriesInstanceuids for this batch\n",
        "    series_list = df['SeriesInstanceUID'].tolist()\n",
        "\n",
        "    # Format the series list as a JSON dictionary\n",
        "    json_series_dict = json.dumps({\"SeriesInstanceUIDs\": series_list})\n",
        "\n",
        "    # Create a new row with the batch information and the series list\n",
        "    new_row = pd.DataFrame({\n",
        "        batch_id_column: [i+1],\n",
        "        'SeriesInstanceUIDs': [json_series_dict],\n",
        "        'dicomSegAndSRcpu': [cpu],\n",
        "        'dicomSegAndSRram': [ram],\n",
        "        'idc-version': 'v17'\n",
        "    })\n",
        "    # Add the new row to the batch dataframe\n",
        "    batch_df = pd.concat([batch_df, new_row], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "batch_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQP6BC9heDgE"
      },
      "outputs": [],
      "source": [
        "# Import the necessary library\n",
        "import os\n",
        "\n",
        "# Set the directory for the manifests\n",
        "manifests_dir = 'manifests'\n",
        "\n",
        "# Make sure the directory exists\n",
        "os.makedirs(manifests_dir, exist_ok=True)\n",
        "\n",
        "# Write each batch to a separate CSV file\n",
        "for i, batch in enumerate(batch_df.iterrows()):\n",
        "    # Get the batch data\n",
        "    batch_data = batch[1]\n",
        "\n",
        "    # Convert the Series to a DataFrame and transpose it\n",
        "    batch_data_df = pd.DataFrame(batch_data).T\n",
        "\n",
        "    # Create the filename\n",
        "    filename = os.path.join(manifests_dir, f'batch_{i+1}.csv')\n",
        "\n",
        "    # Write the batch data to a CSV file\n",
        "    batch_data_df.to_csv(filename, index=False)\n",
        "\n",
        "batch_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THk0oLDol4A3"
      },
      "outputs": [],
      "source": [
        "batch_df.to_csv(f'terra_data_table_manifest_{now}.tsv', sep='\\t', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "X_Civfd1ciHp",
        "w1mv3rg8e7Tv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
