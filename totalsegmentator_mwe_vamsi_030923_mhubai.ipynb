{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmb_HiJv7Sit"
   },
   "source": [
    "# **ModelHub - Whole Body CT Segmentation**\n",
    "\n",
    "This notebook provides a minimal working example of TotalSegmentator, a tool for the segmentation of 104 anatomical structures from CT images. The model was trained using a wide range of imaging CT data of different pathologies from several scanners, protocols and institutions.\n",
    "\n",
    "We test TotalSegmentator by implementing an end-to-end (cloud-based) pipeline on publicly available whole body CT scans hosted on the [Imaging Data Commons (IDC)](https://portal.imaging.datacommons.cancer.gov/), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI model. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed by a wide variety of image types (from the area covered by the scan, to the presence of contrast and various types of artefacts).\n",
    "\n",
    "The way all the operations are executed - from pulling data, to data postprocessing, and the standardisation of the results - have the goal of promoting transparency and reproducibility. Furthermore, this notebook is part of [a collection of code, notebooks and Docker containers](https://github.com/AIM-Harvard/mhub/blob/main/mhub/totalsegmentator/notebooks/totalsegmentator_mwe.ipynb) we are developing with the goal of making a wide range of machine learning models for medicine available through a standardized I/O framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhTN-Q6jujt4"
   },
   "source": [
    "Please cite the following article if you use this code or pre-trained models:\n",
    "\n",
    "Wasserthal, J., Meyer, M., Breit, H.C., Cyriac, J., Yang, S. and Segeroth, M., 2022. TotalSegmentator: robust segmentation of 104 anatomical structures in CT images. arXiv preprint arXiv:2208.05868, [\n",
    "https://doi.org/10.48550/arXiv.2208.05868]( \t\n",
    "https://doi.org/10.48550/arXiv.2208.05868).\n",
    "\n",
    "The original code is published on\n",
    "[GitHub](https://github.com/wasserth/TotalSegmentator)  using the [Apache-2.0 license](https://github.com/wasserth/TotalSegmentator/blob/master/LICENSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS7cUQaJujzN"
   },
   "source": [
    "### **Disclaimer**\n",
    "\n",
    "The code and data of this repository are provided to promote reproducible research. They are not intended for clinical care or commercial use.\n",
    "\n",
    "The software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZnoRi9Y7nEB"
   },
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This demo notebook is intended to be run using a GPU.\n",
    "\n",
    "To access a free GPU on Colab:\n",
    "`Edit > Notebooks Settings`.\n",
    "\n",
    "From the dropdown menu under `Hardware accelerator`, select `GPU`. Let's check the Colab instance is indeed equipped with a GPU.\n",
    "\n",
    "This notebook when intended to use in a workflow in Terra or CGC-Seven bridges, can also be run with out gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvqytwpu0ZXX",
    "outputId": "973fe934-1bb1-4c89-f67e-9e364ccedc81"
   },
   "outputs": [],
   "source": [
    "# #install s5cmd\n",
    "# !wget \"https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_Linux-64bit.tar.gz\"\n",
    "# !tar -xvzf \"s5cmd_2.0.0_Linux-64bit.tar.gz\"\n",
    "# !rm \"s5cmd_2.0.0_Linux-64bit.tar.gz\"\n",
    "# !mv s5cmd /usr/local/bin/s5cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "non5qVLIcG4M",
    "outputId": "a2533449-b2a4-415a-bc2f-0e5257cc103c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# useful information\n",
    "curr_dir   = Path().absolute()\n",
    "curr_droid = !hostname\n",
    "curr_pilot = !whoami\n",
    "\n",
    "print(time.asctime(time.localtime()))\n",
    "print(\"\\nCurrent directory :{}\".format( curr_dir))\n",
    "print(\"Hostname          :\", curr_droid[-1])\n",
    "print(\"Username          :\", curr_pilot[-1])\n",
    "print(\"Python version    :\", sys.version.split('\\n')[0])\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Numpy version                : \", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PKq2z2jkVJz"
   },
   "source": [
    "The authentication to Google is necessary to run BigQuery queries.\n",
    "\n",
    "Every operation throughout the whole notebook (BigQuery, fetching data from the IDC buckets) is completely free. The only thing that is needed in order to run the notebook is the set-up of a Google Cloud project. In order for the notebook to work as intended, you will need to specify the name of the project in the cell after the authentication one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DMUqTOVF5WP"
   },
   "outputs": [],
   "source": [
    "# # when running on Terra or Seven bridges this cell can be commented out as \n",
    "# # we will not need any authentication to google cloud except when we want to use \n",
    "# # bigquery to download dicom files. In that case, authentication is taken care of \n",
    "# # by using a service account\n",
    "\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Y43S1F35h8m"
   },
   "outputs": [],
   "source": [
    "## from google.colab import files \n",
    "\n",
    "## from google.cloud import storage\n",
    "## from google.cloud import bigquery as bq\n",
    "\n",
    "## INSERT THE ID OF YOUR PROJECT HERE!\n",
    "## project_id = \"idc-external-030\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftNvkrON7MvK"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # utility to make yaml files easily editable in a notebook cell\n",
    "# !pip install yamlmagic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PExNNZv67Q4N"
   },
   "outputs": [],
   "source": [
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT7coYaGfd4t"
   },
   "source": [
    "Throughout this Colab notebook, for image pre-processing we will use [Plastimatch](https://plastimatch.org), a reliable and open source software for image computation. We will be running Plastimatch using the simple [PyPlastimatch](https://github.com/AIM-Harvard/pyplastimatch/tree/main/pyplastimatch) python wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NLhlFqyEgBf"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !apt install plastimatch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ILwxgbcfcpr",
    "outputId": "faf1245e-d831-4b1b-ac7f-c654b856bcac"
   },
   "outputs": [],
   "source": [
    "#check plastimatch was correctly installed\n",
    "!plastimatch --version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MibNJESjwrmT"
   },
   "source": [
    "Install Apache's subversion. \n",
    "\n",
    "We will use subversion to clone only specific subfolders of the mhub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpCgi-Wywr98"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !apt install subversion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrNlgYZJIxkf"
   },
   "source": [
    "---\n",
    "\n",
    "Start by cloning the AIMI hub repository on the Colab instance.\n",
    "\n",
    "The AIMI hub repository stores all the code we will use for pulling, preprocessing, processing, and postprocessing the data for this use case - as long as the others shared through AIMI hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dsPpD35w8RM"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !svn checkout https://github.com/MHubAI/mhubio/trunk/mhubio mhubio\n",
    "# !svn checkout https://github.com/MHubAI/mhubio/trunk/mhubio/utils/ymldicomseg mhubai/ymldicomseg\n",
    "# !svn checkout https://github.com/MHubAI/models/trunk/models/totalsegmentator mhubai/totalsegmentator\n",
    "\n",
    "!svn checkout https://github.com/vkt1414/mhubio/trunk/mhubio mhubio\n",
    "!svn checkout https://github.com/vkt1414/mhubio/trunk/mhubio/utils/ymldicomseg mhubai/ymldicomseg\n",
    "!svn checkout https://github.com/vkt1414/models/trunk/models/totalsegmentator mhubai/totalsegmentator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pcj1hYK7lzmA"
   },
   "source": [
    "To organise the DICOM data in a more common (and human-understandable) fashion after downloading those from the buckets, we will make use of [DICOMSort](https://github.com/pieper/dicomsort). \n",
    "\n",
    "DICOMSort is an open source tool for custom sorting and renaming of dicom files based on their specific DICOM tags. In our case, we will exploit DICOMSort to organise the DICOM data by `PatientID` and `Modality` - so that the final directory will look like the following:\n",
    "\n",
    "```\n",
    "data/raw/nsclc-radiomics/dicom/$PatientID\n",
    " └─── CT\n",
    "       ├─── $SOPInstanceUID_slice0.dcm\n",
    "       ├─── $SOPInstanceUID_slice1.dcm\n",
    "       ├───  ...\n",
    "       │\n",
    "      RTSTRUCT \n",
    "       ├─── $SOPInstanceUID_RTSTRUCT.dcm\n",
    "      SEG\n",
    "       └─── $SOPInstanceUID_RTSEG.dcm\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ztrKrzSm1WT"
   },
   "source": [
    "We will also use DCMQI for converting the resulting segmentation into standard DICOM SEG objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYf72nA8m0wh"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# dcmqi_release_url = \"https://github.com/QIICR/dcmqi/releases/download/v1.2.5/dcmqi-1.2.5-linux.tar.gz\"\n",
    "# dcmqi_download_path = \"/content/dcmqi-1.2.5-linux.tar.gz\"\n",
    "# dcmqi_path = \"/content/dcmqi-1.2.5-linux\"\n",
    "\n",
    "# !wget -O $dcmqi_download_path $dcmqi_release_url\n",
    "\n",
    "# !tar -xvf $dcmqi_download_path\n",
    "\n",
    "# !mv $dcmqi_path/bin/* /bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JBJmF7rmz1S"
   },
   "source": [
    "---\n",
    "\n",
    "Let's now install example-specific python dependencies we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CthT1kRuywEh"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# !pip install thedicomsort\n",
    "# !pip install pyplastimatch nnunet\n",
    "# !pip install TotalSegmentator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZOtlGtcmOq5"
   },
   "source": [
    "Provided everything was set up correctly, we can run the BigQuery query and get all the information we need to download the testing data from the IDC platform.\n",
    "\n",
    "For this specific use case, we are going to be working with the \"CT lymph nodes\" collection hosted on IDC - which groups a collections of series that are close to whole body CT scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxjiDNaPHxoO",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#JSONServiceAccountFile='graceful-goods-375814-d6d8e1553699.json'\n",
    "SeriesInstanceUIDs=['1.2.840.113654.2.55.269907831379668070404580302097056025638','1.3.6.1.4.1.14519.5.2.1.7009.9004.231985094753394017986102999426']\n",
    "FastModeStatus= True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWqmvKUsecZV"
   },
   "outputs": [],
   "source": [
    "# ##authenticating google cloud with a service account to run bigquery\n",
    "# #when the new IDC storage schema is live, there will be no need to use service account\n",
    "\n",
    "# from google.cloud import storage\n",
    "# from google.cloud import bigquery as bq\n",
    "# from google.oauth2 import service_account\n",
    "# !gsutil cp gs://fc-859bd570-893c-4e71-a677-875d7d0d7af3/graceful-goods-375814-d6d8e1553699.json .\n",
    "# credentials = service_account.Credentials.from_service_account_file(\n",
    "#      os.path.join(curr_dir,'graceful-goods-375814-d6d8e1553699.json'), scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],)\n",
    "# bq_client = bq.Client(credentials=credentials, project=credentials.project_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6axI1OFlrkB"
   },
   "outputs": [],
   "source": [
    "# selection_query = \"\"\"\n",
    "# select distinct\n",
    "# nlst.SeriesInstanceUID,\n",
    "# idc.gcs_url,\n",
    "# CONCAT(\"cp \",REPLACE(idc.gcs_url, \"gs://\", \"s3://\"), \" data/idc_data/\") as s5cmd_cp_url\n",
    "\n",
    "#  from `graceful-goods-375814.terra.nlst` nlst\n",
    "\n",
    "#  join `bigquery-public-data.idc_current.dicom_all` idc\n",
    "\n",
    "#  on nlst.SeriesInstanceUID = idc.SeriesInstanceUID\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# cohort_df = bq_client.query(selection_query).to_dataframe()\n",
    "# #cohort_df=cohort_df[cohort_df[\"SeriesInstanceUID\"]==SeriesInstanceUID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3FxH9UDJo-i",
    "outputId": "a68ec837-ff13-4bbc-fb3b-3a653de8dd3b"
   },
   "outputs": [],
   "source": [
    "#get the path of the zip file containing csv file in the current working directory\n",
    "import glob\n",
    "#zip_file_path='/home/vamsi/Downloads/result.zip'\n",
    "\n",
    "zip_file_path = glob.glob('*.zip')[0]\n",
    "#zip_file_path='/content/drive/MyDrive/result.zip'\n",
    "!unzip $zip_file_path\n",
    "\n",
    "csv_file_path = glob.glob('*.csv')[0]\n",
    "#csv_file_path= '/home/vamsi/Downloads/result.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50NBcD7f1prE"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install modin[ray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "Xxx_-tcIBulw",
    "outputId": "339621dc-fe29-46af-e103-672f0a37023f"
   },
   "outputs": [],
   "source": [
    "#import ray\n",
    "#!pip install dask\n",
    "#!pip install pandas==1.5.3\n",
    "import os\n",
    "#os.environ[\"MODIN_ENGINE\"] = \"ray\" \n",
    "#import modin.pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "#import dask.dataframe as dd\n",
    "#import csv\n",
    "#import pandas as pd\n",
    "#cohort_df = dd.read_csv(csv_file_path, delimiter=',',quoting=csv.QUOTE_NONE, encoding='utf-8').compute()\n",
    "start_time = time.time()\n",
    "cohort_df=pd.read_csv(csv_file_path, delimiter=',', encoding='utf-8')\n",
    "read_time=time.time() -start_time\n",
    "print('read in '+str(read_time)+ '  seconds')\n",
    "#cohort_df=pd.read_csv(csv_file_path, delimiter=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZApCCVLkZyb"
   },
   "outputs": [],
   "source": [
    "# # N.B. - this works as intended only if the BQ query parses data from a single dataset\n",
    "# # if not, feel free to set the name manually!\n",
    "# dataset_name = cohort_df[\"collection_id\"].values[0]\n",
    "# dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNutD-SRN52T"
   },
   "outputs": [],
   "source": [
    "# create the directory tree\n",
    "!rm -r data\n",
    "!mkdir -p data\n",
    "!mkdir -p data/idc_data\n",
    "!mkdir -p data/input_data \n",
    "!mkdir -p data/output_data \n",
    "!mkdir -p data/sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qu6eXpHx6iV"
   },
   "source": [
    "# **Parsing Cohort Information from BigQuery Tables**\n",
    "\n",
    "We can check the various fields of the table we populated by running the BigQuery query.\n",
    "\n",
    "This table will store one entry for each DICOM file in the dataset (therefore, expect thousands of rows!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V27IJihCi9Kv"
   },
   "outputs": [],
   "source": [
    "# pat_id_list = sorted(list(set(cohort_df[\"PatientID\"].values)))\n",
    "\n",
    "# print(\"Total number of unique Patient IDs:\", len(pat_id_list))\n",
    "\n",
    "# display(cohort_df.info())\n",
    "# display(cohort_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AolLtXOLVt7D"
   },
   "source": [
    "# **Setup mhubio**\n",
    "\n",
    "`mhbio` is the module of MHub that deals with all of the basic operations shared between a large majority of the models.\n",
    "\n",
    "Let's import all of the modules we will need to run the `TotalSegmentator` pipeline from end to end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymJsrRMQ0esM"
   },
   "outputs": [],
   "source": [
    "# !pip install pyplastimatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRmrt00589lv"
   },
   "outputs": [],
   "source": [
    "sys.path.append('.')\n",
    "\n",
    "from mhubio.core import Config, DataType, FileType, CT, SEG\n",
    "\n",
    "from mhubio.modules.importer.UnsortedDicomImporter import UnsortedInstanceImporter\n",
    "from mhubio.modules.importer.DataSorter import DataSorter\n",
    "from mhubio.modules.convert.NiftiConverter import NiftiConverter\n",
    "from mhubio.modules.convert.DsegConverter import DsegConverter\n",
    "from mhubio.modules.organizer.DataOrganizer import DataOrganizer\n",
    "\n",
    "from mhubai.totalsegmentator.utils.TotalSegmentatorRunner import TotalSegmentatorRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBUqa6Nj9SKe"
   },
   "source": [
    "For instance, the workflow for this example looks like the following:\n",
    "\n",
    "```\n",
    "DICOM CT >> NIfTI >> TotalSegmentator >> NIfTI >> DICOM SEG \n",
    "```\n",
    "\n",
    "We want to start from DICOM CT data and save the results in DICOM SEG format. We are therefore going to need the relevant `DataType`(s) and `FileType`(s) imported (`CT` and the `SEG`) from our `Config` module. \n",
    "\n",
    "We will also need to import all the converters to run the aforementioned operations (`UnsortedInstanceImporter`, `DataSorter`, `NiftiConverter`, `DsegConverter`, `DataOrganizer`) and the model runner (`TotalSegmentatorRunner`).\n",
    "\n",
    "For more in-depth explanation of the modules, see the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n0qjPp1B_mv"
   },
   "source": [
    "# **Running the Analysis for a Single Patient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pFSCy5HoOp_"
   },
   "source": [
    "The following cells run all the processing pipeline, from pre-processing to post-processing.\n",
    "\n",
    "Let's start by slicing the datafram storing the metadata we pulled from IDC to get a single series from a single patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTURQtGPgeQF"
   },
   "outputs": [],
   "source": [
    "# # randomly select one patient from the cohort\n",
    "# pat_id = random.choice(cohort_df[\"PatientID\"].values)\n",
    "# patient_df = cohort_df[cohort_df[\"PatientID\"] == pat_id].reset_index(drop = True)\n",
    "\n",
    "# # select only data for which the modality is CT \n",
    "# #patient_df = patient_df[patient_df[\"Modality\"] == \"CT\"].reset_index(drop = True)\n",
    "\n",
    "# # if more than one series are available for the selected patient, pick one\n",
    "# if len(np.unique(patient_df[\"SeriesInstanceUID\"].values)) > 1:\n",
    "#   series_uid = random.choice(patient_df[\"SeriesInstanceUID\"].values)\n",
    "#   patient_df = patient_df[patient_df[\"SeriesInstanceUID\"] == series_uid].reset_index(drop = True)\n",
    "\n",
    "# # sanity check\n",
    "# assert len(np.unique(patient_df[\"SeriesInstanceUID\"].values)) == 1\n",
    "\n",
    "# display(patient_df.info())\n",
    "# display(patient_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fv1HZ6K8m5h"
   },
   "source": [
    "Write a custom configuration file containing the specifics for all the MHub modules we're going to use, using the `%%writefile` magik (from the `yamlmagic` package).\n",
    "\n",
    "This file is going to be tailored to this specific use case and example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ittQxkqwRYcO"
   },
   "source": [
    "The config file stores all the information needed by the different modules, such as the path to a given file or folder, to execute the end-to-end pipeline.\n",
    "\n",
    "For instance, we specify `data_base_dir` as a `general` argument all the modules are going to share. Then, we specify the module-specific arguments.\n",
    "\n",
    "For instance, we specify the directory where the DICOM data to be sorted are for the `UnsortedInstanceImporter`, by adding `input_dir` to the config file. We can also specify the structure of the sorted directory by setting the `structure` argument for the `DataSorter`, or which `TotalSegmentator` model to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkUYdQcg6o0-"
   },
   "outputs": [],
   "source": [
    "%%writefile totalsegmentator_config.yml\n",
    "\n",
    "general:\n",
    "  data_base_dir: /content/data\n",
    "modules:\n",
    "  UnsortedInstanceImporter:\n",
    "    input_dir: /content/data/idc_data\n",
    "  DataSorter:\n",
    "    base_dir: /content/data/sorted\n",
    "    structure: '%SeriesInstanceUID/dicom/%SOPInstanceUID.dcm'\n",
    "  DsegConverter:\n",
    "    dicomseg_json_path: mhubai/totalsegmentator/config/dicomseg_metadata_whole.json\n",
    "    #dicomseg_yml_path: mhubai/totalsegmentator/config/dseg.yml\n",
    "    skip_empty_slices: True\n",
    "  TotalSegmentatorRunner:\n",
    "    use_fast_mode: FastModeStatus\n",
    "    use_multi_label_output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2yqx6DmqGVi"
   },
   "outputs": [],
   "source": [
    "with open('totalsegmentator_config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Update the value of use_fast_mode with the value of FastModeStatus\n",
    "config['general']['data_base_dir'] = os.path.join(curr_dir,'data')\n",
    "config['modules']['UnsortedInstanceImporter']['input_dir'] = os.path.join(curr_dir,'data/idc_data')\n",
    "config['modules']['DataSorter']['base_dir'] = os.path.join(curr_dir,'data/sorted')\n",
    "config['modules']['TotalSegmentatorRunner']['use_fast_mode'] = FastModeStatus\n",
    "\n",
    "# Write the updated dictionary back to the YAML file\n",
    "with open('totalsegmentator_config.yml', 'w') as file:\n",
    "    yaml.safe_dump(config, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neN3TTcEw_27"
   },
   "source": [
    "After defining a config file for our use case, we can initialize a `Config` object using such `yml` file.\n",
    "\n",
    "The `Config` object is passed along to all the modules, and keeps track of all of the information that need to shared among these modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xl7eLutk4_Jv"
   },
   "outputs": [],
   "source": [
    "# config\n",
    "config = Config('totalsegmentator_config.yml')\n",
    "config.verbose = True  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QLU18xBxp33"
   },
   "source": [
    "In the next cells, we define a utility function to pull data from the Imaging Data Commons - starting from the DataFrame `patient_df` we have previously defined - and then cross-load the data from the IDC Buckets to this Colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXLwDynGpGoY"
   },
   "outputs": [],
   "source": [
    "# def download_patient_data(download_path, patient_df):\n",
    "\n",
    "#   \"\"\"\n",
    "#   Download raw DICOM data and run dicomsort to standardise the input format.\n",
    "#   Arguments:\n",
    "#     download_path : required - path to the folder where the raw data will be downloaded.\n",
    "#     patient_df    : required - Pandas dataframe storing all the information required\n",
    "#                                to pull data  from the IDC buckets.\n",
    "#   \"\"\"\n",
    "\n",
    "#   gs_file_path = \"gcs_paths.txt\"\n",
    "#   patient_df[\"gcs_url\"].to_csv(gs_file_path, header = False, index = False)\n",
    "\n",
    "#   pat_id = patient_df[\"PatientID\"].values[0]\n",
    "#   download_path = os.path.join(download_path, pat_id)\n",
    "\n",
    "#   if not os.path.exists(download_path): os.mkdir(download_path)\n",
    "\n",
    "#   start_time = time.time()\n",
    "#   print(\"Copying files from IDC buckets to %s...\"%(download_path))\n",
    "\n",
    "#   !cat $gs_file_path | gsutil -q -m cp -Ir $download_path >> /dev/null\n",
    "\n",
    "#   elapsed = time.time() - start_time\n",
    "#   print(\"Done in %g seconds.\"%elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJ9J_tygkabp"
   },
   "outputs": [],
   "source": [
    "# def download_patient_data(download_path,series_id):\n",
    "\n",
    "#   \"\"\"\n",
    "#   Download raw DICOM data and run dicomsort to standardise the input format.\n",
    "#   Arguments:\n",
    "#     download_path : required - path to the folder where the raw data will be downloaded.\n",
    "#     patient_df    : required - Pandas dataframe storing all the information required\n",
    "#                                to pull data  from the IDC buckets.\n",
    "#   \"\"\"\n",
    "\n",
    "#   gs_file_path = \"s5cmd_manifest.txt\"\n",
    "#   cohort_df = bq_client.query(selection_query).to_dataframe()\n",
    "#   cohort_df=cohort_df[cohort_df['SeriesInstanceUID']==series_id]\n",
    "#   cohort_df[\"s5cmd_cp_url\"].to_csv(gs_file_path, header = False, index = False)\n",
    "\n",
    "#   download_path = os.path.join(download_path)\n",
    "\n",
    "#   if not os.path.exists(download_path): os.mkdir(download_path)\n",
    "\n",
    "#   start_time = time.time()\n",
    "#   print(\"Copying files from IDC buckets to %s...\"%(download_path))\n",
    "\n",
    "#   #!cat $gs_file_path | gsutil -q -m cp -Ir $download_path >> /dev/null\n",
    "#   !s5cmd --no-sign-request --endpoint-url https://storage.googleapis.com run s5cmd_manifest.txt\n",
    "\n",
    "#   elapsed = time.time() - start_time\n",
    "#   print(\"Done in %g seconds.\"%elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxthXfJbCjYu"
   },
   "outputs": [],
   "source": [
    "def download_patient_data(download_path,series_id):\n",
    "\n",
    "  \"\"\"\n",
    "  Download raw DICOM data and run dicomsort to standardise the input format.\n",
    "  Arguments:\n",
    "    download_path : required - path to the folder where the raw data will be downloaded.\n",
    "    patient_df    : required - Pandas dataframe storing all the information required\n",
    "                               to pull data  from the IDC buckets.\n",
    "  \"\"\"\n",
    "  global cohort_df\n",
    "  gs_file_path = \"s5cmd_manifest.txt\"\n",
    "  #cohort_df = bq_client.query(selection_query).to_dataframe()\n",
    "  series_df=cohort_df[cohort_df['SeriesInstanceUID']==series_id]\n",
    "  series_df[\"s5cmd_cp_url\"].to_csv(gs_file_path, header = False, index = False)\n",
    "  #remove double quotes from the manifest file\n",
    "  !sed -i 's/\"//g' s5cmd_manifest.txt  \n",
    "\n",
    "  download_path = os.path.join(download_path)\n",
    "\n",
    "  if not os.path.exists(download_path): os.mkdir(download_path)\n",
    "\n",
    "  start_time = time.time()\n",
    "  print(\"Copying files from IDC buckets to %s...\"%(download_path))\n",
    "\n",
    "  #!cat $gs_file_path | gsutil -q -m cp -Ir $download_path >> /dev/null\n",
    "  !s5cmd --no-sign-request --endpoint-url https://storage.googleapis.com run s5cmd_manifest.txt  >> /dev/null\n",
    "\n",
    "  elapsed = time.time() - start_time\n",
    "  print(\"Done in %g seconds.\"%elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZPCanDagR2D"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MemoryMonitor:\n",
    "    def __init__(self):\n",
    "        self.keep_measuring = True\n",
    "\n",
    "    def measure_usage(self):\n",
    "        cpu_usage = []\n",
    "        ram_usage_mb=[]\n",
    "        time_stamps = []\n",
    "        start_time = time.time()\n",
    "        while self.keep_measuring:\n",
    "            cpu = psutil.cpu_percent()\n",
    "            ram = psutil.virtual_memory()\n",
    "            ram_total_mb = psutil.virtual_memory().total / 1024 / 1024\n",
    "            ram_mb = (ram.total - ram.available) / 1024 / 1024\n",
    "            cpu_usage.append(cpu)\n",
    "            ram_usage_mb.append(ram_mb)\n",
    "            time_stamps.append(time.time()- start_time)\n",
    "            sleep(1)\n",
    "\n",
    "        return cpu_usage, ram_usage_mb, time_stamps, ram_total_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qsFwVHWgUUO"
   },
   "outputs": [],
   "source": [
    "# runTimeStatistics=pd.DataFrame()\n",
    "\n",
    "# def download_and_process_series(series_id):\n",
    "#     global runTimeStatistics\n",
    "#     log=pd.DataFrame()\n",
    "#     log['SeriesInstanceUID']=[series_id]\n",
    "#     start_time = time.time()\n",
    "#     download_patient_data(\"/content/data/idc_data\", series_id)\n",
    "#     download_time=time.time()- start_time\n",
    "#     log[series_id]['download_time']=download_time\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     importer = UnsortedInstanceImporter(config)\n",
    "#     importer.execute()\n",
    "#     importer_time=time.time()- start_time\n",
    "#     log[series_id]['importer_time']=importer_time\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     DataSorter(config).execute()\n",
    "#     DataSorter_time=time.time()- start_time\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     NiftiConverter(config).execute()\n",
    "#     NiftiConverter_time=time.time()- start_time\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     TotalSegmentatorRunner(config).execute()\n",
    "#     TotalSegmentatorRunner_time=time.time()- start_time\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     DsegConverter(config).execute()\n",
    "#     DsegConverter_time=time.time()- start_time\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     archive_fn = \"{}.zip\".format(series_id)\n",
    "#     archiving_time=time.time()- start_time\n",
    "#     # Annotate on the plots\n",
    "#     try:\n",
    "#         os.remove(archive_fn)\n",
    "#     except OSError:\n",
    "#         pass\n",
    "#     with zipfile.ZipFile(archive_fn, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "#         zip_file.write(\"data/sorted/{}/seg.dcm\".format(series_id))\n",
    "\n",
    "#     # Clean up data directories\n",
    "#     os.system(\"rm -r data/ s5cmd_manifest.txt\")\n",
    "#     os.system(\"mkdir -p data/\")\n",
    "#     os.system(\"mkdir -p data/idc_data\")\n",
    "#     os.system(\"mkdir -p data/input_data\")\n",
    "#     os.system(\"mkdir -p data/output_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfbmrWT0uAG-"
   },
   "outputs": [],
   "source": [
    "def download_and_process_series(series_id):\n",
    "    log = pd.DataFrame({'SeriesInstanceUID': [series_id]})\n",
    "    start_time = time.time()\n",
    "    download_patient_data(os.path.join(curr_dir,'data/idc_data'), series_id)\n",
    "    download_time = time.time() - start_time\n",
    "    log['download_time'] = download_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    importer = UnsortedInstanceImporter(config)\n",
    "    importer.execute()\n",
    "    importer_time = time.time() - start_time\n",
    "    log['importer_time'] = importer_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    DataSorter(config).execute()\n",
    "    DataSorter_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    NiftiConverter(config).execute()\n",
    "    NiftiConverter_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    TotalSegmentatorRunner(config).execute()\n",
    "    TotalSegmentatorRunner_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    DsegConverter(config).execute()\n",
    "    DsegConverter_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    archive_fn = \"{}.zip\".format(series_id)\n",
    "    archiving_time = time.time() - start_time\n",
    "    try:\n",
    "     os.remove(archive_fn)\n",
    "    except OSError:\n",
    "     pass\n",
    "    !zip -j -r \"$archive_fn\" \"data/sorted/$series_id/seg.dcm\" \n",
    "\n",
    "    os.system(\"rm -r data/ s5cmd_manifest.txt\")\n",
    "    os.system(\"mkdir -p data/\")\n",
    "    os.system(\"mkdir -p data/idc_data\")\n",
    "    os.system(\"mkdir -p data/input_data\")\n",
    "    os.system(\"mkdir -p data/output_data\")\n",
    "    os.system(\"mkdir -p data/sorted\")\n",
    "\n",
    "    log['DataSorter_time'] = DataSorter_time\n",
    "    log['NiftiConverter_time'] = NiftiConverter_time\n",
    "    log['TotalSegmentatorRunner_time'] = TotalSegmentatorRunner_time\n",
    "    log['DsegConverter_time'] = DsegConverter_time\n",
    "    log['archiving_time'] = archiving_time\n",
    "    global runtime_stats\n",
    "    runtime_stats = runtime_stats.append(log, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9MVJRvf_EKM"
   },
   "outputs": [],
   "source": [
    "runtime_stats = pd.DataFrame(columns=['SeriesInstanceUID','download_time','importer_time','DataSorter_time',\n",
    "                                      'NiftiConverter_time', 'TotalSegmentatorRunner_time',\n",
    "                                      'DsegConverter_time',\t'archiving_time', 'cpu_usage','ram_usage_mb', 'ram_total_mb'\n",
    "                                      ])\n",
    "if __name__ == \"__main__\":\n",
    "    for series_id in SeriesInstanceUIDs:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            monitor = MemoryMonitor()\n",
    "            mem_thread = executor.submit(monitor.measure_usage)\n",
    "            try:\n",
    "                proc_thread = executor.submit(download_and_process_series, series_id)\n",
    "                proc_thread.result()\n",
    "            finally:\n",
    "                monitor.keep_measuring = False\n",
    "                cpu_usage, ram_usage_mb, time_stamps, ram_total_mb = mem_thread.result()\n",
    "                \n",
    "                cpu_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
    "                runtime_stats.iloc[cpu_idx, runtime_stats.columns.get_loc('cpu_usage')] = [[cpu_usage]]\n",
    "\n",
    "                ram_usage_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
    "                runtime_stats.iloc[ram_usage_mb_idx, runtime_stats.columns.get_loc('ram_usage_mb')] = [[ram_usage_mb]]\n",
    "                \n",
    "                ram_total_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
    "                runtime_stats.iloc[ram_total_mb_idx, runtime_stats.columns.get_loc('ram_total_mb')] = [[ram_total_mb]]\n",
    "                \n",
    "                \n",
    "                #runtime_stats.at[runtime_stats['SeriesInstanceUID']==series_id, 'cpu_usage']=[[cpu_usage]]\n",
    "                #runtime_stats.at[runtime_stats['SeriesInstanceUID']==series_id, 'ram_usage_mb']=[[ram_usage_mb]]\n",
    "                #runtime_stats.at[runtime_stats['SeriesInstanceUID']==series_id, 'ram_total_mb']=[[ram_total_mb]]\n",
    "\n",
    "                plt.plot(time_stamps, cpu_usage)\n",
    "                plt.ylim(0, 100)\n",
    "                plt.xlabel('Time (s)')\n",
    "                plt.ylabel('CPU usage (%)')\n",
    "                plt.show()\n",
    "\n",
    "                plt.plot(time_stamps, ram_usage_mb)\n",
    "                plt.ylim(0, ram_total_mb)\n",
    "                plt.xlabel('Time (s)')\n",
    "                plt.ylabel('Memory usage (MB)')\n",
    "                plt.show()\n",
    "\n",
    "runtime_stats.to_csv('runtime.csv')\n",
    "import subprocess\n",
    "subprocess.run([\"zip\", \"-j\", \"-r\", \"runtime_stats.zip\", os.path.join(curr_dir, \"runtime.csv\")])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdhVcxUvMvOm"
   },
   "outputs": [],
   "source": [
    "runtime_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bn5lsFL9Nvn"
   },
   "outputs": [],
   "source": [
    "# for series_id in SeriesInstanceUIDs:  \n",
    "#   download_patient_data(\"data/idc_data\",series_id)\n",
    "\n",
    "#   importer = UnsortedInstanceImporter(config).execute()\n",
    "#   DataSorter(config).execute()\n",
    "#   NiftiConverter(config).execute()\n",
    "#   TotalSegmentatorRunner(config).execute()\n",
    "#   DsegConverter(config).execute()\n",
    "#   # organizer = DataOrganizer(config, set_file_permissions = sys.platform.startswith('linux'))\n",
    "#   # organizer.setTarget(DataType(FileType.NIFTI, CT), \"{}/data/output_data/[i:SeriesID]/[path]\".format(curr_dir))\n",
    "#   # organizer.setTarget(DataType(FileType.DICOMSEG, SEG), \"{}/data/output_data/[i:SeriesID]/TotalSegmentator.seg.dcm\".format(curr_dir))\n",
    "#   # organizer.execute()\n",
    "#   archive_fn = \"{}.zip\".format(series_id)\n",
    "\n",
    "#   # import pydicom\n",
    "#   # seg_series = pydicom.dcm_read(f\"output_data/{series_id}/TotalSegmentator.dcm\")\n",
    "#   # series_uid = seg_series.SeriesInstanceUID\n",
    "\n",
    "#   # # cp TotalSegmentator.seg.dcm to {series_uid}.dcm and zip\n",
    "\n",
    "#   try:\n",
    "#      os.remove(archive_fn)\n",
    "#   except OSError:\n",
    "#      pass\n",
    "#   !zip -j -r \"$archive_fn\" \"data/sorted/$series_id/seg.dcm\" \n",
    "#   !rm -r data/ s5cmd_manifest.txt \n",
    "#   !mkdir -p data/\n",
    "#   !mkdir -p data/idc_data\n",
    "#   !mkdir -p data/input_data \n",
    "#   !mkdir -p data/output_data \n",
    "#   !mkdir -p data/sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFEBiU-Eyd5Y"
   },
   "source": [
    "We then import the DICOM data found at `data/idc_data`, as specified in the config file at:\n",
    "\n",
    "```\n",
    "general:\n",
    "  data_base_dir: /content/data\n",
    "modules:\n",
    "  UnsortedInstanceImporter:\n",
    "    input_dir: idc_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzpjNG6o4_R8"
   },
   "outputs": [],
   "source": [
    "# # import a collection of unsorted DICOM data\n",
    "# importer = UnsortedInstanceImporter(config)\n",
    "# importer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZQ_Gl748ujP"
   },
   "source": [
    "After importing the data, we sort the DICOM files in the fashion specified in the config file - and after, we convert to NIfTI, which is the format accepted by TotalSegmentator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3S77Epj5CtJ"
   },
   "outputs": [],
   "source": [
    "# # sort such collection of DICOM data using dicomsort\n",
    "# DataSorter(config).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWmKV_wB5C3M"
   },
   "outputs": [],
   "source": [
    "# # convert the DICOM data to NIfTI, as required by TotalSegmentator, using plastimatch\n",
    "# NiftiConverter(config).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "og-3ruZM9sEf"
   },
   "source": [
    "Finally, we run TotalSegmentator using the parameters specified at the `TotalSegmentatorRunner` module of the config file, and convert the results back to DICOM SEG using the parameters set in the `DsegConverter` (`dicomseg_yml_path` and `dicomseg_json_path`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnPzkfAe5C6p"
   },
   "outputs": [],
   "source": [
    "# # run the inference phase \n",
    "# TotalSegmentatorRunner(config).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FS_JNVrW5C-R"
   },
   "outputs": [],
   "source": [
    "# # convert the results to DICOM SEG\n",
    "# DsegConverter(config).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4Zufdwo_qbN"
   },
   "source": [
    "As a last step, we run MHub's `DataOrganizer` to organize the output data in a predetermined and standardized structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9ubPDjO5DCC"
   },
   "outputs": [],
   "source": [
    "# # organize data into output folder\n",
    "# # FIXME: don't save stuff under /app\n",
    "# organizer = DataOrganizer(config, set_file_permissions = sys.platform.startswith('linux'))\n",
    "# organizer.setTarget(DataType(FileType.NIFTI, CT), \"{}/data/output_data/[i:SeriesID]/[path]\".format(curr_dir))\n",
    "# organizer.setTarget(DataType(FileType.DICOMSEG, SEG), \"{}/data/output_data/[i:SeriesID]/TotalSegmentator.seg.dcm\".format(curr_dir))\n",
    "# organizer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2QqrdvXqQq0"
   },
   "source": [
    "---\n",
    "\n",
    "# **Downloading the Results Locally**\n",
    "\n",
    "After the end-to-end processing is done, we can download the results (and the pre-processed input data) running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARmBD0lHdz_2"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# archive_fn = \"%s.zip\"%(SeriesInstanceUID)\n",
    "\n",
    "# try:\n",
    "#   os.remove(archive_fn)\n",
    "# except OSError:\n",
    "#   pass\n",
    "\n",
    "# !zip -j -r $archive_fn \"{}/data/output_data\".format(curr_dir) \"{}/data/input_data\".format(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pE0X9kid0D-"
   },
   "outputs": [],
   "source": [
    "# filesize = os.stat(archive_fn).st_size/1024e03\n",
    "# print('Starting the download of \"%s\" (%2.1f MB)...\\n'%(archive_fn, filesize))\n",
    "\n",
    "# files.download(archive_fn)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
