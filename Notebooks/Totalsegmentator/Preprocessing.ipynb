{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/Cloud-Resources-Workflows/blob/notebooks2/Notebooks/Totalsegmentator/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgDs7gUHE-dh"
      },
      "source": [
        "##**How to Generate a Datatable for Terra to Run the TotalSegmentatortwoVmWorkflowOnTerra**\n",
        "\n",
        "This notebook provides a step-by-step guide on how to prepare a datatable for Terra that is compatible with the [TotalSegmentatortwoVmWorkflowOnTerra](https://dockstore.org/workflows/github.com/ImagingDataCommons/Cloud-Resources-Workflows/TotalSegmentatortwoVmWorkflowOnTerra:dev?tab=info) workflow. This workflow performs segmentation and feature extraction on DICOM images using two virtual machines (VMs) on Terra.\n",
        "\n",
        "The steps are:\n",
        "\n",
        "1. **Filter out localizer and inconsistent series**. Run an SQL query to exclude series that are localizer scans or have geometric inconsistencies from the cohort of interest.\n",
        "2. **Extract the AWS URLs of the series**. The IDC buckets store the DICOM images at the series level, (i.e a reference to the series folder is enough, and there is no need to get the location of each SOPInstance's url) so the AWS URL of each series is the only input required for the workflow. However, you can also include other attributes that may help you organize or filter the data. The `s5cmdurl` column of the resulting table contains the command that can be used with `s5cmd` to download the series. Note: The query configures downloading the series to the `idc_data` folder by default, as this folder is cleaned after processing each series in the notebooks. You can change the destination folder if needed by modifying the sql query.\n",
        "3. **Split the cohort into chunks**. Create manifests of 12 series each, so you can leverage Terra's parallel computing capabilities and run the workflow across thousands of VMs on Terra. Note: Rawls, the underlying engine of Terra, can run up to 3000 jobs and up to 28800 tasks (a job may contain multiple tasks) at a time.\n",
        "4. **Copy the manifests to the Terra workspace bucket**. Use the `gsutil` command to copy the manifests from your local machine to the bucket associated with your Terra workspace.\n",
        "5. **Generate a Terra datatable**. Use the manifests and the AWS URLs of the series to create a datatable that has the inputs for the TotalSegmentatortwoVmWorkflowOnTerra. The datatable should have one row per series and one column per input parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tlXtbeCLeuY"
      },
      "source": [
        "##**Authenticate gcloud**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CakOUR9MAe3"
      },
      "outputs": [],
      "source": [
        "project_id='my-test-project'\n",
        "terra_workspace_bucket__folder_url='gs://my-test-terra-workspace-bucket/nlst-121523'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzVR2TF-KmRA"
      },
      "outputs": [],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1xYbz8CK5gZ",
        "outputId": "6223aee6-0471-4f4f-dd42-73d68b3f7288"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project $project_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tNALARLLjp-"
      },
      "source": [
        "##**Download and run the sql query which removes localizer and geometrically inconsistent series**##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4t3HJSKLcQs",
        "outputId": "5a85c5f6-c2fa-47c9-85ba-8a4722dd543d"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/ImagingDataCommons/Cloud-Resources-Workflows/sqlqueryfix/sqlQueries/nlstCohort.sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDaww2epJUef",
        "outputId": "2532839a-85d0-4b43-8691-2224e0c88e25"
      },
      "outputs": [],
      "source": [
        "!cat nlstCohort.sql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuwCM_1ImNYI"
      },
      "source": [
        "###Run this command twice as the first time bq is run, it returns a initialization message.\n",
        "\n",
        "https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/issues/35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvFH7z5WKhzM"
      },
      "outputs": [],
      "source": [
        "!cat nlstCohort.sql | bq query --format=csv  --project_id=$project_id --max_rows=999999999 --use_legacy_sql=false > nlst_cohort.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV_Af9tVoeh5"
      },
      "source": [
        "##**Generate Batches of 12 series and a terra data table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "9-o79VfxlZfQ",
        "outputId": "6ce1d2e8-22e4-452e-ef2d-13b1a15cb902"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "df= pd.read_csv('nlst_cohort.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "1BD5fGfInGe6",
        "outputId": "68d9c483-467f-46a8-9103-1e23d4dde909"
      },
      "outputs": [],
      "source": [
        "df['projected_batch_number']=np.ceil((df.index + 1) / 12)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87O2LPHzqiOo"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    shutil.rmtree(f'urls')\n",
        "except OSError:\n",
        "    pass\n",
        "os.makedirs('urls')\n",
        "\n",
        "\n",
        "# Set the number of rows per file\n",
        "rows_per_file = 12\n",
        "\n",
        "# Calculate the number of files needed\n",
        "num_files = math.ceil(len(df) / rows_per_file)\n",
        "\n",
        "# Split the dataframe into multiple dataframes\n",
        "dfs = [df[i*rows_per_file:(i+1)*rows_per_file] for i in range(num_files)]\n",
        "\n",
        "# Get the current date and time formatted with underscores up to minutes\n",
        "now = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
        "\n",
        "# Create a new column name for the batch_id column\n",
        "batch_id_column = f'entity:twoVM_{now}_id'\n",
        "\n",
        "# Create a new dataframe to store the batch information\n",
        "batch_df = pd.DataFrame(columns=[batch_id_column, 'dicomToNiftiConverterTool', 's5cmd_url', 'dicomSegAndSRcpu', 'dicomSegAndSRram'])\n",
        "\n",
        "# Analyze each file and add a row to the batch dataframe\n",
        "for i, df in enumerate(dfs):\n",
        "    max_sopinstancecount = df['sopInstanceCount'].max()\n",
        "    filename = f'urls/batch_{i+1}.csv'\n",
        "    url_suffix = f'batch_{i+1}.csv'\n",
        "    df.to_csv(filename, index=False)\n",
        "    s5cmd_url = f'{terra_workspace_bucket__folder_url}/{url_suffix}'\n",
        "\n",
        "    if max_sopinstancecount >= 300:\n",
        "        cpu = 8\n",
        "        ram = 32\n",
        "    else:\n",
        "        cpu = 4\n",
        "        ram = 16\n",
        "\n",
        "    new_row = pd.DataFrame({\n",
        "        batch_id_column: [i+1],\n",
        "        'dicomToNiftiConverterTool': ['dcm2niix'],\n",
        "        's5cmd_url': [s5cmd_url],\n",
        "        'dicomSegAndSRcpu': [cpu],\n",
        "        'dicomSegAndSRram': [ram]\n",
        "    })\n",
        "    batch_df = pd.concat([batch_df, new_row], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "THk0oLDol4A3",
        "outputId": "933f945e-73fa-4ed0-ac37-c39d1c93e9fe"
      },
      "outputs": [],
      "source": [
        "batch_df.to_csv(f'terra_data_table_manifest_{now}.tsv', sep='\\t', index=False)\n",
        "batch_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Civfd1ciHp"
      },
      "source": [
        "##**Copy files to terra workspace bucket**\n",
        "A folder need not be created first. gsutil automatically creates the destination folder if not present\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz-s_fsbaa15"
      },
      "outputs": [],
      "source": [
        "!gsutil -m cp -r urls/* $terra_workspace_bucket__folder_url"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMnEbhYRfzpmRq46bkrc5v2",
      "collapsed_sections": [
        "X_Civfd1ciHp",
        "w1mv3rg8e7Tv"
      ],
      "include_colab_link": true,
      "mount_file_id": "1Pwo3GrVLSLebAKPRxqd0jKZqRu7YYmQ8",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
