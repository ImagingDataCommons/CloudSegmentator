{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkt1414/Cloud-Resources-Workflows/blob/main/Notebooks/Totalsegmentator/totalsegmentator_mwe_vamsi_040423_mhubai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmb_HiJv7Sit"
      },
      "source": [
        "# **Whole Body CT Segmentation with TotalSegmentator**\n",
        "\n",
        "This notebook provides a minimal working example of TotalSegmentator, a tool for the segmentation of 104 anatomical structures from CT images. The model was trained using a wide range of imaging CT data of different pathologies from several scanners, protocols and institutions.\n",
        "\n",
        "\n",
        "\n",
        "We test TotalSegmentator by implementing an end-to-end (cloud-based) pipeline on publicly available whole body CT scans hosted on the [Imaging Data Commons (IDC)](https://portal.imaging.datacommons.cancer.gov/), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI model. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed by a wide variety of image types (from the area covered by the scan, to the presence of contrast and various types of artefacts).\n",
        "\n",
        "\n",
        "The way all the operations are executed - from pulling data, to data postprocessing, and the standardisation of the results - have the goal of promoting transparency and reproducibility. Furthermore, this notebook is part of [a collection of code, notebooks and Docker containers](https://github.com/AIM-Harvard/mhub/blob/main/mhub/totalsegmentator/notebooks/totalsegmentator_mwe.ipynb) we are developing with the goal of making a wide range of machine learning models for medicine available through a standardized I/O framework.\n",
        "\n",
        "\n",
        "\n",
        "Papermill package is used as a tool for parameterizing and executing Jupyter notebooks. One advantage of using Papermill is that it allows you to use the same notebook in different environments, such as in a Docker container. By parameterizing the notebook using Papermill, you can easily change the values of variables or comment out cells that install packages when running the notebook in a Docker environment. This makes it easier to reuse and share notebooks across different environments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhTN-Q6jujt4"
      },
      "source": [
        "Please cite the following article if you use this code or pre-trained models:\n",
        "\n",
        "Wasserthal, J., Meyer, M., Breit, H.C., Cyriac, J., Yang, S. and Segeroth, M., 2022. TotalSegmentator: robust segmentation of 104 anatomical structures in CT images. arXiv preprint arXiv:2208.05868, [\n",
        "https://doi.org/10.48550/arXiv.2208.05868]( \t\n",
        "https://doi.org/10.48550/arXiv.2208.05868).\n",
        "\n",
        "The original code is published on\n",
        "[GitHub](https://github.com/wasserth/TotalSegmentator)  using the [Apache-2.0 license](https://github.com/wasserth/TotalSegmentator/blob/master/LICENSE).\n",
        "\n",
        "Li X, Morgan PS, Ashburner J, Smith J, Rorden C. (2016) The first step for neuroimaging data analysis: DICOM to NIfTI conversion. J Neurosci Methods. 264:47-56.\n",
        "\n",
        "Shackleford, James A., Nagarajan Kandasamy and Gregory C. Sharp. “Plastimatch—An Open-Source Software for Radiotherapy Imaging.” (2014).\n",
        "\n",
        "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat Methods 18, 203–211 (2021). https://doi.org/10.1038/s41592-020-01008-z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS7cUQaJujzN"
      },
      "source": [
        "### **Disclaimer**\n",
        "\n",
        "The code and data of this repository are provided to promote reproducible research. They are not intended for clinical care or commercial use.\n",
        "\n",
        "The software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZnoRi9Y7nEB"
      },
      "source": [
        "# **Installing Packages**\n",
        "(Comment out these lines when using for Terra/ Seven Bridges Genomics as a docker image will provide the environment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rpCgi-Wywr98"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get update && apt-get install lz4 git dcm2niix subversion plastimatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ftNvkrON7MvK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#utility to make yaml files easily editable in a notebook cell\n",
        "!pip install yamlmagic thedicomsort pyplastimatch TotalSegmentator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install git+https://github.com/MHubAI/mhubio.git"
      ],
      "metadata": {
        "id": "KoeQgruoW6uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zvqytwpu0ZXX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Download s5cmd version 2.0.0 for Linux 64-bit from GitHub\n",
        "!wget \"https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_Linux-64bit.tar.gz\"\n",
        "\n",
        "# Extract the downloaded tar.gz file\n",
        "!tar -xvzf \"s5cmd_2.0.0_Linux-64bit.tar.gz\"\n",
        "\n",
        "# Remove the downloaded tar.gz file\n",
        "!rm \"s5cmd_2.0.0_Linux-64bit.tar.gz\"\n",
        "\n",
        "# Move the s5cmd binary to /usr/local/bin so it can be used from anywhere on the system\n",
        "!mv s5cmd /usr/local/bin/s5cmd\n",
        "\n",
        "# Remove the dcmqi path\n",
        "!rm CHANGELOG.md LICENSE README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iYf72nA8m0wh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#Set the URL for the dcmqi release to download\n",
        "dcmqi_release_url = \"https://github.com/QIICR/dcmqi/releases/download/v1.2.5/dcmqi-1.2.5-linux.tar.gz\"\n",
        "\n",
        "#Set the path where the downloaded file will be saved\n",
        "dcmqi_download_path = \"dcmqi-1.2.5-linux.tar.gz\"\n",
        "\n",
        "#set the path where the extracted contents of the downloaded file will be saved\n",
        "dcmqi_path = \"dcmqi-1.2.5-linux\"\n",
        "\n",
        "#Download the dcmqi release from the specified URL and save it to the specified path\n",
        "!wget -O $dcmqi_download_path $dcmqi_release_url\n",
        "\n",
        "#Extract the contents of the downloaded file to the specified path\n",
        "!tar -xvf $dcmqi_download_path\n",
        "\n",
        "# Remove the downloaded tar.gz file\n",
        "!rm $dcmqi_download_path\n",
        "\n",
        "#Move the contents of the bin directory within the extracted dcmqi directory to the /bin directory on the system\n",
        "!mv $dcmqi_path/bin/* /bin\n",
        "\n",
        "# Remove the dcmqi path\n",
        "!rm -r $dcmqi_path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing Packages**#"
      ],
      "metadata": {
        "id": "3W2KTzf8U4MR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "non5qVLIcG4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4291b1-3289-4ca6-b1dc-dbff301308ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 04 09:12:15 2023\n",
            "\n",
            "Current directory :/content\n",
            "Python version    : 3.9.16 (main, Dec  7 2022, 01:11:51) \n"
          ]
        }
      ],
      "source": [
        "# Import commonly used libraries\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pathlib import Path\n",
        "from time import sleep\n",
        "\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "import yaml\n",
        "\n",
        "# Set the current working directory \n",
        "curr_dir   = Path().absolute()\n",
        "\n",
        "# Set the time zone to 'US/Eastern' \n",
        "os.environ['TZ'] = 'US/Eastern'\n",
        "time.tzset()\n",
        "\n",
        "# Get the current time \n",
        "current_time = time.strftime('%a %b %d %H:%M:%S %Y', time.localtime())\n",
        "print(current_time)\n",
        "\n",
        "# Print the current directory and the version of Python being used\n",
        "print(\"\\nCurrent directory :{}\".format( curr_dir))\n",
        "print(\"Python version    :\", sys.version.split('\\n')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PExNNZv67Q4N"
      },
      "outputs": [],
      "source": [
        "#Load yamlmagic extension that will be used to create yamlfiles\n",
        "%load_ext yamlmagic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parameters for Papermill**#\n",
        "(useful for parameterizing and/or for Terra/Seven Bridges Genomics)"
      ],
      "metadata": {
        "id": "edIe5qvwa8pS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FxjiDNaPHxoO",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# #Papermill will inject a cell below this cell with the values passed from the runtime\n",
        "# #Remember to tag this cell as 'parameters' in jupyterlab/jupyter notebook \n",
        "# #https://papermill.readthedocs.io/en/latest/\n",
        "\n",
        "# compressedCSVFilePath =''\n",
        "# FastModeStatus= True\n",
        "# converterType='dcm2niix'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Reading CSV File containing s5cmd Urls**#\n",
        "(The csv file expects SeriesInstanceUID and their corresponding S5cmd Urls) The query used for this study: https://github.com/vkt1414/Cloud-Resources-Workflows/blob/main/sqlQueries/nlstCohort.sql\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z5p8tu7egO-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!lz4 /content/drive/MyDrive/idc/batch_1.csv compressedCSVFilePath.csv.lz4\n",
        "\n",
        "\n",
        "# compressedCSVFilePath=glob.glob('*.csv.lz4')[0]"
      ],
      "metadata": {
        "id": "uCiUpt_85lm4",
        "outputId": "7184df60-6fb1-4c6c-aae5-807c8b6529e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compressedCSVFilePath.csv.lz4 already exists; do you wish to overwrite (y/N) ? y\n",
            "Compressed 138154 bytes into 69117 bytes ==> 50.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r3FxH9UDJo-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e652275-4b9e-4735-cd6f-efd053db52a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-04 09:41:12--  https://raw.githubusercontent.com/vkt1414/Cloud-Resources-Workflows/main/sampleManifests/batch_1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 139861 (137K) [text/plain]\n",
            "Saving to: ‘batch_1.csv’\n",
            "\n",
            "\rbatch_1.csv           0%[                    ]       0  --.-KB/s               \rbatch_1.csv         100%[===================>] 136.58K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-04 09:41:12 (6.09 MB/s) - ‘batch_1.csv’ saved [139861/139861]\n",
            "\n",
            "read in 0.016513586044311523 seconds\n"
          ]
        }
      ],
      "source": [
        "# Decompress the compressed csv file in .lz4 format in the current working directory\n",
        "#!lz4 -d $compressedCSVFilePath\n",
        "!wget https://raw.githubusercontent.com/vkt1414/Cloud-Resources-Workflows/main/sampleManifests/batch_1.csv\n",
        "# Use glob to find all files with a .csv extension in the current working directory\n",
        "csv_file_path = glob.glob('*.csv')[0]\n",
        "\n",
        "# Alternatively, you can manually specify the path to the csv file\n",
        "# csv_file_path= '/home/vamsi/Downloads/result.csv'\n",
        "# Record the current time\n",
        "start_time = time.time()\n",
        "\n",
        "# Read the csv file specified by the csv_file_path variable using the pandas library\n",
        "cohort_df = pd.read_csv(csv_file_path, delimiter=',', encoding='utf-8')\n",
        "\n",
        "# Calculate the time it took to read the csv file\n",
        "read_time = time.time() - start_time\n",
        "\n",
        "print('read in ' + str(read_time) + ' seconds')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SeriesInstanceUIDs= cohort_df[\"SeriesInstanceUID\"].values.tolist()\n",
        "SeriesInstanceUIDs"
      ],
      "metadata": {
        "id": "MseN4iOtlGvJ",
        "outputId": "c0ba04cf-1ed1-49ef-f0af-393019c30ed2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1.3.6.1.4.1.14519.5.2.1.7009.9004.118722452529394350711166589345',\n",
              " '1.3.6.1.4.1.14519.5.2.1.7009.9004.431377773401197924485551006033',\n",
              " '1.2.840.113654.2.55.100875189782210690344207306235124901243',\n",
              " '1.2.840.113654.2.55.113040386178547843571271236478024341696',\n",
              " '1.2.840.113654.2.55.142419057730651121165090739113900499978',\n",
              " '1.2.840.113654.2.55.14382674871619950799472325766084940706',\n",
              " '1.2.840.113654.2.55.146601594654322994982630019583270053397',\n",
              " '1.2.840.113654.2.55.154809705591242159075253605419469935510',\n",
              " '1.2.840.113654.2.55.185309182591805634517860395342326800332',\n",
              " '1.2.840.113654.2.55.216614002338888733987350522981366678482',\n",
              " '1.2.840.113654.2.55.22770087029972268579113866309746562015',\n",
              " '1.2.840.113654.2.55.243990451406006403331425809632881193215']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing Source Code From MHubAI repo**#\n",
        "MHubAI utilities and curated TotalSegmentator model are used for converting DICOM Series to a NIfTI and then applying the TotalSegmentator model. Finally the NIfTI file from Inference is converted back to DICOM SEG object \n",
        "```\n",
        "DICOM CT >> NIfTI >> TotalSegmentator >> NIfTI >> DICOM SEG \n",
        "```"
      ],
      "metadata": {
        "id": "j_Xduc9_VBNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Try to remove the directories 'models' and 'mhubio' if they exist\n",
        "try:\n",
        "    shutil.rmtree('models')\n",
        "    shutil.rmtree('mhubio')\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "# Clone the repository from GitHub using a sparse checkout to only download specific files or directories\n",
        "!git clone --filter=blob:none --no-checkout https://github.com/vkt1414/models.git models\n",
        "\n",
        "# Change the current working directory to the newly cloned repository\n",
        "%cd models\n",
        "\n",
        "# Checkout a specific commit\n",
        "!git checkout 33fb1dd\n",
        "\n",
        "\n",
        "# Initialize the sparse-checkout\n",
        "!git sparse-checkout init\n",
        "\n",
        "# Set the sparse-checkout to only include the 'models/totalsegmentator' directory\n",
        "!git sparse-checkout set models/totalsegmentator\n",
        "\n",
        "# Change the current working directory back to the previous one\n",
        "%cd .."
      ],
      "metadata": {
        "id": "WYzfwBH24BJw"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AolLtXOLVt7D"
      },
      "source": [
        "## **Setup mhubio**##\n",
        "\n",
        "`mhbio` is the module of MHubAI that deals with all of the basic operations shared between a large majority of the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "TRmrt00589lv"
      },
      "outputs": [],
      "source": [
        "sys.path.append('.')\n",
        "\n",
        "from mhubio.core import Config, DataType, FileType, CT, SEG\n",
        "\n",
        "from mhubio.modules.importer.UnsortedDicomImporter import UnsortedInstanceImporter\n",
        "from mhubio.modules.importer.DataSorter import DataSorter\n",
        "from mhubio.modules.convert.NiftiConverter import NiftiConverter\n",
        "from mhubio.modules.convert.DsegConverter import DsegConverter\n",
        "from mhubio.modules.organizer.DataOrganizer import DataOrganizer\n",
        "from models.models.totalsegmentator.utils.TotalSegmentatorRunner import TotalSegmentatorRunner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ittQxkqwRYcO"
      },
      "source": [
        "The config file stores all the information needed by the different modules, such as the path to a given file or folder, to execute the end-to-end pipeline.\n",
        "\n",
        "For instance, we specify `data_base_dir` as a `general` argument all the modules are going to share. Then, we specify the module-specific arguments.\n",
        "\n",
        "For instance, we specify the directory where the DICOM data to be sorted are for the `UnsortedInstanceImporter`, by adding `input_dir` to the config file. We can also specify the structure of the sorted directory by setting the `structure` argument for the `DataSorter`, or which `TotalSegmentator` model to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fv1HZ6K8m5h"
      },
      "source": [
        "Write a custom configuration file containing the specifics for all the MHub modules we're going to use, using the `%%writefile` magik (from the `yamlmagic` package).\n",
        "\n",
        "This file is going to be tailored to this specific use case and example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "rkUYdQcg6o0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47c47be-ec12-4be6-cf69-0abd634bf414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting totalsegmentator_config.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile totalsegmentator_config.yml\n",
        "general:\n",
        "  data_base_dir: /content/data\n",
        "modules:\n",
        "  NiftiConverter:\n",
        "    engine: plastimatch\n",
        "  UnsortedInstanceImporter:\n",
        "    input_dir: /content/idc_data\n",
        "  DataSorter:\n",
        "    bypass: True\n",
        "    base_dir: /content/data/sorted\n",
        "    structure: '%SeriesInstanceUID/dicom/%SOPInstanceUID.dcm'\n",
        "  DsegConverter:\n",
        "    dicomseg_json_path: /content/models/models/totalsegmentator/config/dicomseg_metadata_whole.json\n",
        "    skip_empty_slices: True\n",
        "  TotalSegmentatorRunner:\n",
        "    use_fast_mode: True\n",
        "    use_multi_label_output: True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2yqx6DmqGVi"
      },
      "outputs": [],
      "source": [
        "# #Edit the yaml with parameters\n",
        "\n",
        "# with open('totalsegmentator_config.yml', 'r') as file:\n",
        "#     config = yaml.safe_load(file)\n",
        "\n",
        "# # Update the value of use_fast_mode with the value of FastModeStatus\n",
        "# config['general']['data_base_dir'] = os.path.join(curr_dir,'idc_data')\n",
        "# config['modules']['NiftiConverter']['engine'] = converterType\n",
        "# config['modules']['UnsortedInstanceImporter']['input_dir'] = os.path.join(curr_dir,'idc_data')\n",
        "# config['modules']['TotalSegmentatorRunner']['use_fast_mode'] = FastModeStatus\n",
        "\n",
        "# # Write the updated dictionary back to the YAML file\n",
        "# with open('totalsegmentator_config.yml', 'w') as file:\n",
        "#     yaml.safe_dump(config, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n0qjPp1B_mv"
      },
      "source": [
        "# **Running the Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBUqa6Nj9SKe"
      },
      "source": [
        "For instance, the workflow for this example looks like the following:\n",
        "\n",
        "\n",
        "We want to start from DICOM CT data and save the results in DICOM SEG format. We are therefore going to need the relevant `DataType`(s) and `FileType`(s) imported (`CT` and the `SEG`) from our `Config` module. \n",
        "\n",
        "We will also need to import all the converters to run the aforementioned operations (`UnsortedInstanceImporter`, `DataSorter`, `NiftiConverter`, `DsegConverter`, `DataOrganizer`) and the model runner (`TotalSegmentatorRunner`).\n",
        "\n",
        "For more in-depth explanation of the modules, see the following sections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neN3TTcEw_27"
      },
      "source": [
        "After defining a config file for our use case, we can initialize a `Config` object using such `yml` file.\n",
        "\n",
        "The `Config` object is passed along to all the modules, and keeps track of all of the information that need to shared among these modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "Xl7eLutk4_Jv",
        "outputId": "fe5f5ab6-f00a-4892-f8ca-e12d7b59e91d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mhubio.core.Config.Config"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "# config\n",
        "config = Config('totalsegmentator_config.yml')\n",
        "config.verbose = True  \n",
        "type(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QLU18xBxp33"
      },
      "source": [
        "In the next cells, we define a utility function to pull data from the Imaging Data Commons - starting from the DataFrame `patient_df` we have previously defined - and then cross-load the data from the IDC Buckets to this Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "fxthXfJbCjYu"
      },
      "outputs": [],
      "source": [
        "def download_patient_data(series_id):\n",
        "\n",
        "  \"\"\"\n",
        "  Download raw DICOM data and run dicomsort to standardise the input format.\n",
        "  Arguments:\n",
        "    series_id : required - SeriesInstanceUID\n",
        "\n",
        "  Make sure to check the s5cmd urls for the destination directory \n",
        "  In this study, the s5cmd urls have idc_data as the destination \n",
        "\n",
        "  \"\"\"\n",
        "  global cohort_df\n",
        "  s5cmd_file_path = \"s5cmd_manifest.txt\"\n",
        "  series_df=cohort_df[cohort_df['SeriesInstanceUID']==series_id]\n",
        "  series_df[\"s5cmdUrls\"].to_csv(s5cmd_file_path, header = False, index = False)\n",
        "  #remove double quotes from the manifest file\n",
        "  !sed -i 's/\"//g' s5cmd_manifest.txt  \n",
        "\n",
        "  start_time = time.time()\n",
        "  print(\"Copying files from IDC buckets\")\n",
        "\n",
        "  !s5cmd --no-sign-request --endpoint-url https://storage.googleapis.com run s5cmd_manifest.txt  >> /dev/null\n",
        "\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"Done in %g seconds.\"%elapsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "CfbmrWT0uAG-"
      },
      "outputs": [],
      "source": [
        "def download_and_process_series(series_id):\n",
        "    try:\n",
        "      shutil.rmtree('data')\n",
        "      shutil.rmtree('idc_data')\n",
        "      os.remove('s5cmd_manifest')\n",
        "    except OSError:\n",
        "      pass\n",
        "    os.system(\"mkdir -p data/\")\n",
        "    os.system(\"mkdir -p data/idc_data\")\n",
        "    os.system(\"mkdir -p data/input_data\")\n",
        "    os.system(\"mkdir -p data/output_data\")\n",
        "    os.system(\"mkdir -p data/sorted\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    download_patient_data(series_id)\n",
        "    download_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    importer = UnsortedInstanceImporter(config)\n",
        "    importer.execute()\n",
        "    importer_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    DataSorter(config).execute()\n",
        "    DataSorter_time=time.time()- start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    NiftiConverter(config).execute()\n",
        "    NiftiConverter_time = time.time() - start_time\n",
        "\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    TotalSegmentatorRunner(config).execute()\n",
        "    TotalSegmentatorRunner_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    DsegConverter(config).execute()\n",
        "    DsegConverter_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    archive_fn = \"{}.lz4\".format(series_id)\n",
        "    archiving_time = time.time() - start_time\n",
        "\n",
        "    try:\n",
        "     os.remove(archive_fn)\n",
        "    except OSError:\n",
        "     pass\n",
        "    !lz4 --rm \"data/sorted/seg.dcm\" \"$archive_fn\"\n",
        "\n",
        "\n",
        "    log = pd.DataFrame({'SeriesInstanceUID': [series_id]})\n",
        "    log['download_time'] = download_time\n",
        "    log['importer_time'] = importer_time\n",
        "    log['NiftiConverter_time'] = NiftiConverter_time\n",
        "    log['TotalSegmentatorRunner_time'] = TotalSegmentatorRunner_time\n",
        "    log['DsegConverter_time'] = DsegConverter_time\n",
        "    log['archiving_time'] = archiving_time\n",
        "    global runtime_stats\n",
        "    runtime_stats = runtime_stats.append(log, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "QZPCanDagR2D"
      },
      "outputs": [],
      "source": [
        "class MemoryMonitor:\n",
        "    def __init__(self):\n",
        "        self.keep_measuring = True\n",
        "\n",
        "    def measure_usage(self):\n",
        "        cpu_usage = []\n",
        "        ram_usage_mb=[]\n",
        "        time_stamps = []\n",
        "        start_time = time.time()\n",
        "        while self.keep_measuring:\n",
        "            cpu = psutil.cpu_percent()\n",
        "            ram = psutil.virtual_memory()\n",
        "            ram_total_mb = psutil.virtual_memory().total / 1024 / 1024\n",
        "            ram_mb = (ram.total - ram.available) / 1024 / 1024\n",
        "            cpu_usage.append(cpu)\n",
        "            ram_usage_mb.append(ram_mb)\n",
        "            time_stamps.append(time.time()- start_time)\n",
        "            sleep(1)\n",
        "\n",
        "        return cpu_usage, ram_usage_mb, time_stamps, ram_total_mb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9MVJRvf_EKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4065e3-b815-48e5-df5e-a6861acd46d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying files from IDC buckets\n",
            "Done in 0.62939 seconds.\n",
            "\n",
            "--------------------------\n",
            "Start UnsortedInstanceImporter\n",
            "Done in 0.000159025 seconds.\n",
            "\n",
            "--------------------------\n",
            "Start DataSorter\n",
            "adding ct in dicom format with resolved path:  /content/data/sorted/dicom\n",
            "Done in 0.131546 seconds.\n",
            "\n",
            "--------------------------\n",
            "Start NiftiConverter\n",
            "\n",
            "Running 'plastimatch convert' with the specified arguments:\n",
            "  --input /content/data/sorted/dicom\n",
            "  --output-img /content/data/sorted/image.nii.gz\n",
            "... Done.\n",
            "Done in 22.6849 seconds.\n",
            "\n",
            "--------------------------\n",
            "Start TotalSegmentatorRunner\n",
            "Generating multi-label output ('--ml')\n",
            "Running TotalSegmentator in fast mode ('--fast', 3mm)\n",
            ">> run:  TotalSegmentator -i /content/data/sorted/image.nii.gz -o /content/data/sorted/ts-model-out/segmentations.nii.gz --ml --fast\n"
          ]
        }
      ],
      "source": [
        "runtime_stats = pd.DataFrame(columns=['SeriesInstanceUID','download_time','importer_time','DataSorter_time',\n",
        "                                      'NiftiConverter_time', 'TotalSegmentatorRunner_time',\n",
        "                                      'DsegConverter_time',\t'archiving_time', 'cpu_usage','ram_usage_mb', 'ram_total_mb'\n",
        "                                      ])\n",
        "if __name__ == \"__main__\":\n",
        "    for series_id in SeriesInstanceUIDs:\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            monitor = MemoryMonitor()\n",
        "            mem_thread = executor.submit(monitor.measure_usage)\n",
        "            try:\n",
        "                proc_thread = executor.submit(download_and_process_series, series_id)\n",
        "                proc_thread.result()\n",
        "            finally:\n",
        "                monitor.keep_measuring = False\n",
        "                cpu_usage, ram_usage_mb, time_stamps, ram_total_mb = mem_thread.result()\n",
        "                \n",
        "                cpu_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[cpu_idx, runtime_stats.columns.get_loc('cpu_usage')] = [[cpu_usage]]\n",
        "\n",
        "                ram_usage_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_usage_mb_idx, runtime_stats.columns.get_loc('ram_usage_mb')] = [[ram_usage_mb]]\n",
        "                \n",
        "                ram_total_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_total_mb_idx, runtime_stats.columns.get_loc('ram_total_mb')] = [[ram_total_mb]]\n",
        "                \n",
        "                plt.plot(time_stamps, cpu_usage)\n",
        "                plt.ylim(0, 100)\n",
        "                plt.xlabel('Time (s)')\n",
        "                plt.ylabel('CPU usage (%)')\n",
        "                plt.show()\n",
        "\n",
        "                plt.plot(time_stamps, ram_usage_mb)\n",
        "                plt.ylim(0, ram_total_mb)\n",
        "                plt.xlabel('Time (s)')\n",
        "                plt.ylabel('Memory usage (MB)')\n",
        "                plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Compressing Output Files**##"
      ],
      "metadata": {
        "id": "3k9YUDTY3XEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start_time = time.time()\n",
        "# try:\n",
        "#   os.remove('inferenceNiftiFiles.tar.lz4')\n",
        "#   os.remove('metadata.tar.lz4')\n",
        "# except OSError:\n",
        "#   pass\n",
        "# !tar cvf - Inference | lz4 > inferenceNiftiFiles.tar.lz4\n",
        "# !tar cvf - metadata | lz4 > inferenceMetaData.tar.lz4\n",
        "# output_file_archiving_time = time.time() - start_time\n"
      ],
      "metadata": {
        "id": "mRsx128p3Zdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2QqrdvXqQq0"
      },
      "source": [
        "\n",
        "\n",
        "# **Utilization Metrics**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARmBD0lHdz_2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# runtime_stats.to_csv('runtime.csv')\n",
        "# runtime_stats['output_file_archiving_time']=output_file_archiving_time\n",
        "# try:\n",
        "#   os.remove('inferenceUsageMetrics.lz4')\n",
        "# except OSError:\n",
        "#   pass\n",
        "# !lz4 {curr_dir}/runtime.csv inferenceUsageMetrics.lz4\n",
        "# runtime_stats"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
