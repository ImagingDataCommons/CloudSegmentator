{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkt1414/Cloud-Resources-Workflows/blob/main/Notebooks/Totalsegmentator/structuredReports\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deploying your own OHIF viewer instance**"
      ],
      "metadata": {
        "id": "GPX06ozxNh0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to view the results, we will use the output json files created from the body part regression and create a structured report. This will show a label for each slice as an annotation in the custom OHIF viewer. \n",
        "\n",
        "Use these instructions to first deploy your own OHIF instance: \n",
        "\n",
        "https://tinyurl.com/idc-ohif-gcp "
      ],
      "metadata": {
        "id": "CDs6qa5rNfTX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKOcbyxs__u"
      },
      "source": [
        "# **Parameterization**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters for nnUNet"
      ],
      "metadata": {
        "id": "kpOUOudhNl5l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgjKUG-ds-8O"
      },
      "outputs": [],
      "source": [
        "nnunet_model = '3d_fullres' # choose from: \"2d\", \"3d_lowres\", \"3d_fullres\", \"3d_cascade_fullres\"\n",
        "use_tta = True\n",
        "export_prob_maps = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As explained above,  you will have the option of being able to deploy your own instance of the OHIF viewer. Replace the name with your own in the variable `my_ohif_app`. "
      ],
      "metadata": {
        "id": "EkfqWtCjNz5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my_ohif_app = None ### UNCOMMENT if you do not want to set up and use a viewer \n",
        "# my_ohif_app = 'idc-tester-dk' ### COMMENT out if you do not want to set up and use a viewer \n",
        "# my_ohif_app = 'idc2serversdeploy-3c769' # the two server solution -- we will need to have instructions to user on how to set this up. \n",
        "my_ohif_app = 'idc-tester-dk-2-server'"
      ],
      "metadata": {
        "id": "kPxQOcRbNQZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZnoRi9Y7nEB"
      },
      "source": [
        "# **Environment Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These packages need to be updated first."
      ],
      "metadata": {
        "id": "vOeZn4GpEKwo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FLzDDB9dEae"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "\n",
        "!pip install pyradiomics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages for both use cases. "
      ],
      "metadata": {
        "id": "pHlRsAvxE_4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "non5qVLIcG4M",
        "outputId": "73a03449-8436-4a4e-844c-4530a2205a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug 25 22:39:34 2022\n",
            "\n",
            "Current directory : /content\n",
            "Hostname          : f71feffedd67\n",
            "Username          : root\n",
            "Python version    : 3.7.12 (default, Jan 15 2022, 18:48:18) \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import yaml\n",
        "import time\n",
        "import tqdm\n",
        "import copy\n",
        "import json \n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# useful information\n",
        "curr_dir = !pwd\n",
        "curr_droid = !hostname\n",
        "curr_pilot = !whoami\n",
        "\n",
        "print(time.asctime(time.localtime()))\n",
        "\n",
        "print(\"\\nCurrent directory :\", curr_dir[-1])\n",
        "print(\"Hostname          :\", curr_droid[-1])\n",
        "print(\"Username          :\", curr_pilot[-1])\n",
        "\n",
        "print(\"Python version    :\", sys.version.split('\\n')[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the other packages that nnUNet and BPR depend on. "
      ],
      "metadata": {
        "id": "GpZSBCG4FEgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install and import the packages needed to create Structured Reports (SR). "
      ],
      "metadata": {
        "id": "23B3Xy8buP45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages for the structured report \n",
        "\n",
        "!pip uninstall highdicom\n",
        "!git clone https://github.com/herrmannlab/highdicom.git\n",
        "#!cd highdicom && python setup.py install\n",
        "!cd highdicom && pip install .\n",
        "\n",
        "import highdicom\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import highdicom as hd\n",
        "\n",
        "from pydicom.uid import generate_uid\n",
        "from pydicom.filereader import dcmread\n",
        "from pydicom.sr.codedict import codes\n",
        "\n",
        "from highdicom.sr.content import (\n",
        "    FindingSite,\n",
        "    ImageRegion,\n",
        "    ImageRegion3D,\n",
        "    SourceImageForRegion,\n",
        "    SourceImageForMeasurement,\n",
        "    SourceImageForMeasurementGroup\n",
        ")\n",
        "from highdicom.sr.enum import GraphicTypeValues3D\n",
        "from highdicom.sr.enum import GraphicTypeValues\n",
        "from highdicom.sr.sop import Comprehensive3DSR, ComprehensiveSR\n",
        "from highdicom.sr.templates import (\n",
        "    DeviceObserverIdentifyingAttributes,\n",
        "    Measurement,\n",
        "    MeasurementProperties,\n",
        "    MeasurementReport,\n",
        "    MeasurementsAndQualitativeEvaluations,\n",
        "    ObservationContext,\n",
        "    ObserverContext,\n",
        "    PersonObserverIdentifyingAttributes,\n",
        "    PlanarROIMeasurementsAndQualitativeEvaluations,\n",
        "    RelationshipTypeValues,\n",
        "    TrackingIdentifier,\n",
        "    QualitativeEvaluation,\n",
        "    ImageLibrary,\n",
        "    ImageLibraryEntryDescriptors\n",
        ")\n",
        "from highdicom.sr.value_types import (\n",
        "    CodedConcept,\n",
        "    CodeContentItem,\n",
        ")\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(\"highdicom.sr.sop\")\n",
        "logger.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "m0Ijwc0uuT4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1GXUtRRrIlT"
      },
      "source": [
        "Copy the JSON metadata file (generated using [...]) from the repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQiQ9Zw9rMoD"
      },
      "outputs": [],
      "source": [
        "# bucket_data_base_uri = os.path.join(bucket_base_uri, \"nnunet/data\")\n",
        "# dicomseg_json_uri = \"s3://idc-medima-paper/nnunet/data/dicomseg_metadata.json\"\n",
        "# dicomseg_json_path = \"/content/data/dicomseg_metadata.json\"\n",
        "\n",
        "# !$s5cmd_path --endpoint-url https://storage.googleapis.com cp $dicomseg_json_uri $dicomseg_json_path\n",
        "\n",
        "dicomseg_json_path = \"/content/data/dicomseg_metadata.json\"\n",
        "!wget -N -P '/content/data' https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/nnunet/data/dicomseg_metadata.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the csv that will contain the mapping for the nnUNet features -- this is needed for the creation of the structured report. "
      ],
      "metadata": {
        "id": "k96HJvPLR467"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/nnunet/data/nnunet_segments_code_mapping.csv\n",
        "nnunet_segments_code_mapping_df = pd.read_csv(\"nnunet_segments_code_mapping.csv\")\n",
        "\n",
        "!wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/nnunet/data/nnunet_shape_features_code_mapping.csv\n",
        "nnunet_shape_features_code_mapping_df = pd.read_csv(\"nnunet_shape_features_code_mapping.csv\")"
      ],
      "metadata": {
        "id": "r1lGxWNoR-Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the csv that will contain the mapping for the BPR regions - this is needed for the creation of the structured report. "
      ],
      "metadata": {
        "id": "j80jt2Syqy7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/bpr/data/bpr_regions_code_mapping.csv\n",
        "bpr_regions_df = pd.read_csv(\"bpr_regions_code_mapping.csv\")"
      ],
      "metadata": {
        "id": "P0UDG2pBq3Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the csv that will contain the mapping for the BPR landmarks -- this is needed for the creation of the structured report. "
      ],
      "metadata": {
        "id": "Dxy1iuXseHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -N https://raw.githubusercontent.com/ImagingDataCommons/ai_medima_misc/main/bpr/data/bpr_landmarks_code_mapping.csv\n",
        "landmarks_df = pd.read_csv(\"bpr_landmarks_code_mapping.csv\")"
      ],
      "metadata": {
        "id": "S06l8-SjeE7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JBJmF7rmz1S"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Z2S0OdAtkz"
      },
      "source": [
        "---\n",
        "\n",
        "# **Function Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtbQY2pf6UlF"
      },
      "source": [
        "## **Data Download and Preparation**\n",
        "\n",
        "The following function handles the download of a single patient data from the IDC buckets using `gsutil cp`. Furthermore, to organise the data in a more human-understandable and, above all, standardized fashion, the function makes use of [DICOMSort](https://github.com/pieper/dicomsort).\n",
        "\n",
        "DICOMSort is an open source tool for custom sorting and renaming of dicom files based on their specific DICOM tags. In our case, we will exploit DICOMSort to organise the DICOM data by `PatientID` and `Modality` - so that the final directory will look like the following:\n",
        "\n",
        "```\n",
        "raw/nsclc-radiomics/dicom/$PatientID\n",
        " └─── CT\n",
        "       ├─── $SOPInstanceUID_slice0.dcm\n",
        "       ├─── $SOPInstanceUID_slice1.dcm\n",
        "       ├───  ...\n",
        "       │\n",
        "      RTSTRUCT \n",
        "       ├─── $SOPInstanceUID_RTSTRUCT.dcm\n",
        "      SEG\n",
        "       └─── $SOPInstanceUID_RTSEG.dcm\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JDNAPJivEUm"
      },
      "outputs": [],
      "source": [
        "def download_series_data_s5cmd(raw_base_path, \n",
        "                               sorted_base_path,\n",
        "                               series_df, \n",
        "                               remove_raw = True):\n",
        "\n",
        "  \"\"\"\n",
        "  Download raw DICOM data and run dicomsort to standardise the input format.\n",
        "  Uses s5cmd instead of gsutil which allows for a faster download. \n",
        "\n",
        "  Arguments:\n",
        "    raw_base_path    : required - path to the folder where the raw data will be stored.\n",
        "    sorted_base_path : required - path to the folder where the sorted data will be stored.\n",
        "    series_df        : required - Pandas dataframe (returned from BQ) storing all the\n",
        "                                  series information required to pull data from the IDC buckets.\n",
        "    remove_raw       : optional - whether to remove or not the raw non-sorted data\n",
        "                                  (after sorting with dicomsort). Defaults to True.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # Get and create the download path \n",
        "  series_id = series_df[\"SeriesInstanceUID\"].values[0]\n",
        "  download_path = os.path.join(raw_base_path, series_id)\n",
        "  if not os.path.exists(download_path):\n",
        "    os.mkdir(download_path)\n",
        "\n",
        "  # Create the text file to hold gsc_url \n",
        "  gcsurl_temp = \"cp \" + series_df[\"gcs_url\"].str.replace(\"gs://\",\"s3://\") + \" \" + download_path \n",
        "  gs_file_path = \"gcs_paths.txt\"\n",
        "  gcsurl_temp.to_csv(gs_file_path, header = False, index = False)\n",
        "\n",
        "  # Get the path for s5cmd\n",
        "  if os.path.exists('/content/s5cmd'):\n",
        "    s5cmd_path = '/content/s5cmd'\n",
        "  else:\n",
        "    s5cmd_path = 's5cmd'\n",
        "\n",
        "  # Download using s5cmd \n",
        "  start_time = time.time()\n",
        "  # download_cmd = [\"/content/s5cmd\",\"--endpoint-url\", \"https://storage.googleapis.com\", \"run\", gs_file_path]\n",
        "  # download_cmd = [\"s5cmd\",\"--endpoint-url\", \"https://storage.googleapis.com\", \"run\", gs_file_path]\n",
        "  download_cmd = [s5cmd_path,\"--endpoint-url\", \"https://storage.googleapis.com\", \"run\", gs_file_path]\n",
        "  proc = subprocess.Popen(download_cmd)\n",
        "  proc.wait()\n",
        "  elapsed = time.time() - start_time \n",
        "  print (\"Done download in %g seconds.\"%elapsed)\n",
        "\n",
        "  # Sort files \n",
        "  start_time = time.time()\n",
        "  print(\"\\nSorting DICOM files...\" )\n",
        "  # !python src/dicomsort/dicomsort.py -u $download_path $sorted_base_path/%PatientID/%Modality/%SOPInstanceUID.dcm\n",
        "  !python src/dicomsort/dicomsort.py -u $download_path $sorted_base_path/%SeriesInstanceUID/%Modality/%SOPInstanceUID.dcm\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"Done sorting in %g seconds.\"%elapsed)\n",
        "\n",
        "  print(\"Sorted DICOM data saved at: %s\"%(os.path.join(sorted_base_path, series_id)))\n",
        "\n",
        "  # get rid of the temporary folder, storing the unsorted DICOM data \n",
        "  if remove_raw:\n",
        "    print(\"Removing un-sorted data at %s...\"%(download_path))\n",
        "    !rm -r $download_path\n",
        "    print(\"... Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLGqxh6063lE"
      },
      "source": [
        "---\n",
        "\n",
        "## **Data Preprocessing**\n",
        "\n",
        "Brief description here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dcm2niix_dicom_ct_to_nifti(sorted_base_path,\n",
        "                               processed_nifti_path,\n",
        "                               pat_id):\n",
        "  \n",
        "  \"\"\"\n",
        "    This function converts the sorted DICOM patient data to a NIfTI file \n",
        "    (CT volume) using dcm2niix. \n",
        "\n",
        "    Arguments:\n",
        "      sorted_base_path     : required - path to the folder where the sorted data should be stored.\n",
        "      processed_nifti_path : required - path to the folder where the preprocessed NIfTI data are stored\n",
        "      pat_id               : required - patient ID (used for naming purposes).\n",
        "    \n",
        "    Outputs:\n",
        "      writes the NIfTI file os.path.join(processed_nifti_path, pat_id, pat_id + 'CT.nii.gz') out \n",
        "      \n",
        "    \"\"\"\n",
        "\n",
        "  success = 0\n",
        "\n",
        "  # given that everything is standardised already, compute the paths\n",
        "  path_to_dicom_ct_folder = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "  \n",
        "  # sanity check\n",
        "  assert(os.path.exists(path_to_dicom_ct_folder))\n",
        "  \n",
        "  pat_dir_nifti_path = os.path.join(processed_nifti_path, pat_id)\n",
        "  if not os.path.exists(pat_dir_nifti_path):\n",
        "    os.mkdir(pat_dir_nifti_path)\n",
        "\n",
        "  # output nii CT\n",
        "  ct_nifti_path = os.path.join(pat_dir_nifti_path, pat_id + \"_CT.nii.gz\")\n",
        "\n",
        "  # DICOM CT to nii conversion \n",
        "  if not os.path.exists(ct_nifti_path):\n",
        "    cmd = 'dcm2niix -z y -m y -o %s  %s ' % (pat_dir_nifti_path, path_to_dicom_ct_folder)\n",
        "    print(cmd)\n",
        "    ret = os.system(cmd)\n",
        "    print (ret)\n",
        "    num_files = len([f for f in os.listdir(pat_dir_nifti_path) if f.endswith('.nii.gz')])\n",
        "    print('num_files: ' + str(num_files))\n",
        "\n",
        "    # If only one file created, keep it. \n",
        "    if num_files == 1: \n",
        "      success = 0 \n",
        "      # rename the file \n",
        "      src = os.path.join(pat_dir_nifti_path, [f for f in os.listdir(pat_dir_nifti_path) if f.endswith('.nii.gz')][0]) \n",
        "      os.rename(src, ct_nifti_path)  \n",
        "    # For each file, if multiple are created, find the one that has the same number of slices as dicom files \n",
        "    else: \n",
        "      file_names = [os.path.join(pat_dir_nifti_path,f) for f in os.listdir(pat_dir_nifti_path) if f.endswith('.nii.gz')]\n",
        "      num_slices = [nib.load(f).get_fdata().shape[2] for f in file_names] \n",
        "      num_dicom_files = len(os.listdir(path_to_dicom_ct_folder))\n",
        "      success = -1 \n",
        "      if num_dicom_files in num_slices:\n",
        "        index = num_slices.index(num_dicom_files) \n",
        "        src = file_names[index]\n",
        "        # rename the file\n",
        "        os.rename(src, ct_nifti_path)\n",
        "        success = 0  \n",
        "\n",
        "    # if num_files != 1: \n",
        "    #   success = -1 \n",
        "    # else: \n",
        "    #   success = 0\n",
        "    #   # rename the file \n",
        "    #   src = os.path.join(pat_dir_nifti_path, [f for f in os.listdir(pat_dir_nifti_path) if f.endswith('.nii.gz')][0]) \n",
        "    #   os.rename(src, ct_nifti_path)  \n",
        "\n",
        "  return success \n"
      ],
      "metadata": {
        "id": "JQcdwBbV1qd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0lqtQ8vNlkW"
      },
      "outputs": [],
      "source": [
        "def prep_input_data_bpr(processed_nifti_path, model_input_folder, pat_id):\n",
        "  \n",
        "  \"\"\"\n",
        "  Copies the nifti file to the appropriate model_input_folder\n",
        "\n",
        "  Arguments:\n",
        "    processed_nifti_path : required - path to the folder where the input nifti are stored \n",
        "    model_input_folder   : required - path to the folder where the nifti files for BPR will be stored\n",
        "    pat_id               : required - patient ID (used for naming purposes).\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # FIXME: ok for a notebook; for scripting, change this to `shutil`\n",
        "\n",
        "  pat_dir_nifti_path = os.path.join(processed_nifti_path, pat_id)\n",
        "  ct_nifti_path = os.path.join(pat_dir_nifti_path, pat_id + \"_CT.nii.gz\")\n",
        "  \n",
        "  # copy_to_path = os.path.join(model_input_folder, pat_id + \"_0000.nii.gz\")\n",
        "  # copy_to_path = os.path.join(model_input_folder, pat_id + \"_CT.nii.gz\")\n",
        "  copy_to_path = os.path.join(model_input_folder, pat_id + \".nii.gz\")\n",
        "    \n",
        "  # copy NIfTI to the right dir for bpr processing\n",
        "  if not os.path.exists(copy_to_path):\n",
        "    print(\"Copying %s\\nto %s...\"%(ct_nifti_path, copy_to_path))\n",
        "    !cp $ct_nifti_path $copy_to_path\n",
        "    print(\"... Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-7IE4iEIBpD"
      },
      "source": [
        "---\n",
        "\n",
        "## **Data Processing**\n",
        "\n",
        "Brief description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIwS_f9gRou0"
      },
      "outputs": [],
      "source": [
        "def process_patient_bpr(model_input_folder, model_output_folder, model):\n",
        "  \n",
        "  \"\"\"\n",
        "  Predict the body part region scores from the model_input_folder. \n",
        "\n",
        "  Arguments:\n",
        "    model_input_folder  : required - path to the folder where the data to be inferred should be stored.\n",
        "    model_output_folder : required - path to the folder where the output json files will be stored.\n",
        "\n",
        "  Outputs:\n",
        "    This function produces a json file per input nifti that stores the body part scores. \n",
        "  \"\"\"\n",
        "\n",
        "  start_time = time.time()\n",
        "  # bpreg_inference(model_input_folder, model_output_folder)\n",
        "  bpreg_inference(model_input_folder, model_output_folder, model)\n",
        "  end_time = time.time()\n",
        "  elapsed = end_time-start_time \n",
        "  print(\"Done in %g seconds.\"%elapsed)\n",
        "\n",
        "\n",
        "  return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iENa_4flTd1m"
      },
      "source": [
        "---\n",
        "\n",
        "## **Data Postprocessing**\n",
        "\n",
        "Description here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwYHPQwsj620"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIzPeXuog9K"
      },
      "source": [
        "---\n",
        "\n",
        "## **General Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM4vXFnxolA-"
      },
      "outputs": [],
      "source": [
        "def file_exists_in_bucket_s3(project_name, bucket_name, file_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Check whether a file exists in the specified Google Cloud Storage Bucket.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - file GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_exists : boolean variable, True if the file exists in the specified,\n",
        "                  bucket, at the specified location; False if it doesn't.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  # bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  bucket_gs_url = \"s3://%s/\"%(bucket_name)\n",
        "  path_to_file_relative = file_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "  print(\"Searching `%s` for: \\n%s\\n\"%(bucket_gs_url, path_to_file_relative))\n",
        "\n",
        "  file_exists = bucket.blob(path_to_file_relative).exists(storage_client)\n",
        "\n",
        "  return file_exists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS6WfCfgiS-b"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwQKxMXchi0h"
      },
      "outputs": [],
      "source": [
        "def listdir_bucket_s3(project_name, bucket_name, dir_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Export DICOM SEG object from segmentation masks stored in NRRD files.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - directory GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_list : list of files in the specified GCS bucket.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  # bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  bucket_gs_url = \"s3://%s/\"%(bucket_name)\n",
        "  path_to_dir_relative = dir_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "\n",
        "  print(\"Getting the list of files at `%s`...\"%(dir_gs_uri))\n",
        "\n",
        "  file_list = list()\n",
        "\n",
        "  for blob in storage_client.list_blobs(bucket_name,  prefix = path_to_dir_relative):\n",
        "    fn = os.path.basename(blob.name)\n",
        "    file_list.append(fn)\n",
        "\n",
        "  return file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm-gnaS8JqRB"
      },
      "outputs": [],
      "source": [
        "def create_dataset(project_name, dataset_id):\n",
        "\n",
        "  \"\"\"\n",
        "  Create a dataset that will store the cohort_df table \n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    dataset_id   : required - name of the dataset to create\n",
        "  \n",
        "  Returns:\n",
        "    dataset : returns the dataset created \n",
        "  \"\"\"\n",
        "\n",
        "  # Construct a BigQuery client object.\n",
        "  client = bigquery.Client(project=project_name)\n",
        "\n",
        "  # Construct a full Dataset object to send to the API.\n",
        "  dataset_id_full = \".\".join([project_name, dataset_id])\n",
        "  dataset = bigquery.Dataset(dataset_id_full)\n",
        "\n",
        "  # TODO(developer): Specify the geographic location where the dataset should reside.\n",
        "  dataset.location = \"US\"\n",
        "\n",
        "  # Send the dataset to the API for creation, with an explicit timeout.\n",
        "  # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
        "  # exists within the project.\n",
        "  # dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
        "  dataset = client.create_dataset(dataset)  # Make an API request.\n",
        "\n",
        "  print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
        "\n",
        "  return dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLz4e7-E7xqt"
      },
      "outputs": [],
      "source": [
        "def dataset_exists_in_project(project_id, dataset_id):\n",
        "  \"\"\"Check if a dataset exists in a project\"\"\"\n",
        "\n",
        "  from google.cloud import bigquery\n",
        "  from google.cloud.exceptions import NotFound\n",
        "\n",
        "  client = bigquery.Client()\n",
        "  dataset_id_full = '.'.join([project_id, dataset_id])\n",
        "\n",
        "  try:\n",
        "      client.get_dataset(dataset_id_full)  \n",
        "      return True \n",
        "  except NotFound:\n",
        "      return False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewKnaXcF72iz"
      },
      "outputs": [],
      "source": [
        "def create_dataset(project_name, dataset_id):\n",
        "\n",
        "  \"\"\"\n",
        "  Create a dataset that will store the cohort_df table \n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    dataset_id   : required - name of the dataset to create\n",
        "  \n",
        "  Returns:\n",
        "    dataset : returns the dataset created \n",
        "  \"\"\"\n",
        "\n",
        "  # Construct a BigQuery client object.\n",
        "  client = bigquery.Client(project=project_name)\n",
        "\n",
        "  # Construct a full Dataset object to send to the API.\n",
        "  dataset_id_full = \".\".join([project_name, dataset_id])\n",
        "  dataset = bigquery.Dataset(dataset_id_full)\n",
        "\n",
        "  # TODO(developer): Specify the geographic location where the dataset should reside.\n",
        "  dataset.location = \"US\"\n",
        "\n",
        "  # Send the dataset to the API for creation, with an explicit timeout.\n",
        "  # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
        "  # exists within the project.\n",
        "  # dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
        "  dataset = client.create_dataset(dataset)  # Make an API request.\n",
        "\n",
        "  print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
        "\n",
        "  return dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfojbZ4GTuEf"
      },
      "outputs": [],
      "source": [
        "def get_dataframe_from_table(project_name, dataset_id, table_id_name):\n",
        "  \n",
        "  \"\"\"\n",
        "  Gets the pandas dataframe from the saved big query table \n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    dataset_id   : required - name of the dataset already created \n",
        "    table_id     : required - name of the table to create \n",
        "  \n",
        "  Returns: \n",
        "    cohort_df    : the cohort as a pandas dataframe\n",
        "  \"\"\"\n",
        "\n",
        "  table_id = \"%s.%s.%s\"%(project_name, dataset_id, table_id_name)\n",
        "\n",
        "  # the query we are going to execute to gather data about the selected cohort\n",
        "  query_str = \"SELECT * FROM `%s`\"%(table_id)\n",
        "\n",
        "  # init the BQ client\n",
        "  client = bigquery.Client(project = project_name)\n",
        "\n",
        "  # run the query\n",
        "  query_job = client.query(query_str)\n",
        "\n",
        "  # convert the results to a Pandas dataframe\n",
        "  # cohort_df = query_job.to_dataframe()\n",
        "  cohort_df = query_job.to_dataframe(create_bqstorage_client=True)\n",
        "\n",
        "\n",
        "  return cohort_df \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cya9qL50iU1S"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF9cFrUlx-dk"
      },
      "outputs": [],
      "source": [
        "def modify_dicomseg_json_file(dicomseg_json_path, dicomseg_json_path_modified, SegmentAlgorithmName):\n",
        "\n",
        "  \"\"\"\n",
        "  This function writes out a new metadata json file for the DICOM Segmentation object. \n",
        "  It sets the SegmentAlgorithmName to the one provided as input. \n",
        "\n",
        "  Arguments:\n",
        "    dicomseg_json_path          : path of the original dicomseg json file \n",
        "    dicomseg_json_path_modified : the new json file to write to disk \n",
        "    SegmentAlgorithmName        : the field to replace\n",
        "    \n",
        "  Returns:\n",
        "    The json file is written out to dicomseg_json_path_modified \n",
        "\n",
        "  \"\"\"\n",
        "  f = open(dicomseg_json_path)\n",
        "  meta_json = json.load(f)\n",
        "\n",
        "  meta_json_modified = copy.deepcopy(meta_json)\n",
        "  num_regions = len(meta_json_modified['segmentAttributes'])\n",
        "  for n in range(0,num_regions): \n",
        "    meta_json_modified['segmentAttributes'][n][0]['SegmentAlgorithmName'] = SegmentAlgorithmName\n",
        "\n",
        "  with open(dicomseg_json_path_modified, 'w') as f: \n",
        "    json.dump(meta_json_modified, f)\n",
        "\n",
        "  return \n",
        "\n",
        "  # dicomseg_json_uri = \"s3://idc-medima-paper/nnunet/data/dicomseg_metadata.json\"\n",
        "  # dicomseg_json_path = \"/content/data/dicomseg_metadata.json\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def append_row_to_bq_table_with_query(project_name, dataset_name, table_name, value_SeriesInstanceUID, row_to_insert):\n",
        "\n",
        "  bpr_table_id_fullname = '.'.join([project_name, dataset_name, table_name])\n",
        "\n",
        "  query = f\"\"\"\n",
        "    SELECT \n",
        "      COUNT(SeriesInstanceUID) AS num_instances\n",
        "    FROM \n",
        "      {bpr_table_id_fullname}\n",
        "    WHERE \n",
        "      SeriesInstanceUID IN UNNEST (@pat_id_change);\n",
        "  \"\"\"\n",
        "  job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter(\"pat_id_change\", \"STRING\", [value_SeriesInstanceUID])]) \n",
        "  client = bigquery.Client(project=project_name)\n",
        "  result = client.query(query, job_config=job_config)\n",
        "  df_result = result.to_dataframe()\n",
        "  count = df_result['num_instances'][0]\n",
        "  print(count)\n",
        "  \n",
        "  if (count>=1):\n",
        "    print(\"Cannot insert row because seriesInstanceUID exists in table\")\n",
        "  else:\n",
        "    print(\"Inserting row into table\")\n",
        "    client = bigquery.Client(project=project_name)\n",
        "    dataset = client.dataset(dataset_name)\n",
        "    table_ref = dataset.table(table_name)\n",
        "    client.insert_rows_json(table = table_ref,\n",
        "                            json_rows = row_to_insert,\n",
        "                            skip_invalid_rows = False,\n",
        "                            ignore_unknown_values = False)\n",
        "\n",
        "  return "
      ],
      "metadata": {
        "id": "rQ47mqMBpOw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP healthcare functions"
      ],
      "metadata": {
        "id": "yw76AqnXKQ9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gcp_healthcare_dataset(project_name, location_id, dataset_id):\n",
        "  \"\"\"Creates a GCP healthcare dataset using gcloud commands if it doesn't exist\n",
        "\n",
        "  Inputs: \n",
        "    project_name : the project id \n",
        "    location_id  : the location of the project, e.g. us-central1\n",
        "    dataset_id   : the name of the dataset to create \n",
        "\n",
        "  Outputs:\n",
        "    creates the dataset if it doesn't exist \n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  # First let's list the datasets that we already have for our particular project_id and location_id\n",
        "  datasets = !gcloud healthcare datasets list --project $project_name --location $location_id --format=\"value(ID)\" \n",
        "  print ('datasets that exist for project_id ' + str(project_name) + ', location ' + str(location_id) + ': ' + str(datasets))\n",
        "  \n",
        "  # If the dataset doesn't exist, create it \n",
        "  if not (dataset_id in datasets):\n",
        "    try:\n",
        "      !gcloud healthcare datasets create --project $project_name $dataset_id --location $location_id\n",
        "      print (\"Created dataset \" + str(dataset_id))\n",
        "    except OSError as err:\n",
        "      print(\"Error: %s : %s\" % (\"Unable to create dataset\", err.strerror)) \n",
        "\n",
        "  # As a check, we'll list the datasets again.\n",
        "  datasets = !gcloud healthcare datasets list --project $project_name --location $location_id --format=\"value(ID)\" \n",
        "  print ('datasets that exist for project_id ' + str(project_name) + ', location ' + str(location_id) + ': ' + str(datasets))\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "uUlI4_u_KXIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Iro99RvLtoI"
      },
      "outputs": [],
      "source": [
        "def create_gcp_healthcare_dicom_datastore(project_name, location_id, dataset_id, datastore_id):\n",
        "  \"\"\"Creates a GCP healthcare DICOM datastore using gcloud commands if it doesn't exist\n",
        "\n",
        "  Inputs: \n",
        "    project_name : the project id \n",
        "    location_id  : the location of the project, e.g. us-central1\n",
        "    dataset_id   : the dataset id\n",
        "    datastore_id : the DICOM datastore to create \n",
        "\n",
        "  Outputs:\n",
        "    creates the DICOM datastore if it doesn't exist\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # First list the datastores that exist in the dataset\n",
        "  datastores = !gcloud healthcare dicom-stores list --project $project_name --location $location_id --dataset $dataset_id --format=\"value(ID)\"\n",
        "  print ('datastores that exist for project_id ' + str(project_name) + ', location ' + str(location_id) + ', dataset ' + str(dataset_id) + ': ' + str(datastores))\n",
        "\n",
        "  # If the dicom_store_id doesn't exist, create it\n",
        "  if not (datastore_id in datastores):\n",
        "    try:\n",
        "      !gcloud healthcare dicom-stores create $datastore_id --project $project_name --location $location_id --dataset $dataset_id --format=\"value(ID)\" \n",
        "      print (\"Created DICOM datastore \" + str(datastore_id))\n",
        "    except OSError as err:\n",
        "      print(\"Error: %s : %s\" % (\"Unable to create datastore\", err.strerror)) \n",
        "\n",
        "  # As a check, we'll list the datastores again.\n",
        "  datastores = !gcloud healthcare dicom-stores list --project $project_name --location $location_id --dataset $dataset_id --format=\"value(ID)\"\n",
        "  print ('datastores that exist for project_id ' + str(project_name) + ', location ' + str(location_id) + ', dataset ' + str(dataset_id) + ': ' + str(datastores))\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ohif_viewer_2server_url(my_ohif_app, project_name, location_id, dataset_id, datastore_id, study_id): \n",
        "  \"\"\"Forms the url for the 2 server OHIF viewer app \n",
        "\n",
        "  Inputs: \n",
        "    my_ohif_app  : name of the app \n",
        "    project_name : the project id \n",
        "    location_id  : the location of the project, e.g. us-central1\n",
        "    dataset_id   : the dataset id\n",
        "    datastore_id : the DICOM datastore to create \n",
        "    study_id     : the study id where the raw CT data is located \n",
        "\n",
        "  Outputs:\n",
        "    creates the OHIF viewer url, e.g \n",
        "    https://idc2serversdeploy-3c769.web.app/viewer/1.2.840.113654.2.55.171889818252959598680709500375734141432!secondGoogleServer=/projects/idc-external-018/locations/us-central1/datasets/dataset_nlst/dicomStores/2d-tta-seg-only\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  ohif_viewer_url = os.path.join(\"https://\", \n",
        "                                 my_ohif_app + '.web.app', \n",
        "                                 'viewer', \n",
        "                                 study_id + '!secondGoogleServer=',\n",
        "                                 'projects', project_name, \n",
        "                                 'locations', location_id, \n",
        "                                 'datasets', dataset_id, \n",
        "                                 'dicomStores', datastore_id)\n",
        "\n",
        "  return ohif_viewer_url \n",
        "  "
      ],
      "metadata": {
        "id": "5ngz0TLKxaQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ohif_viewer_url(my_ohif_app, project_name, location_id, dataset_id, datastore_id, study_id): \n",
        "  \"\"\"Forms the url for the OHIF viewer app \n",
        "\n",
        "  Inputs: \n",
        "    my_ohif_app  : name of the app \n",
        "    project_name : the project id \n",
        "    location_id  : the location of the project, e.g. us-central1\n",
        "    dataset_id   : the dataset id\n",
        "    datastore_id : the DICOM datastore to create \n",
        "    study_id     : the study id where the raw CT data is located \n",
        "\n",
        "  Outputs:\n",
        "    creates the OHIF viewer url, e.g \n",
        "    https://idc-tester-dk.web.app/projects/idc-external-018/locations/us-central1/datasets/siim_cmimi22_dataset/dicomStores/siim_cmimi22_datastore/study/1.3.6.1.4.1.32722.99.99.62087908186665265759322018723889952421\n",
        "  \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  ohif_viewer_url = os.path.join(\"https://\", \n",
        "                                 my_ohif_app + '.web.app', \n",
        "                                 'projects', project_name, \n",
        "                                 'locations', location_id, \n",
        "                                 'datasets', dataset_id, \n",
        "                                 'dicomStores', datastore_id, \n",
        "                                 'study', study_id)\n",
        "\n",
        "  return ohif_viewer_url \n",
        "  "
      ],
      "metadata": {
        "id": "wYlkareQCtcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup"
      ],
      "metadata": {
        "id": "oj6yeGqirUqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_files_in_nnunet_path(series_id, processed_base_path, model_output_folder_nnunet, processed_nrrd_path_nnunet):\n",
        "\n",
        "  \"\"\"Cleans up the nnUnet files for the next run\n",
        "\n",
        "  Inputs: \n",
        "    series_id                  : series_id being processed \n",
        "    processed_base_path        : base path to hold processed results -- e.g. \"/content/data/processed/nlst/\"\n",
        "    model_output_folder_nnunet : output results for nnuNet -- e.g. \"/content/data/nnunet/nnunet_output/\"\n",
        "    processed_nrrd_path_nnunet : the nrrd results of nnUNet -- e.g. os.path.join(processed_base_path, \"nnunet\", \"nrrd\")\n",
        "\n",
        "  Outputs:\n",
        "    cleans the files in the respective directories \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # delete the files in the pat_dir_dicomseg_path\n",
        "  processed_dicomseg_path = os.path.join(processed_base_path, \"nnunet\", \"dicomseg\")\n",
        "  pat_dir_dicomseg_path = os.path.join(processed_dicomseg_path, series_id)\n",
        "  # print ('removing files from: ' + pat_dir_dicomseg_path)\n",
        "  !rm -r $pat_dir_dicomseg_path/*\n",
        "  !rmdir $pat_dir_dicomseg_path\n",
        "\n",
        "  # remove files from the nnunet output prediction \n",
        "  !rm $model_output_folder_nnunet/*.nii.gz\n",
        "  !rm $model_output_folder_nnunet/*.npz\n",
        "  !rm $model_output_folder_nnunet/*.pkl\n",
        "  !rm $model_output_folder_nnunet/*.json\n",
        "\n",
        "  # delete the files in the processed_nrrd_path_nnunet\n",
        "  processed_nrrd_path_nnunet_segments = os.path.join(processed_nrrd_path_nnunet,series_id,'pred_softmax')\n",
        "  !rm $processed_nrrd_path_nnunet_segments/*.nrrd\n",
        "  !rm $processed_nrrd_path_nnunet/$series_id/*.log\n",
        "  !rm $processed_nrrd_path_nnunet/$series_id/*_pred_segthor*.nrrd\n",
        "\n",
        "  # delete the series folder in /content/data/processed/nlst/nnunet/nrrd/series_id\n",
        "  nrrd_dir = os.path.join(processed_base_path,'nnunet','nrrd',series_id)\n",
        "  !rmdir $nrrd_dir\n",
        "\n",
        "  return "
      ],
      "metadata": {
        "id": "FRD7o384rmyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_files_in_bpr_path(series_id, \n",
        "                            model_input_folder_bpr, \n",
        "                            model_input_folder_tmp_bpr, \n",
        "                            model_output_folder_tmp_bpr, \n",
        "                            model_output_folder_sr_regions_bpr, \n",
        "                            model_output_folder_sr_landmarks_bpr,\n",
        "                            input_nifti_path, \n",
        "                            sorted_base_path, \n",
        "                            processed_nifti_path):\n",
        "  \n",
        "  \"\"\"Cleans up the BPR files for the next run\n",
        "\n",
        "  Inputs: \n",
        "    series_id                            : series_id being processed \n",
        "    model_input_folder_bpr               : input used for BPR \n",
        "    model_input_folder_tmp_bpr           : temp folder for input bpr results \n",
        "    model_output_folder_tmp_bpr          : temp folder for output bpr results \n",
        "    model_output_folder_sr_regions_bpr   : folder for SR dcm files (regions)\n",
        "    model_output_folder_sr_landmarks_bpr : folder for SR dcm files (landmarks)\n",
        "    input_nifti_path                     : the nifti file BPR was run on \n",
        "    sorted_base_path                     : where the raw DICOM files are stored \n",
        "    processed_nifti_path                 : where the processed nifti files are stored \n",
        "\n",
        "  Outputs:\n",
        "    cleans the files in the respective directories \n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  # remove the file from the model_input_tmp folder\n",
        "  model_input_folder_tmp_series = os.path.join(model_input_folder_tmp_bpr,series_id+'.nii.gz')\n",
        "  !rm $model_input_folder_tmp_series\n",
        "\n",
        "  # remove the file from the model_output_tmp folder \n",
        "  model_output_folder_tmp_series = os.path.join(model_output_folder_tmp_bpr,series_id+'.json')\n",
        "  !rm $model_output_folder_tmp_series\n",
        "\n",
        "  # remove the NIfTI file the prediction was computed from\n",
        "  !rm $input_nifti_path\n",
        "\n",
        "  # Remove the raw patient data \n",
        "  sorted_base_path_remove = os.path.join(sorted_base_path,series_id)\n",
        "  !rm -r $sorted_base_path_remove \n",
        "\n",
        "  # Delete json from bpr_output folder \n",
        "  model_output_folder_series = os.path.join(model_output_folder_bpr,series_id+'.json')\n",
        "  !rm $model_output_folder_series\n",
        "\n",
        "  # Delete the files in /content/data/processed/bpr/nii\n",
        "  # !rm -r /content/data/processed/bpr/nii\n",
        "\n",
        "  # remove from model input \n",
        "  # !rm /content/data/nnunet/model_input/*\n",
        "\n",
        "  # remove from processed path\n",
        "  !rm -r /content/data/processed/nlst/nii/*\n",
        "\n",
        "  # remove SR regions folder contents\n",
        "  # !rm -r $model_output_folder_sr_regions_bpr\n",
        "  !rm -r $model_output_folder_sr_regions_bpr/*\n",
        "\n",
        "  # remove SR landmarks folder contents\n",
        "  # !rm -r $model_output_folder_sr_landmarks_bpr\n",
        "  !rm -r $model_output_folder_sr_landmarks_bpr/*\n",
        "\n",
        "\n",
        "  return \n"
      ],
      "metadata": {
        "id": "idZoD9AquJHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPR regions SR creation"
      ],
      "metadata": {
        "id": "Y3rL4gy_kEZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes as input a json file and slice_index, returns the regions assigned to the slice \n",
        "def convert_slice_to_region(bpr_data, slice_index):\n",
        "\n",
        "  \"\"\" \n",
        "  Given the slice_index, this returns a list of corresponding regions that match\n",
        "\n",
        "  Inputs: \n",
        "    bpr_data    : a dictionary, where for each of the six regions, a list of \n",
        "                  slice indices are provided \n",
        "    slice_index : slice number you want to obtain the list of classified regions \n",
        "                  for\n",
        "  Returns\n",
        "    regions     : list of regions for that slice_index\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # find where the slice_index appears across all regions\n",
        "  # get the names of the regions\n",
        "\n",
        "  num_regions = len(bpr_data)\n",
        "  region_names = list(bpr_data.keys())\n",
        "  regions = [] \n",
        "\n",
        "  for n in range(0,num_regions):\n",
        "    vals = bpr_data[region_names[n]]\n",
        "    if slice_index in vals:\n",
        "      regions.append(region_names[n])\n",
        "\n",
        "  return regions \n"
      ],
      "metadata": {
        "id": "MCRIvgCf2g2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert a list of regions (single slice) to a qualitative_evaluations \n",
        "# def convert_regions_list_to_qualitative_evaluations(regions):\n",
        "\n",
        "#   \"\"\" \n",
        "#   Forms the qualitative_evalautions array needed for the creation of the \n",
        "#   Structured Report. This is for a particular set of regions. \n",
        "\n",
        "#   Inputs: \n",
        "#     regions                    : list of regions that a particular slice\n",
        "#                                  includes \n",
        "#   Returns\n",
        "#     qualitative_evaluations    : array of the qualitative_evaluations. \n",
        "\n",
        "#   \"\"\"\n",
        "\n",
        "#   qualitative_evaluations = [] \n",
        "#   num_regions = len(regions)\n",
        "\n",
        "#   for region in regions: \n",
        "    \n",
        "#     if (region==\"legs\"):\n",
        "#       qualitative_evaluations.append(QualitativeEvaluation(CodedConcept(\n",
        "#         value=\"123014\",\n",
        "#         meaning=\"Target Region\",\n",
        "#         scheme_designator=\"DCM\"\n",
        "#       ), \n",
        "#       CodedConcept(\n",
        "#           value=\"30021000\",\n",
        "#           meaning=\"Legs\",\n",
        "#           scheme_designator=\"SCT\"\n",
        "#       )\n",
        "#       #, RelationshipTypeValues.CONTAINS\n",
        "#       ))\n",
        "\n",
        "#     elif (region==\"pelvis\"):\n",
        "#       qualitative_evaluations.append(QualitativeEvaluation(CodedConcept(\n",
        "#         value=\"123014\",\n",
        "#         meaning=\"Target Region\",\n",
        "#         scheme_designator=\"DCM\"\n",
        "#       ), \n",
        "#       CodedConcept(\n",
        "#           value=\"12921003\",\n",
        "#           meaning=\"Pelvis\",\n",
        "#           scheme_designator=\"SCT\"\n",
        "#       )\n",
        "#       #, RelationshipTypeValues.CONTAINS\n",
        "#       ))\n",
        "\n",
        "#     elif (region==\"abdomen\"):\n",
        "#       qualitative_evaluations.append(QualitativeEvaluation(CodedConcept(\n",
        "#         value=\"123014\",\n",
        "#         meaning=\"Target Region\",\n",
        "#         scheme_designator=\"DCM\"\n",
        "#       ), \n",
        "#       CodedConcept(\n",
        "#           value=\"113345001\",\n",
        "#           meaning=\"Abdomen\",\n",
        "#           scheme_designator=\"SCT\"\n",
        "#       )\n",
        "#       #, RelationshipTypeValues.CONTAINS\n",
        "#       ))\n",
        "\n",
        "#     elif (region==\"chest\"):\n",
        "#       qualitative_evaluations.append(QualitativeEvaluation(CodedConcept(\n",
        "#         value=\"123014\",\n",
        "#         meaning=\"Target Region\",\n",
        "#         scheme_designator=\"DCM\"\n",
        "#       ), \n",
        "#       CodedConcept(\n",
        "#           value=\"51185008\",\n",
        "#           meaning=\"Chest\",\n",
        "#           scheme_designator=\"SCT\"\n",
        "#       )\n",
        "#       #, RelationshipTypeValues.CONTAINS\n",
        "#       ))\n",
        "\n",
        "#     elif (region==\"shoulder-neck\"):\n",
        "#       qualitative_evaluations.append(QualitativeEvaluation(CodedConcept(\n",
        "#         value=\"123014\",\n",
        "#         meaning=\"Target Region\",\n",
        "#         scheme_designator=\"DCM\"\n",
        "#       ), \n",
        "#       CodedConcept(\n",
        "#           value=\"45048000\",\n",
        "#           meaning=\"Neck\",\n",
        "#           scheme_designator=\"SCT\"\n",
        "#       )\n",
        "#       #, RelationshipTypeValues.CONTAINS\n",
        "#       ))\n",
        "\n",
        "#     elif (region==\"head\"):\n",
        "#       qualitative_evaluations.append(QualitativeEvaluation(CodedConcept(\n",
        "#         value=\"123014\",\n",
        "#         meaning=\"Target Region\",\n",
        "#         scheme_designator=\"DCM\"\n",
        "#         ), \n",
        "#         CodedConcept(\n",
        "#             value=\"69536005\",\n",
        "#             meaning=\"Head\",\n",
        "#             scheme_designator=\"SCT\"\n",
        "#         )\n",
        "#         #, RelationshipTypeValues.CONTAINS\n",
        "#         ))\n",
        "\n",
        "\n",
        "\n",
        "#   return qualitative_evaluations "
      ],
      "metadata": {
        "id": "clUG-oL04OfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_for_body_part_regression_regions(files, \n",
        "                                                              json_file, \n",
        "                                                              output_SR_file, \n",
        "                                                              bpr_revision_number,\n",
        "                                                              bpr_regions_df):\n",
        "\n",
        "  \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "     and writes a structured report (SR) to disk\n",
        "     \n",
        "  Inputs: \n",
        "    files               : list of CT dicom files \n",
        "    json_file           : the json file created from the BodyPartRegression prediction\n",
        "    output_SR_file      : output filename for the structured report \n",
        "    bpr_revision_number : specific revision number of the bpr repo \n",
        "    bpr_regions_df      : holds the metadata needed for the bpr target regions \n",
        "\n",
        "  Outputs:\n",
        "    writes the SR out to the output_SR_file.    \n",
        "     \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "  num_files = len(files)\n",
        "  # print (\"num_files: \" + str(num_files))\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "    # print(ds)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # print(ds['0x0020','0x0037'].value)\n",
        "    ImageOrientation = ds['0x0020','0x0037'].value\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    sop = ds['0x0008', '0x0018'].value\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  #---- Open the json file and parse the list of regions per slice -----# \n",
        "\n",
        "  f = open(json_file)\n",
        "  json_data = json.load(f)\n",
        "  bpr_data = json_data['body part examined']\n",
        "\n",
        "  # return a list where each entry is per slice and is a array of possible regions \n",
        "  bpr_slice_scores = json_data['cleaned slice scores']\n",
        "  num_slices = len(bpr_slice_scores)\n",
        "\n",
        "  num_regions = len(bpr_data)\n",
        "  regions = [] \n",
        "\n",
        "  # print('num_slices: ' + str(num_slices))\n",
        "\n",
        "  for slice_index in range(0,num_slices):\n",
        "    region = convert_slice_to_region(bpr_data, slice_index)\n",
        "    regions.append(region)\n",
        "\n",
        "  # ----- Create the structured report ----- # \n",
        "\n",
        "  # Create the report content\n",
        "\n",
        "  procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "                                meaning=\"Imaging procedure\")\n",
        "\n",
        "  # Describe the context of reported observations: the person that reported\n",
        "  # the observations and the device that was used to make the observations\n",
        "  observer_person_context = ObserverContext(\n",
        "      observer_type=codes.DCM.Person,\n",
        "      observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "          name='Anonymous^Reader'\n",
        "      )\n",
        "  )\n",
        "  # observer_device_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Device,\n",
        "  #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "  #         uid=generate_uid(), name=\"BodyPartRegression\"\n",
        "  #     )\n",
        "  observer_device_context = ObserverContext(\n",
        "    observer_type=codes.DCM.Device,\n",
        "    observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "        uid=generate_uid(), name=\"BodyPartRegression\", \n",
        "        model_name = bpr_revision_number\n",
        "    )\n",
        "  )\n",
        "  observation_context = ObservationContext(\n",
        "      #observer_person_context=observer_person_context,\n",
        "      observer_device_context=observer_device_context,\n",
        "  )\n",
        "\n",
        "  imaging_measurements = []\n",
        "  evidence = []\n",
        "\n",
        "  tracking_uid = generate_uid()\n",
        "\n",
        "  qualitative_evaluations = []\n",
        "\n",
        "  print('num_slices: ' + str(num_slices))\n",
        "\n",
        "\n",
        "  #----------- Per slice ---------#\n",
        "\n",
        "  for n in range(0,num_slices):\n",
        "\n",
        "    slice_region = regions[n]\n",
        "\n",
        "    # qualitative_evaluations = convert_regions_list_to_qualitative_evaluations(slice_region)\n",
        "\n",
        "    # ----- per region ---- # \n",
        "    qualitative_evaluations = [] \n",
        "    # num_regions = len(regions)\n",
        "    num_regions = len(slice_region)\n",
        "\n",
        "    # for region in regions: \n",
        "    for region in slice_region: \n",
        "      row = bpr_regions_df.loc[bpr_regions_df['BPR_code_region'] == region]\n",
        "      qualitative_evaluations.append(\n",
        "          QualitativeEvaluation(\n",
        "              CodedConcept(\n",
        "                            value=str(row[\"target_CodeValue\"].values[0]),\n",
        "                            meaning=str(row[\"target_CodeMeaning\"].values[0]).replace(u'\\xa0', u' '),\n",
        "                            # meaning = \"Target Region\",\n",
        "                            scheme_designator=str(row[\"target_CodingSchemeDesignator\"].values[0])\n",
        "                            ), \n",
        "              CodedConcept(\n",
        "                            value=str(row[\"CodeValue\"].values[0]),\n",
        "                            meaning=str(row[\"CodeMeaning\"].values[0]),\n",
        "                            scheme_designator=str(row[\"CodingSchemeDesignator\"].values[0])\n",
        "                          )\n",
        "              )\n",
        "          )\n",
        "\n",
        "    # In the correct order \n",
        "    reference_dcm_file = files_sorted[n]\n",
        "    image_dataset = dcmread(reference_dcm_file)\n",
        "    evidence.append(image_dataset)\n",
        "\n",
        "    src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "    # tracking_id = \"Annotations group x\"\n",
        "    tracking_id = \"Annotations group \" + str(n+1) # start indexing with 1\n",
        "\n",
        "    measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "                  tracking_identifier=TrackingIdentifier(\n",
        "                      uid=tracking_uid,\n",
        "                      identifier=tracking_id\n",
        "                  ),\n",
        "                  qualitative_evaluations=qualitative_evaluations,\n",
        "                  source_images=[src_image]\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    imaging_measurements.append(\n",
        "      measurements_group\n",
        "            )\n",
        "    \n",
        "  #-------------------#\n",
        "    \n",
        "  measurement_report = MeasurementReport(\n",
        "      observation_context=observation_context,\n",
        "      procedure_reported=procedure_code,\n",
        "      imaging_measurements=imaging_measurements\n",
        "  )\n",
        "\n",
        "  # Create the Structured Report instance\n",
        "  series_instance_uid = generate_uid()\n",
        "  sr_dataset = Comprehensive3DSR(\n",
        "      evidence=evidence,\n",
        "      content=measurement_report[0],\n",
        "      series_number=100,\n",
        "      series_instance_uid=series_instance_uid,\n",
        "      sop_instance_uid=generate_uid(),\n",
        "      instance_number=1,\n",
        "      manufacturer='IDC',\n",
        "      is_complete = True,\n",
        "      is_final=True,\n",
        "      series_description='BPR region annotations'\n",
        "  )\n",
        "  # series_description='BPR region annotations'\n",
        "\n",
        "  pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "  return sr_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "MbJ9tLsCJRh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPR landmarks SR creation"
      ],
      "metadata": {
        "id": "N97Mrr4PdYOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_indices_from_json(filename, tag_start, tag_end):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the indices of the particular anatomy specified by the tag_start and \n",
        "  tag_end for a particular patient. \n",
        "\n",
        "  Arguments:\n",
        "    filename  : required - the patient json filename \n",
        "    tag_start : required - the string for the start of the anatomical region \n",
        "    tag_end   : required - the string for the end of the anatomical region \n",
        "\n",
        "  Outputs:\n",
        "    min_index : the minimum index in the patient coordinate system\n",
        "    max_index : the maximum index in the patient coordinate system \n",
        "  \"\"\"\n",
        "\n",
        "  # These scores are the same for all patients  \n",
        "  x = load_json(filename)\n",
        "  start_score = x[\"look-up table\"][tag_start][\"mean\"]\n",
        "  end_score = x[\"look-up table\"][tag_end][\"mean\"]\n",
        "\n",
        "  # The actual indices \n",
        "  min_index, max_index = crop_scores(x[\"cleaned slice scores\"], start_score, end_score)\n",
        "\n",
        "  return min_index, max_index "
      ],
      "metadata": {
        "id": "Y8PW3Qtkmxgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_landmark_indices_from_json(filename):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the indices of each landmark in the patient json filename and converts \n",
        "  it to slice indices. \n",
        "\n",
        "  Arguments:\n",
        "    filename  : required - the patient json filename \n",
        "\n",
        "  Outputs:\n",
        "    list of the corresponding landmark slices in the space of the patient. \n",
        "    Landmarks at the extreme ends - most inferior and most superior axial slices\n",
        "    are removed. \n",
        "  \"\"\"\n",
        "\n",
        "  # Get the cleaned slice scores to determine the number of slices \n",
        "  x = load_json(filename)\n",
        "  num_slices = len(x[\"cleaned slice scores\"])\n",
        "\n",
        "  # Get the list of landmarks \n",
        "  landmarks = list(x[\"look-up table\"].keys())\n",
        "  num_landmarks = len(landmarks)\n",
        "\n",
        "  # Get the expected z_spacing - if less than 0, slices are in reverse order \n",
        "  valid_z_spacing = x[\"valid z-spacing\"]\n",
        "\n",
        "  # Get values for all tags \n",
        "  # Reorder the landmarks according to the mean values in ascending order \n",
        "  landmarks_dict_sorted = {}\n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    landmarks_dict_sorted[landmark] = x[\"look-up table\"][landmark]['mean']\n",
        "  landmark_dict_sorted = dict(sorted(landmarks_dict_sorted.items(), key=lambda item: item[1]))\n",
        "  landmarks = list(landmark_dict_sorted.keys())\n",
        "\n",
        "  # Calculate the actual slice indices of each landmark \n",
        "  landmark_indices = {}\n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    score = landmark_dict_sorted[landmark] # x[\"look-up table\"][landmark][\"mean\"]\n",
        "    min_index, max_index = crop_scores(x[\"cleaned slice scores\"], score, score)\n",
        "    if (valid_z_spacing > 0): \n",
        "      landmark_indices[landmark] = min_index \n",
        "    else: \n",
        "      landmark_indices[landmark] = num_slices - min_index \n",
        "\n",
        "  # Programmatically remove values from dictionary if it is at most inferior\n",
        "  # or most superior slice \n",
        "  for n in range(0,num_landmarks):\n",
        "    landmark = landmarks[n]\n",
        "    if (landmark_indices[landmark]==0):\n",
        "      del landmark_indices[landmark]\n",
        "    elif (landmark_indices[landmark]==num_slices-1):\n",
        "      del landmark_indices[landmark]\n",
        "\n",
        "  return landmark_indices \n"
      ],
      "metadata": {
        "id": "dZhMmzebrVvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_landmark_indices_list_in_slice(landmark_indices, slice_number):\n",
        "\n",
        "  \"\"\"\n",
        "  Gets the list of landmarks that are assigned to a particular slice number.\n",
        "\n",
        "  Arguments:\n",
        "    flandmark_indices  : the dictionary holding the slice number for each \n",
        "                         landmark\n",
        "    slice_number       : the particular slice to obtain the list of landmarks\n",
        "                         for\n",
        "\n",
        "  Outputs:\n",
        "    list of the landmarks that correspond to the slice_number\n",
        "  \"\"\"\n",
        "\n",
        "  key_list = list()\n",
        "  items_list = landmark_indices.items()\n",
        "  for item in items_list:\n",
        "    if item[1] == slice_number:\n",
        "        key_list.append(item[0])\n",
        "\n",
        "  return key_list\n",
        "\n",
        "def convert_landmarks_list_to_qualitative_evaluations(landmark_list,\n",
        "                                                      landmarks_df):\n",
        "\n",
        "  \"\"\"\n",
        "  Converts the list of landmarks to a qualitative_evaluations for the \n",
        "  structured report. \n",
        "\n",
        "  Arguments:\n",
        "    landmark_list  : list of landmarks that correspond to a particular slice\n",
        "    landmarks_df   : the dataframe holding the bpr landmarks metadata needed to\n",
        "                     create the structured report\n",
        "\n",
        "  Outputs:\n",
        "    list of QualitativeEvaluation\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  qualitative_evaluations = [] \n",
        "  num_landmarks_in_slice = len(landmark_list)\n",
        "  for landmark in landmark_list: \n",
        "    if not landmark in landmarks_df[\"BPR_code\"].values:\n",
        "      print(\"ERROR: Failed to map BPR landmark \"+landmark)\n",
        "      break\n",
        "    else:\n",
        "      landmark_row = landmarks_df[landmarks_df[\"BPR_code\"] == landmark]\n",
        "      # landmark_code = CodedConcept(value = str(int(landmark_row[\"CodeValue\"].values[0])),\n",
        "      #                              meaning = landmark_row[\"CodeMeaning\"].values[0],\n",
        "      #                              scheme_designator = landmark_row[\"CodingSchemeDesignator\"].values[0])\n",
        "      landmark_code = CodedConcept(value = str(landmark_row[\"CodeValue\"].values[0].astype(np.int64)),\n",
        "                                meaning = str(landmark_row[\"CodeMeaning\"].values[0]),\n",
        "                                scheme_designator = str(landmark_row[\"CodingSchemeDesignator\"].values[0]))\n",
        "      #print(landmarks_df[\"CodingSchemeDesignator\"].values[0])\n",
        "      landmark_modifier_code = None\n",
        "      if not pd.isna(landmarks_df[\"modifier_CodeValue\"].values[0]):\n",
        "        # landmark_modifier_code = CodedConcept(value = str(int(landmark_row[\"modifier_CodeValue\"].values[0])),\n",
        "        #                              meaning = landmark_row[\"modifier_CodeMeaning\"].values[0],\n",
        "        #                              scheme_designator = landmark_row[\"modifier_CodingSchemeDesignator\"].values[0])\n",
        "        landmark_modifier_code = CodedConcept(value = str(landmark_row[\"modifier_CodeValue\"].values[0].astype(np.int64)),\n",
        "                                meaning = str(landmark_row[\"modifier_CodeMeaning\"].values[0]),\n",
        "                                scheme_designator = str(landmark_row[\"modifier_CodingSchemeDesignator\"].values[0]))\n",
        "        #print(landmarks_df[\"modifier_CodingSchemeDesignator\"].values[0])\n",
        "\n",
        "    qual = QualitativeEvaluation(CodedConcept(\n",
        "                value=\"123014\",\n",
        "                meaning=\"Target Region\",\n",
        "                scheme_designator=\"DCM\"  \n",
        "                ), \n",
        "                landmark_code\n",
        "              )\n",
        "    \n",
        "    if landmark_modifier_code is not None:\n",
        "      qual_modifier = CodeContentItem(\n",
        "          name=CodedConcept(\n",
        "              value='106233006',\n",
        "              meaning='Topographical modifier',\n",
        "              scheme_designator='SCT',\n",
        "          ),\n",
        "          value=landmark_modifier_code,\n",
        "          relationship_type=RelationshipTypeValues.HAS_CONCEPT_MOD\n",
        "      )\n",
        "      qual.append(qual_modifier)\n",
        "\n",
        "    qualitative_evaluations.append(qual)\n",
        "\n",
        "  return qualitative_evaluations \n"
      ],
      "metadata": {
        "id": "fgzvmhRSTwkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_for_body_part_regression_landmarks(files, \n",
        "                                                                json_file, \n",
        "                                                                output_SR_file, \n",
        "                                                                bpr_revision_number,\n",
        "                                                                landmarks_df):\n",
        "\n",
        "  \"\"\"Takes as input a set of DICOM files and the corresponding body part regression json file, \n",
        "     and writes a structured report (SR) to disk\n",
        "     \n",
        "  Inputs: \n",
        "    files               : list of CT dicom files \n",
        "    json_file           : the json file created from the BodyPartRegression prediction\n",
        "    output_SR_file      : output filename for the structured report \n",
        "    bpr_revision_number : specific revision number of the bpr repo \n",
        "    landmarks_df        : the dataframe holding the bpr landmarks metadata needed to\n",
        "                          create the structured report\n",
        "\n",
        "  Outputs:\n",
        "    writes the SR out to the output_SR_file.    \n",
        "     \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # ------ order the CT files according to the ImagePositionPatient and ImageOrientation ----# \n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "    # print(ds)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "    #ImageOrientation = ds.ImageOrientationPatient.value\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    sop = ds['0x0008', '0x0018'].value\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  #----- Get the landmarks as indices -----# \n",
        "\n",
        "  landmark_indices = get_landmark_indices_from_json(json_file)\n",
        "\n",
        "  # ----- Create the structured report ----- # \n",
        "\n",
        "  # Create the report content\n",
        "\n",
        "  procedure_code = CodedConcept(value=\"363679005\", scheme_designator=\"SCT\", \n",
        "                                meaning=\"Imaging procedure\")\n",
        "\n",
        "  # Describe the context of reported observations: the person that reported\n",
        "  # the observations and the device that was used to make the observations\n",
        "  # observer_person_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Person,\n",
        "  #     observer_identifying_attributes=PersonObserverIdentifyingAttributes(\n",
        "  #         name='Anonymous^Reader'\n",
        "  #     )\n",
        "  # )\n",
        "\n",
        "  # observer_device_context = ObserverContext(\n",
        "  #     observer_type=codes.DCM.Device,\n",
        "  #     observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "  #         uid=generate_uid(), name=\"BodyPartRegression_landmarks\"\n",
        "  #     )\n",
        "  # )\n",
        "\n",
        "  observer_device_context = ObserverContext(\n",
        "      observer_type=codes.DCM.Device,\n",
        "      observer_identifying_attributes=DeviceObserverIdentifyingAttributes(\n",
        "          uid=generate_uid(), name=\"BodyPartRegression_landmarks\", \n",
        "          model_name = bpr_revision_number\n",
        "      )\n",
        "  )\n",
        "  # inputMetadata[\"observerContext\"] = {\n",
        "  #                   \"ObserverType\": \"DEVICE\",\n",
        "  #                   \"DeviceObserverName\": \"pyradiomics\",\n",
        "  #                   \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "  #                 }\n",
        "\n",
        "  observation_context = ObservationContext(\n",
        "      #observer_person_context=observer_person_context,\n",
        "      observer_device_context=observer_device_context,\n",
        "  )\n",
        "\n",
        "  imaging_measurements = []\n",
        "  evidence = []\n",
        "\n",
        "  qualitative_evaluations = []\n",
        "\n",
        "  tracking_uid = generate_uid()\n",
        "\n",
        "  #------------- Per slice - only include landmarks that exist ------# \n",
        "\n",
        "  num_slices = len(files_sorted)\n",
        "  annotation_count = 1 \n",
        "\n",
        "  for n in range(0,num_slices):\n",
        "\n",
        "    # find all the dictionary entries that match this slice number - returns a list of landmarks \n",
        "    landmark_indices_list = get_landmark_indices_list_in_slice(landmark_indices, n) # n = slice number \n",
        "\n",
        "    # Only include if there is a landmark for a slice \n",
        "    if (landmark_indices_list):\n",
        "    \n",
        "      # Create QualitativeEvaluations\n",
        "      qualitative_evaluations = convert_landmarks_list_to_qualitative_evaluations(landmark_indices_list,\n",
        "                                                                                  landmarks_df)\n",
        "\n",
        "      # In the correct order \n",
        "      reference_dcm_file = files_sorted[n]\n",
        "      image_dataset = dcmread(reference_dcm_file)\n",
        "      evidence.append(image_dataset)\n",
        "\n",
        "      src_image = hd.sr.content.SourceImageForMeasurementGroup.from_source_image(image_dataset)\n",
        "\n",
        "      # tracking_id = \"Annotations group x\"\n",
        "      # tracking_id = \"Annotations group landmarks\"\n",
        "      # tracking_id = \"Annotations group landmarks \" + str(n+1) # start indexing with 1\n",
        "      tracking_id = \"Annotations group landmarks \" + str(annotation_count) # start indexing with 1\n",
        "\n",
        "      measurements_group = MeasurementsAndQualitativeEvaluations(\n",
        "                    tracking_identifier=TrackingIdentifier(\n",
        "                        uid=tracking_uid,\n",
        "                        identifier=tracking_id\n",
        "                    ),\n",
        "                    qualitative_evaluations=qualitative_evaluations,\n",
        "                    source_images=[src_image]\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      imaging_measurements.append(\n",
        "        measurements_group\n",
        "              )\n",
        "      \n",
        "      annotation_count += 1 # keep track of number of annotations\n",
        "    \n",
        "\n",
        "\n",
        "  #-------------------#\n",
        "    \n",
        "  measurement_report = MeasurementReport(\n",
        "      observation_context=observation_context,\n",
        "      procedure_reported=procedure_code,\n",
        "      imaging_measurements=imaging_measurements\n",
        "  )\n",
        "\n",
        "  # Create the Structured Report instance\n",
        "  series_instance_uid = generate_uid()\n",
        "  sr_dataset = Comprehensive3DSR(\n",
        "      evidence=evidence,\n",
        "      content=measurement_report[0],\n",
        "      series_number=101, # was 100 for regions\n",
        "      series_instance_uid=series_instance_uid,\n",
        "      sop_instance_uid=generate_uid(),\n",
        "      instance_number=1,\n",
        "      manufacturer='IDC',\n",
        "      is_complete = True,\n",
        "      is_final=True,\n",
        "      series_description='BPR landmark annotations'\n",
        "  )\n",
        "  # series_description='BPR landmark annotations'\n",
        "\n",
        "  pydicom.write_file(output_SR_file, sr_dataset)\n",
        "\n",
        "  return sr_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "4JY_g0rCQ1nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nnUNet 3D shape features SR creation"
      ],
      "metadata": {
        "id": "dO1EPJ6rdcV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_and_names_from_metadata_json(dicomseg_json):\n",
        "\n",
        "  \"\"\"Returns two lists containing the label values and the corresponding\n",
        "     CodeMeaning values\n",
        "\n",
        "  Inputs: \n",
        "    dicomseg_json : metajson file\n",
        "\n",
        "  Outputs:\n",
        "    label_values  : label values from the metajson file \n",
        "    label_names   : the corresponding CodeMeaning values \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  f = open(dicomseg_json)\n",
        "  meta_json = json.load(f)\n",
        "\n",
        "  print(meta_json)\n",
        "\n",
        "  num_regions = len(meta_json['segmentAttributes'][0])\n",
        "  print ('num_regions: ' + str(num_regions))\n",
        "\n",
        "  label_values = []\n",
        "  label_names = [] \n",
        "  for n in range(0,num_regions):\n",
        "    # label_values.append(n)\n",
        "    label_value = meta_json['segmentAttributes'][0][n]['labelID']\n",
        "    label_name = meta_json['segmentAttributes'][0][n]['SegmentedPropertyTypeCodeSequence']['CodeMeaning']\n",
        "    label_values.append(label_value)\n",
        "    label_names.append(label_name)\n",
        "\n",
        "  return label_values, label_names "
      ],
      "metadata": {
        "id": "UpPtxw3hbfwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_nii(input_file, output_directory, label_names):\n",
        "\n",
        "  \"\"\"Function to split a single multilabel nii into individual nii files. Used\n",
        "     for pyradiomics feature extraction. \n",
        "\n",
        "  Inputs: \n",
        "    input_file       : input multi-label nii file \n",
        "    output_directory : where to save the individual nii segments \n",
        "    label_names      : the names of the labels that correspond to the order of \n",
        "                       the values in the nii input_file \n",
        "\n",
        "  Outputs:\n",
        "    saves the individual nii files to the output_directory \n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  if not os.path.isdir(output_directory):\n",
        "    os.mkdir(output_directory)\n",
        "\n",
        "  # save with the values in the files \n",
        "  nii = nib.load(input_file)\n",
        "  header = nii.header \n",
        "  img = nii.get_fdata() \n",
        "  unique_labels = list(np.unique(img))\n",
        "  unique_labels.remove(0) # remove the background \n",
        "\n",
        "  # split and save \n",
        "  num_labels = len(unique_labels)\n",
        "  for n in range(0,num_labels):\n",
        "    ind = np.where(img==unique_labels[n])\n",
        "    vol = np.zeros((img.shape))\n",
        "    vol[ind] = 1\n",
        "    new_img = nib.Nifti1Image(vol, nii.affine, nii.header)\n",
        "    output_filename = os.path.join(output_directory, label_names[n] + '.nii.gz')\n",
        "    nib.save(new_img, output_filename)\n",
        "\n",
        "  return "
      ],
      "metadata": {
        "id": "-cWhVN_zbjBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pyradiomics_3D_features(ct_nifti_path, \n",
        "                                    label_values, \n",
        "                                    label_names, \n",
        "                                    split_pred_nifti_path, \n",
        "                                    nnunet_shape_features_code_mapping_df):\n",
        "\n",
        "  \"\"\"Function to compute pyradiomics 3D features for each label in a nifti file. \n",
        "     A single csv file is written out to disk. \n",
        "\n",
        "  Inputs: \n",
        "    ct_nifti_path            : the CT nifti file \n",
        "    label_values             : the label value for each of the segments from the json file \n",
        "    label_names              : the corresponding label name for each of the segments \n",
        "    split_pred_nifti_path    : where to save the individual nii segments needed \n",
        "                               for pyradiomics\n",
        "    nnunet_shape_features_code_mapping_df : the df where we will obtain the \n",
        "                                            list of the shape features to \n",
        "                                            compute\n",
        "\n",
        "  Outputs:\n",
        "    Writes the features_csv_path_nnunet to disk. \n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  # Get the names of the features from the nnunet_shape_features_code_mapping_df\n",
        "  shape_features = list(nnunet_shape_features_code_mapping_df['shape_feature'].values)\n",
        "\n",
        "  # Instantiate the extractor and modify the settings to keep the 3D shape features\n",
        "  extractor = featureextractor.RadiomicsFeatureExtractor()\n",
        "  extractor.settings['minimumROIDimensions'] = 3 \n",
        "  extractor.disableAllFeatures()\n",
        "  extractor.enableFeaturesByName(shape=shape_features) \n",
        "\n",
        "  # Calculate features for each label and create a dataframe\n",
        "  num_labels = len([f for f in os.listdir(split_pred_nifti_path) if f.endswith('.nii.gz')])\n",
        "  df_list = [] \n",
        "  for n in range(0,num_labels):\n",
        "    mask_path = os.path.join(split_pred_nifti_path, label_names[n] + '.nii.gz')\n",
        "    # Run the extractor \n",
        "    result = extractor.execute(ct_nifti_path, mask_path) # dictionary\n",
        "    # keep only the features we want\n",
        "    # Get the corresponding label number -- all might not be present \n",
        "    corresponding_label_value = label_values[label_names.index(label_names[n])] \n",
        "    dict_keep = {'ReferencedSegment': corresponding_label_value, \n",
        "                 'label_name': label_names[n]}\n",
        "    keys_keep = [f for f in result.keys() if 'original_shape' in f]\n",
        "    # Just keep the feature keys we want\n",
        "    dict_keep_new_values = {key_keep: result[key_keep] for key_keep in keys_keep}\n",
        "    dict_keep.update(dict_keep_new_values)\n",
        "    df1 = pd.DataFrame([dict_keep])\n",
        "    # change values of columns to remove original_shape_\n",
        "    df1.columns = df1.columns.str.replace('original_shape_', '')\n",
        "    # Append to the ReferencedSegment and label_name df \n",
        "    df_list.append(df1)\n",
        "\n",
        "  # concat all label features \n",
        "  df = pd.concat(df_list)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "UYDxAx7sbbPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def order_dicom_files_image_position(dcm_directory):\n",
        "  \"\"\"\n",
        "  Orders the dicom files according to image position and orientation. \n",
        "\n",
        "  Arguments:\n",
        "    dcm_directory : input directory of dcm files to put in order \n",
        "\n",
        "  Outputs:\n",
        "    files_sorted   : dcm files in sorted order \n",
        "    sop_all_sorted : the SOPInstanceUIDs in sorted order \n",
        "    pos_all_sorted : the image position in sorted order \n",
        "\n",
        "  \"\"\"\n",
        "  files = [os.path.join(dcm_directory,f) for f in os.listdir(dcm_directory)]\n",
        "\n",
        "  num_files = len(files)\n",
        "\n",
        "  pos_all = []  \n",
        "  sop_all = [] \n",
        "\n",
        "  for n in range(0,num_files):\n",
        "    # read dcm file \n",
        "    filename = files[n]\n",
        "    ds = dcmread(filename)\n",
        "\n",
        "    # get ImageOrientation (0020, 0037)\n",
        "    # ImageOrientation = ds['0x0020','0x0037'].value\n",
        "    ImageOrientation = ds.ImageOrientationPatient\n",
        "\n",
        "    # get ImagePositionPatient (0020, 0032) \n",
        "    # ImagePositionPatient = ds['0x0020','0x0032'].value\n",
        "    ImagePositionPatient = ds.ImagePositionPatient\n",
        "\n",
        "    # calculate z value\n",
        "    x_vector = ImageOrientation[0:3]\n",
        "    y_vector = ImageOrientation[3:]\n",
        "    z_vector = np.cross(x_vector,y_vector)\n",
        "\n",
        "    # multiple z_vector by ImagePositionPatient\n",
        "    pos = np.dot(z_vector,ImagePositionPatient)\n",
        "    pos_all.append(pos)\n",
        "\n",
        "    # get the SOPInstanceUID \n",
        "    # sop = ds['0x0008', '0x0018'].value\n",
        "    sop = ds.SOPInstanceUID\n",
        "    sop_all.append(sop)\n",
        "\n",
        "\n",
        "  #----- order the SOPInstanceUID/files by z value ----# \n",
        "\n",
        "  sorted_ind = np.argsort(pos_all)\n",
        "  pos_all_sorted = np.array(pos_all)[sorted_ind.astype(int)]\n",
        "  sop_all_sorted = np.array(sop_all)[sorted_ind.astype(int)]\n",
        "  files_sorted = np.array(files)[sorted_ind.astype(int)]\n",
        "\n",
        "  return files_sorted, sop_all_sorted, pos_all_sorted "
      ],
      "metadata": {
        "id": "citIHr-gboSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_structured_report_metajson_for_shape_features(SeriesInstanceUID, \n",
        "                                                         SOPInstanceUID_seg,\n",
        "                                                         seg_file, \n",
        "                                                         dcm_directory, \n",
        "                                                         segments_code_mapping_df,\n",
        "                                                         shape_features_code_mapping_df,\n",
        "                                                         df_features, \n",
        "                                                         ):\n",
        "  \n",
        "  \"\"\"Function that creates the metajson necessary for the creation of a\n",
        "  structured report from a pandas dataframe of label names and features for \n",
        "  each. \n",
        "\n",
        "  Inputs: \n",
        "    SeriesInstanceUID               : SeriesInstanceUID of the corresponding CT \n",
        "                                      file \n",
        "    SOPInstanceUID_seg              : SOPInstanceUID of the corresponding SEG file \n",
        "    seg_file                        : filename of SEG DCM file \n",
        "    dcm_directory                   : ct directory that will be sorted in \n",
        "                                      terms of axial ordering according to the \n",
        "                                      ImagePositionPatient and ImageOrientation \n",
        "                                      fields\n",
        "    segments_code_mapping_df        : dataframe that holds the names of the \n",
        "                                      segments and the associated code values etc.\n",
        "    shape_features_code_mapping_df  : dataframe that holds the names of the \n",
        "                                      features and the associated code values etc. \n",
        "    df_features                     : a pandas dataframe holding the segments and a \n",
        "                                      set of 3D shape features for each \n",
        "\n",
        "  Outputs:\n",
        "    Returns the metajson for the structured report that will then be used by\n",
        "    dcmqi tid1500writer to create a structured report \n",
        "  \"\"\" \n",
        "\n",
        "  # --- Get the version number for the pyradiomics package --- #\n",
        "\n",
        "  pyradiomics_version_number = str(radiomics.__version__)\n",
        "  \n",
        "  # --- Sort the dcm files first according to --- # \n",
        "  # --- ImagePositionPatient and ImageOrientation --- #\n",
        "\n",
        "  files_sorted, sop_all_sorted, pos_all_sorted = order_dicom_files_image_position(dcm_directory)\n",
        "  files_sorted = [os.path.basename(f) for f in files_sorted]\n",
        "\n",
        "  # --- Create the header for the json --- # \n",
        "  \n",
        "  inputMetadata = {}\n",
        "  inputMetadata[\"@schema\"]= \"https://raw.githubusercontent.com/qiicr/dcmqi/master/doc/schemas/sr-tid1500-schema.json#\"\n",
        "  inputMetadata[\"SeriesDescription\"] = \"Measurements\"\n",
        "  inputMetadata[\"SeriesNumber\"] = \"1001\"\n",
        "  inputMetadata[\"InstanceNumber\"] = \"1\"\n",
        "\n",
        "  inputMetadata[\"compositeContext\"] = [seg_file] # not full path\n",
        "\n",
        "  inputMetadata[\"imageLibrary\"] = files_sorted # not full path \n",
        "\n",
        "  # inputMetadata[\"observerContext\"] = {\n",
        "  #                                     \"ObserverType\": \"PERSON\",\n",
        "  #                                     \"PersonObserverName\": \"Reader1\"\n",
        "  #                                   }\n",
        "  # inputMetadata[\"observerContext\"] = {\n",
        "  #                     \"ObserverType\": \"DEVICE\",\n",
        "  #                     \"DeviceObserverName\": \"pyradiomics\",\n",
        "  #                     \"DeviceObserverModelName\": \"v3.0.1\"\n",
        "  #                   }\n",
        "  inputMetadata[\"observerContext\"] = {\n",
        "                      \"ObserverType\": \"DEVICE\",\n",
        "                      \"DeviceObserverName\": \"pyradiomics\",\n",
        "                      \"DeviceObserverModelName\": pyradiomics_version_number\n",
        "                    }\n",
        "\n",
        "  inputMetadata[\"VerificationFlag\"]  = \"UNVERIFIED\"\n",
        "  inputMetadata[\"CompletionFlag\"] =  \"COMPLETE\"\n",
        "  inputMetadata[\"activitySession\"] = \"1\"\n",
        "  inputMetadata[\"timePoint\"] = \"1\"\n",
        "\n",
        "  # ------------------------------------------------------------------------- # \n",
        "  # --- Create the measurement_dict for each segment - holds all features --- # \n",
        "\n",
        "  measurement = [] \n",
        "\n",
        "  # --- Now create the dict for all features and all segments --- #\n",
        "\n",
        "  # --- Loop over the number of segments --- #\n",
        "\n",
        "  # number of rows in the df_features \n",
        "  num_segments = df_features.shape[0]\n",
        "\n",
        "  # Array of dictionaries - one dictionary for each segment \n",
        "  measurement_across_segments_combined = [] \n",
        "\n",
        "  for segment_id in range(0,num_segments):\n",
        "\n",
        "    ReferencedSegment = df_features['ReferencedSegment'].values[segment_id]\n",
        "    FindingSite = df_features['label_name'].values[segment_id]\n",
        "\n",
        "    print('segment_id: ' + str(segment_id))\n",
        "    print('ReferencedSegment: ' + str(ReferencedSegment))\n",
        "    print('FindingSite: ' + str(FindingSite))\n",
        "\n",
        "    # --- Create the dict for the Measurements group --- # \n",
        "    TrackingIdentifier = \"Measurements group \" + str(ReferencedSegment)\n",
        "\n",
        "    segment_row = segments_code_mapping_df[segments_code_mapping_df[\"segment\"] == FindingSite]\n",
        "    # print(segment_row)\n",
        "        \n",
        "    my_dict = {\n",
        "      \"TrackingIdentifier\": str(TrackingIdentifier),\n",
        "      \"ReferencedSegment\": int(ReferencedSegment),\n",
        "      \"SourceSeriesForImageSegmentation\": str(SeriesInstanceUID),\n",
        "      \"segmentationSOPInstanceUID\": str(SOPInstanceUID_seg),\n",
        "      \"Finding\": {\n",
        "        \"CodeValue\": \"113343008\",\n",
        "        \"CodingSchemeDesignator\": \"SCT\",\n",
        "        \"CodeMeaning\": \"Organ\"\n",
        "      }, \n",
        "      \"FindingSite\": {\n",
        "        \"CodeValue\": str(segment_row[\"FindingSite_CodeValue\"].values[0]),\n",
        "        \"CodingSchemeDesignator\": str(segment_row[\"FindingSite_CodingSchemeDesignator\"].values[0]),\n",
        "        \"CodeMeaning\": str(segment_row[\"FindingSite_CodeMeaning\"].values[0])\n",
        "      }\n",
        "    }\n",
        "\n",
        "    measurement = []  \n",
        "    # number of features - number of columns in df_features - 2 (label_name and ReferencedSegment)\n",
        "    num_values = len(df_features.columns)-2 \n",
        "\n",
        "    feature_list = df_features.columns[2:] # remove first two \n",
        "\n",
        "\n",
        "    # For each measurement per region segment\n",
        "    for n in range(0,num_values): \n",
        "      measurement_dict = {}\n",
        "      row = df_features.loc[df_features['label_name'] == FindingSite]\n",
        "      feature_row = shape_features_code_mapping_df.loc[shape_features_code_mapping_df[\"shape_feature\"] == feature_list[n]]\n",
        "      value = str(np.round(row[feature_list[n]].values[0],3))\n",
        "      measurement_dict[\"value\"] = value\n",
        "      measurement_dict[\"quantity\"] = {}\n",
        "      measurement_dict[\"quantity\"][\"CodeValue\"] = str(feature_row[\"quantity_CodeValue\"].values[0])\n",
        "      measurement_dict[\"quantity\"][\"CodingSchemeDesignator\"] = str(feature_row[\"quantity_CodingSchemeDesignator\"].values[0])\n",
        "      measurement_dict[\"quantity\"][\"CodeMeaning\"] = str(feature_row[\"quantity_CodeMeaning\"].values[0])\n",
        "      measurement_dict[\"units\"] = {}\n",
        "      measurement_dict[\"units\"][\"CodeValue\"] = str(feature_row[\"units_CodeValue\"].values[0])\n",
        "      measurement_dict[\"units\"][\"CodingSchemeDesignator\"] = str(feature_row[\"units_CodingSchemeDesignator\"].values[0])\n",
        "      measurement_dict[\"units\"][\"CodeMeaning\"] = str(feature_row[\"units_CodeMeaning\"].values[0])\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"] = {}\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"][\"AlgorithmName\"] = \"pyradiomics\"\n",
        "      measurement_dict[\"measurementAlgorithmIdentification\"][\"AlgorithmVersion\"] = str(pyradiomics_version_number)\n",
        "      measurement.append(measurement_dict) \n",
        "\n",
        "    measurement_combined_dict = {}\n",
        "    measurement_combined_dict['measurementItems'] = measurement # measurement is an array of dictionaries \n",
        "\n",
        "    output_dict_one_segment = {**my_dict, **measurement_combined_dict}\n",
        "\n",
        "    # append to array for all segments \n",
        "\n",
        "    measurement_across_segments_combined.append(output_dict_one_segment)\n",
        "\n",
        "  # --- Add the measurement data --- # \n",
        "\n",
        "  inputMetadata[\"Measurements\"] = {}\n",
        "  inputMetadata[\"Measurements\"] = measurement_across_segments_combined\n",
        "\n",
        "  return inputMetadata"
      ],
      "metadata": {
        "id": "qTuhUJ1Wbt0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_structured_report_for_shape_features(SeriesInstanceUID, \n",
        "                                              SOPInstanceUID_seg, \n",
        "                                              pred_dicomseg_path, \n",
        "                                              dicomseg_json_path, \n",
        "                                              dcm_directory, \n",
        "                                              pred_nifti_path, \n",
        "                                              split_pred_nii_path, \n",
        "                                              ct_nifti_path, \n",
        "                                              segments_code_mapping_df,\n",
        "                                              shape_features_code_mapping_df,\n",
        "                                              sr_json_path,\n",
        "                                              sr_path\n",
        "                                              ):\n",
        "  \n",
        "  \"\"\" This function creates the SR necessary for the nnUNet shape features \n",
        "\n",
        "  Inputs: \n",
        "  SeriesInstanceUID               : SeriesInstanceUID of the corresponding CT \n",
        "                                    file \n",
        "  SOPInstanceUID_seg              : SOPInstanceUID of the corresponding SEG file \n",
        "  pred_dicomseg_path              : filename of DICOM SEG file \n",
        "  dicomseg_json_path              : json file for DICOM SEG file \n",
        "  dcm_directory                   : list of ct files that will be sorted in \n",
        "                                    terms of axial ordering according to the \n",
        "                                    ImagePositionPatient and ImageOrientation \n",
        "                                    fields\n",
        "  pred_nifti_path                 : predictions in nifti format \n",
        "  nnunet_base_path                : path to hold the split nifti files \n",
        "  ct_nifti_path                   : filename for CT nifti file\n",
        "  segments_code_mapping_df        : dataframe that holds the names of the \n",
        "                                    segments and the associated code values etc.\n",
        "  shape_features_code_mapping_df  : dataframe that holds the names of the \n",
        "                                    features and the associated code values etc. \n",
        "  sr_json_path                    : the path that the metajson for the SR for \n",
        "                                    the 3D shape features will be saved \n",
        "  sr_path                         : the path that the SR for the 3D shape \n",
        "                                    features will be saved \n",
        "\n",
        "  Outputs:\n",
        "    Returns the metajson for the structured report that will then be used by\n",
        "    dcmqi tid1500writer to create a structured report \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # --- get label values and names from metajson file --- #\n",
        "  label_values, label_names = get_label_and_names_from_metadata_json(dicomseg_json_path)\n",
        "\n",
        "  # --- split the multilabel nifti into individual files --- #\n",
        "  split_pred_nii_path = os.path.join(nnunet_base_path, \"split_nii\")\n",
        "  if not os.path.isdir(split_pred_nii_path): \n",
        "    os.mkdir(split_pred_nii_path)\n",
        "  split_nii(pred_nifti_path, split_pred_nii_path, label_names)\n",
        "\n",
        "  # --- compute features and save csv for each region --- #\n",
        "  if not os.path.isdir(features_csv_path_nnunet):\n",
        "    os.mkdir(features_csv_path_nnunet) \n",
        "  df_features = compute_pyradiomics_3D_features(ct_nifti_path, \n",
        "                                                label_values, \n",
        "                                                label_names,\n",
        "                                                split_pred_nii_path, \n",
        "                                                nnunet_shape_features_code_mapping_df)\n",
        "  print ('created df_features')\n",
        "  \n",
        "  # --- upload csv file to bucket --- #\n",
        "  # !$s5cmd_path --endpoint-url https://storage.googleapis.com cp $pred_features_csv_path $gs_uri_features_csv_file\n",
        "\n",
        "  # remove nii files after saving out pyradiomics results\n",
        "  !rm $split_pred_nii_path/*\n",
        "  # remove csv \n",
        "  # !rm $pred_features_csv_path\n",
        "\n",
        "  # --- Create the final metadata for the SR --- #\n",
        "  inputMetadata = create_structured_report_metajson_for_shape_features(SeriesInstanceUID, \n",
        "                                                                       SOPInstanceUID_seg,\n",
        "                                                                       pred_dicomseg_path, \n",
        "                                                                       dcm_directory, \n",
        "                                                                       nnunet_segments_code_mapping_df, \n",
        "                                                                       nnunet_shape_features_code_mapping_df,\n",
        "                                                                       df_features)\n",
        "\n",
        "  print ('created SR json for shape features')\n",
        "\n",
        "  # --- Write out json --- #\n",
        "\n",
        "  with open(sr_json_path, 'w') as f:\n",
        "    json.dump(inputMetadata, f, indent=2)\n",
        "  print ('wrote out json for shape features')\n",
        "\n",
        "  # --- Save the SR for nnUNet shape features --- # \n",
        "  # inputImageLibraryDirectory = os.path.join(\"/content\", \"raw\")\n",
        "  # outputDICOM = os.path.join(\"/content\",\"features_sr.dcm\")\n",
        "  # inputCompositeContextDirectory = os.path.join(\"/content\",\"seg\")\n",
        "  inputImageLibraryDirectory = dcm_directory\n",
        "  # outputDICOM = sr_json_path\n",
        "  outputDICOM = sr_path\n",
        "  # the name of the folder where the seg files are located \n",
        "  inputCompositeContextDirectory = os.path.basename(pred_dicomseg_path) # might need to check this\n",
        "  inputMetadata_json = sr_json_path \n",
        "\n",
        "  print ('inputImageLibraryDirectory: ' + str(inputImageLibraryDirectory))\n",
        "  print ('outputDICOM: ' + str(outputDICOM))\n",
        "  print ('inputCompositeContextDirectory: ' + str(inputCompositeContextDirectory))\n",
        "  print ('inputMetadata_json: ' + str(inputMetadata_json)) \n",
        "  !tid1500writer --inputImageLibraryDirectory $inputImageLibraryDirectory \\\n",
        "                --outputDICOM $outputDICOM  \\\n",
        "                --inputCompositeContextDirectory $inputCompositeContextDirectory \\\n",
        "                --inputMetadata $inputMetadata_json\n",
        "  print ('wrote out SR for shape features')\n",
        "\n",
        "  return "
      ],
      "metadata": {
        "id": "Tw5dk1aPec0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq3qnUI2v1Yr"
      },
      "source": [
        "# Putting everything together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Am4M1_dv1Yu"
      },
      "source": [
        "## Running the Per-series Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will run the analysis over each SeriesInstanceUID. "
      ],
      "metadata": {
        "id": "eMs5A5EGdpI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, series_id in enumerate(series_to_process_id_list):\n",
        "\n",
        "\n",
        "  #  -----------------\n",
        "  # init\n",
        "\n",
        "  start_total_nnunet = time.time()\n",
        "\n",
        "  # init every single time, as the most recent logs are loaded from the bucket\n",
        "  inference_time_dict_nnunet = dict()\n",
        "  total_time_dict_nnunet = dict()\n",
        "  inference_time_dict_bpr = dict()\n",
        "  total_time_dict_bpr = dict()\n",
        "\n",
        "  # set up processing flags? - discuss with Dennis \n",
        "\n",
        "  clear_output(wait = True)\n",
        "\n",
        "  print(\"(%g/%g) Processing series %s\"%(idx + 1, len(series_to_process_id_list), series_id))\n",
        "\n",
        "  # Get the dataframe of the series being analyzed \n",
        "  series_df = cohort_df[cohort_df[\"SeriesInstanceUID\"] == series_id]\n",
        "  num_instances = series_df['num_instances'].to_list()[0]\n",
        "\n",
        "  # Get the corresponding PatientId \n",
        "  patient_id = np.unique(series_df[series_df['Modality'] == \"CT\"][\"PatientID\"].values)\n",
        "  assert len(patient_id) == 1 # sanity check - each PatientID should be unique \n",
        "  patient_id = patient_id[0] \n",
        "  print('patient_id: ' + str(patient_id))\n",
        "\n",
        "  # Get the corresponding StudyInstanceUID\n",
        "  study_id = np.unique(series_df[series_df['Modality'] == \"CT\"][\"StudyInstanceUID\"].values)\n",
        "  assert len(study_id) == 1 # sanity check - each StudyInstanceUID should be unique\n",
        "  study_id = study_id[0] \n",
        "  print('study_id: ' + str(study_id))\n",
        "\n",
        "  dicomseg_fn = series_id + \"_SEG.dcm\"\n",
        "\n",
        "  input_nifti_fn = series_id + \"_0000.nii.gz\"\n",
        "  input_nifti_path = os.path.join(model_input_folder_nnunet, input_nifti_fn)\n",
        "\n",
        "  pred_nifti_fn = series_id + \".nii.gz\"\n",
        "  pred_nifti_path = os.path.join(model_output_folder_nnunet, pred_nifti_fn)\n",
        "\n",
        "  pred_softmax_folder_name = \"pred_softmax\"\n",
        "  pred_softmax_folder_path = os.path.join(processed_nrrd_path_nnunet, series_id, pred_softmax_folder_name)\n",
        "\n",
        "  pred_features_csv_fn = series_id + \".csv\"\n",
        "  pred_features_csv_path = os.path.join(features_csv_path_nnunet, pred_features_csv_fn)\n",
        "\n",
        "  sr_fn = series_id + '_SR.dcm'\n",
        "  sr_path = os.path.join(sr_path_nnunet, sr_fn)\n",
        "\n",
        "  sr_json_fn = series_id + '_SR.json'\n",
        "  sr_json_path = os.path.join(sr_path_nnunet, sr_json_fn)\n",
        "\n",
        "  # -----------------\n",
        "  # GS URI definition\n",
        "\n",
        "  # gs URI at which the *nii.gz object is or will be stored in the bucket\n",
        "  gs_uri_nifti_file = os.path.join(bucket_nifti_folder_uri_nnunet, pred_nifti_fn)\n",
        "\n",
        "  # gs URI at which the folder storing the *.nrrd softmax probabilities is or will be stored in the bucket\n",
        "  gs_uri_softmax_pred_folder = os.path.join(bucket_softmax_pred_folder_uri_nnunet, series_id)\n",
        "\n",
        "  # gs URI at which the DICOM SEG object is or will be stored in the bucket\n",
        "  gs_uri_dicomseg_file = os.path.join(bucket_dicomseg_folder_uri_nnunet, dicomseg_fn)\n",
        "\n",
        "  # DK added - gs URI at which the CT to nii file is or will be stored in the bucket \n",
        "  gs_uri_ct_nifti_file = os.path.join(bucket_dicomseg_folder_uri_nnunet, pred_nifti_fn)\n",
        "\n",
        "  # DK added - gs URI at which the features csv is saved if a 3d model is run \n",
        "  # gs_uri_features_csv_file = os.path.join(bucket_features_csv_folder_uri_nnunet, pred_features_csv_fn)\n",
        "  # gs URI at which the DICOM SR ojbect for the shape features is or will be stored in the bucket \n",
        "  gs_uri_sr_file = os.path.join(bucket_sr_folder_uri_nnunet, sr_fn)\n",
        "\n",
        "  # -----------------\n",
        "  # preprocessing\n",
        "\n",
        "  # Download the DICOM data \n",
        "  download_path = os.path.join(sorted_base_path, series_id) # should be deleted after bpr \n",
        "  if not os.path.exists(download_path):\n",
        "    start_time_download_series_data = time.time()\n",
        "    download_series_data_s5cmd(raw_base_path = raw_base_path, # --> ADD THIS TO GIT REPO. \n",
        "                               sorted_base_path = sorted_base_path,\n",
        "                               series_df = series_df,\n",
        "                               remove_raw = True)\n",
        "    elapsed_time_download_series_data = time.time()-start_time_download_series_data\n",
        "\n",
        "  # # DICOM CT to NIfTI - required for the processing\n",
        "  # start_time_ct_to_nii = time.time()\n",
        "  # preprocessing.pypla_dicom_ct_to_nifti(sorted_base_path = sorted_base_path,\n",
        "  #                                       processed_nifti_path = processed_nifti_path,\n",
        "  #                                       pat_id = series_id, \n",
        "  #                                       verbose = True)\n",
        "  # elapsed_time_ct_to_nii = time.time()-start_time_ct_to_nii \n",
        "\n",
        "  # DICOM CT to NifTI - required for processing \n",
        "  start_time_ct_to_nii = time.time()\n",
        "  success = dcm2niix_dicom_ct_to_nifti(sorted_base_path = sorted_base_path,\n",
        "                                       processed_nifti_path = processed_nifti_path,\n",
        "                                       pat_id = series_id)\n",
        "  elapsed_time_ct_to_nii = time.time()-start_time_ct_to_nii \n",
        "  if success == -1:\n",
        "    print(\"Cannot convert DICOM to NifTI using dcm2niix, either created no nii volumes or multiple volumes - stopping processing. \")\n",
        "    # create file in the log directory \n",
        "    dcm2nii_log_path = os.path.join(processed_base_path, 'dcm2nii_log.txt')\n",
        "    gs_uri_dcm2nii_log = os.path.join(bucket_log_folder_uri_nnunet, series_id + '_dcm2nii_log.txt')\n",
        "    with open(dcm2nii_log_path, 'w') as f:\n",
        "      f.write(\"Cannot convert DICOM to NifTI using dcm2niix, either created no nii volumes or multiple volumes - stopping processing. \")\n",
        "    f.close()\n",
        "    # !$s5cmd_path cp $dcm2nii_log_path $gs_uri_dcm2nii_log\n",
        "    !$s5cmd_path --endpoint-url https://storage.googleapis.com cp $dcm2nii_log_path $gs_uri_dcm2nii_log\n",
        "    continue \n",
        "\n",
        "  # upload nifti file to bucket \n",
        "  ct_nifti_path = os.path.join(processed_nifti_path,series_id,series_id+\"_CT.nii.gz\")\n",
        "  !$s5cmd_path --endpoint-url https://storage.googleapis.com cp $ct_nifti_path $gs_uri_ct_nifti_file\n",
        "\n",
        "\n",
        "  # prepare the `model_input` folder for the inference phase\n",
        "  preprocessing.prep_input_data(processed_nifti_path = processed_nifti_path,\n",
        "                                model_input_folder = model_input_folder_nnunet,\n",
        "                                pat_id = series_id)\n",
        "  \n",
        "  start_inference_nnunet = time.time()\n",
        "  # run the DL-based prediction\n",
        "  processing.process_patient_nnunet(model_input_folder = model_input_folder_nnunet,\n",
        "                                    model_output_folder = model_output_folder_nnunet, \n",
        "                                    nnunet_model = nnunet_model, \n",
        "                                    use_tta = use_tta,\n",
        "                                    export_prob_maps = export_prob_maps)\n",
        "  elapsed_inference_nnunet = time.time() - start_inference_nnunet\n",
        "\n",
        "  if export_prob_maps:\n",
        "    # convert the softmax predictions to NRRD files\n",
        "    postprocessing.numpy_to_nrrd(model_output_folder = model_output_folder_nnunet,\n",
        "                                processed_nrrd_path = processed_nrrd_path_nnunet,\n",
        "                                pat_id = series_id,\n",
        "                                output_folder_name = pred_softmax_folder_name)\n",
        "\n",
        "    # copy the nnU-Net *.npz softmax probabilities in the chosen bucket\n",
        "    !$s5cmd_path --endpoint-url https://storage.googleapis.com cp $pred_softmax_folder_path/ $gs_uri_softmax_pred_folder/\n",
        "\n",
        "  # copy the nnU-Net *.nii.gz binary masks in the chosen bucket --> Do we need this? \n",
        "  # !gsutil -m cp $pred_nifti_path $gs_uri_nifti_file\n",
        "  !$s5cmd_path --endpoint-url https://storage.googleapis.com cp $pred_nifti_path $gs_uri_nifti_file\n",
        "\n",
        "  # -----------------\n",
        "  # post-processing\n",
        "\n",
        "  # FIXME: consider removing this? (if only NIfTIs will be used to produce the DICOM SEGs)\n",
        "  # mandatory post-processing to convert the NIfTI file from the pipeline\n",
        "  # to a NRRD file (same content)\n",
        "  if not os.path.isdir(os.path.join(processed_nrrd_path_nnunet,series_id)):\n",
        "    os.mkdir(os.path.join(processed_nrrd_path_nnunet,series_id))\n",
        "  postprocessing.pypla_postprocess(processed_nrrd_path = processed_nrrd_path_nnunet,\n",
        "                                  model_output_folder = model_output_folder_nnunet,\n",
        "                                  pat_id = series_id)\n",
        "\n",
        "  # Modify the dicomseg_json file so that the SegmentAlgorithmName is representative of the model and other parameters \n",
        "  # Writes out the json file \n",
        "  SegmentAlgorithmName = experiment_folder_name \n",
        "  dicomseg_json_path_modified = \"/content/data/dicomseg_metadata_\" + SegmentAlgorithmName + '.json'\n",
        "  modify_dicomseg_json_file(dicomseg_json_path, dicomseg_json_path_modified, SegmentAlgorithmName)\n",
        "  # upload the json file \n",
        "  gs_uri_dicomseg_json_file = os.path.join(bucket_experiment_folder_uri_nnunet, 'dicomseg_metadata_' + SegmentAlgorithmName + '.json')\n",
        "  !$s5cmd_path --endpoint-url https://storage.googleapis.com cp $dicomseg_json_path_modified $gs_uri_dicomseg_json_file\n",
        "\n",
        "  # -----------------\n",
        "  # extract features if nnunet_model is 3d and save structured report \n",
        "  if ('3d' in nnunet_model):\n",
        "    seg_dcm = pydicom.dcmread(pred_dicomseg_path)\n",
        "    SOPInstanceUID_seg = seg_dcm.file_meta['0x0002', '0x0003'].value\n",
        "    dcm_directory = os.path.join(sorted_base_path, series_id, 'CT')\n",
        "    nnunet_features_metajson = save_structured_report_for_shape_features(series_id, \n",
        "                                                                         SOPInstanceUID_seg, \n",
        "                                                                         pred_dicomseg_path,  \n",
        "                                                                         dicomseg_json_path, \n",
        "                                                                         dcm_directory,\n",
        "                                                                         pred_nifti_path, \n",
        "                                                                         nnunet_base_path, \n",
        "                                                                         ct_nifti_path, \n",
        "                                                                         nnunet_segments_code_mapping_df,\n",
        "                                                                         nnunet_shape_features_code_mapping_df,\n",
        "                                                                         sr_json_path,\n",
        "                                                                         sr_path\n",
        "                                                                         )\n",
        "    # Copy SR to bucket \n",
        "    !$s5cmd_path --endpoint-url https://storage.googleapis.com cp $sr_path $gs_uri_sr_file"
      ],
      "metadata": {
        "id": "B1hg56TYdndS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RkKOcbyxs__u",
        "CZnoRi9Y7nEB",
        "VURToNwUW9Fm",
        "83S7J0xsyH_q",
        "rmAzW3AiN7nX",
        "_pVljPU1oOrO"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}