{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/Cloud-Resources-Workflows/blob/main/Notebooks/Totalsegmentator/downloadDicomAndConvertNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh8wdZYXnGby"
      },
      "source": [
        "#**This Notebook can download CT data from Imaging Data Commons and convert to NIfTI with dcm2niix**\n",
        "\n",
        "DICOM files are downloaded from IDC and converted to NIFTI files with dcm2niix. Whenever there are multiple NIFTI files for a series, such series are prohibited from continuing to Inference. A CSV file is created with a list of such series.\n",
        "\n",
        "Please cite:\n",
        "\n",
        "Li X, Morgan PS, Ashburner J, Smith J, Rorden C. (2016) The first step for neuroimaging data analysis: DICOM to NIfTI conversion. J Neurosci Methods. 264:47-56.\n",
        "\n",
        "Fedorov A, Longabaugh WJR, Pot D, Clunie DA, Pieper SD, Gibbs DL, Bridge C, Herrmann MD, Homeyer A, Lewis R, Aerts HJWL, Krishnaswamy D, Thiriveedhi VK, Ciausu C, Schacherer DP, Bontempi D, Pihl T, Wagner U, Farahani K, Kim E, Kikinis R. National Cancer Institute Imaging Data Commons: Toward Transparency, Reproducibility, and Scalability in Imaging Artificial Intelligence. Radiographics. 2023 Dec;43(12):e230180. doi: 10.1148/rg.230180. PMID: 37999984; PMCID: PMC10716669.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a_FPoOpH_I"
      },
      "source": [
        "##**Ways to utilize this notebook**\n",
        "\n",
        "\n",
        "*   **Colab**\n",
        "*   **DockerContainer/Terra/SB-CGC**\n",
        "\n",
        "\n",
        "####**Colab**\n",
        "*  This notebook was initally developed and tested on Colab, and a working version is saved on github, however reproducibility may not be guaranteed as the run time environment changes with colab updates\n",
        "*  To run this notebook with Colab, Click 'Open In Colab' icon on top left\n",
        "* Uncomment all the cells under \"Installing Packages\"\n",
        "* Provide a path to csv manifest containing SeriesInstanceUID and s5cmd download urls (specific to gcp buckets) under \"Parameters for Papermill\"\n",
        "* A sample manifest is provided for convenience can be downloaded by uncommenting and running the cells in \"For local testing\"\n",
        "* Run each cell to install the packages and to download the data from IDC, convert to NIfTI saved in lz4 compressed format\n",
        "\n",
        "####**Docker**\n",
        "* This notebook is saved by default in a way that's amenable to be used on Terra/SB-CGC platforms using Docker\n",
        "* Running this notebook in a docker container ensures reproduciblity, as we lock the run environment beginning from the base docker image to pip packages in the docker image\n",
        "\n",
        "* Docker images can be found @ https://hub.docker.com/repository/docker/imagingdatacommons/download_convert/tags\n",
        "* The link to dockerfile along with git commit hash used for building the docker image can be found in one of the layers called 'LABEL'\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ImagingDataCommons/Cloud-Resources-Workflows/main/images/download_convert_docker.png\"> \n",
        "\n",
        "* We use a python package called Papermill, that can run the notebook with out having to convert it to python script. This allows us maintain one copy of code instead of two.\n",
        "* To use papermill, download this notebook and tag the cell under 'Parameters for Papermill\" as parameters using jupyternotebook or jupyterlab as instructed @ https://papermill.readthedocs.io/en/latest/usage-parameterize.html#designate-parameters-for-a-cell\n",
        "* A sample papermill command is\n",
        "<pre>\n",
        "papermill -p csvFilePath path_to_csv_manifest downloadDicomAndConvertNotebook.ipynb outputdownloadDicomAndConvertNotebook.ipynb\n",
        "</pre>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVSZRlpTrXTe"
      },
      "source": [
        "###**Installing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT_MQVJ_NbZU"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "\n",
        "# #Install apt packages\n",
        "# !apt-get update \\\n",
        "#   && apt-get install -y --no-install-recommends \\\n",
        "#     dcm2niix\\\n",
        "#     lz4\\\n",
        "#     pigz\\\n",
        "#     #plastimatch\\\n",
        "#     wget\\\n",
        "#     zip\\\n",
        "#   && rm -rf /var/lib/apt/lists/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMi94BlAQrb5"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# #install s5cmd\n",
        "# !wget \"https://github.com/peak/s5cmd/releases/download/v2.2.2/s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "#   && tar -xvzf \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "#   && rm \"s5cmd_2.2.2_Linux-64bit.tar.gz\"\\\n",
        "#   && mv s5cmd /usr/local/bin/s5cmd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ETtcQleyLck"
      },
      "source": [
        "###**Importing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWLvDAwIQcg1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import glob\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from time import sleep\n",
        "from datetime import datetime\n",
        "import psutil\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcQfK2XB9wl5"
      },
      "source": [
        "###**Current Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpjsFVP79z-S",
        "outputId": "5f9d2b47-1c5f-4c69-c54c-bf6424ba0750"
      },
      "outputs": [],
      "source": [
        "curr_dir   = Path().absolute()\n",
        "\n",
        "print(time.asctime(time.localtime()))\n",
        "print(\"\\nCurrent directory :{}\".format( curr_dir))\n",
        "print(\"Python version    :\", sys.version.split('\\n')[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbgSoKrkjdPg"
      },
      "source": [
        "###**Parameters for papermill**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy6QqWR-jjdP",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "csvFilePath=''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u3YBlVdw0_i"
      },
      "source": [
        "###**For local testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPptwqXbw0TD"
      },
      "outputs": [],
      "source": [
        "# !wget -q https://raw.githubusercontent.com/ImagingDataCommons/Cloud-Resources-Workflows/main/sampleManifests/batch_1.csv\n",
        "# csvFilePath = glob.glob('*.csv')[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTenSEj6tada"
      },
      "source": [
        "###**Reading CSV File containing s5cmd Urls**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "LZMjMRsyQyCH",
        "outputId": "8f71bb3a-8f94-4dc3-c560-5f85b7fe7da2"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "cohort_df=pd.read_csv(csvFilePath, delimiter=',', encoding='utf-8')\n",
        "read_time=time.time() -start_time\n",
        "print('read in '+str(read_time)+ '  seconds')\n",
        "cohort_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDxZvqDlR5Cc",
        "outputId": "d64151ff-8ae7-4158-d8a6-ae7a97d748fd"
      },
      "outputs": [],
      "source": [
        "SeriesInstanceUIDs= cohort_df[\"SeriesInstanceUID\"].values.tolist()\n",
        "SeriesInstanceUIDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNhldQv7tEj-"
      },
      "source": [
        "###**Defining Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx_U7rRjtRYk"
      },
      "outputs": [],
      "source": [
        "#Creating Directories\n",
        "try:\n",
        "  shutil.rmtree('dcm2niix')\n",
        "except OSError:\n",
        "  pass\n",
        "os.mkdir('dcm2niix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFG0KcZ_QPyG"
      },
      "outputs": [],
      "source": [
        "def download_dicom_data(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Downloads raw DICOM data\n",
        "\n",
        "    Args:\n",
        "    series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "\n",
        "    # Attempt to remove the directory for the series if it exists\n",
        "    try:\n",
        "        shutil.rmtree(f\"{curr_dir}/idc_data/\")\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    # Access the global dataframe variable\n",
        "    global cohort_df\n",
        "\n",
        "    # Get the series data from the dataframe\n",
        "    aws_file_path = \"s5cmd_manifest.txt\"\n",
        "    series_df = cohort_df[cohort_df[\"SeriesInstanceUID\"] == series_id]\n",
        "\n",
        "    # Write the URLs to a file\n",
        "    series_df[\"s5cmdUrls\"].to_csv(aws_file_path, header=False, index=False)\n",
        "\n",
        "    # Remove double quotes from the manifest file\n",
        "    !sed -i 's/\"//g' s5cmd_manifest.txt\n",
        "\n",
        "    # Start a timer for the download\n",
        "    start_time = time.time()\n",
        "    print(\"Copying files from IDC buckets..\")\n",
        "\n",
        "    # Download the files and suppress output\n",
        "    !s5cmd --no-sign-request --endpoint-url https://s3.amazonaws.com run s5cmd_manifest.txt >> /dev/null\n",
        "\n",
        "    # Calculate and print elapsed time\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"Done in %g seconds.\" % elapsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0SPIZ5RTxDb"
      },
      "outputs": [],
      "source": [
        "def convert_dicom_to_nifti(series_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Converts a DICOM series to a NIfTI file.\n",
        "\n",
        "    Args:\n",
        "      series_id: The DICOM Tag SeriesInstanceUID of the DICOM series to be converted.\n",
        "    \"\"\"\n",
        "\n",
        "    # Attempt to remove the directory for the series if it exists\n",
        "    try:\n",
        "        shutil.rmtree(f\"dcm2niix/{series_id}\")\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    # Create a new directory for the series\n",
        "    os.mkdir(f\"dcm2niix/{series_id}\")\n",
        "\n",
        "    print(\"\\n Converting DICOM files to NIfTI \\n\")\n",
        "\n",
        "    # Run the appropriate converter command and capture the output\n",
        "\n",
        "    result = subprocess.run(\n",
        "        f\"dcm2niix -z y -f %j_%p_%t_%s -b n -m y -o {curr_dir}/dcm2niix/{series_id} {curr_dir}/idc_data/\",\n",
        "        shell=True,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "    )\n",
        "    print(result.stdout)\n",
        "    print(\"\\n Conversion successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKATbtgwPjX9"
      },
      "outputs": [],
      "source": [
        "def download_and_process_series(series_id: str) -> None:\n",
        "  \"\"\"Downloads and processes a DICOM series.\n",
        "\n",
        "  Args:\n",
        "    series_id: The identifier of the DICOM series to be processed.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a DataFrame to track the processing times.\n",
        "  log = pd.DataFrame({'SeriesInstanceUID': [series_id]})\n",
        "\n",
        "  # Start the timer for downloading the DICOM series.\n",
        "  start_time = time.time()\n",
        "  download_dicom_data(series_id)\n",
        "  download_time = time.time() - start_time\n",
        "\n",
        "  # Add the download time to the DataFrame.\n",
        "  log['download_time'] = download_time\n",
        "\n",
        "  # Start the timer for converting the DICOM series to NIfTI.\n",
        "  start_time = time.time()\n",
        "  convert_dicom_to_nifti(series_id)\n",
        "  convert_dicom_to_nifti_time = time.time() - start_time\n",
        "\n",
        "  # Add the conversion time to the DataFrame.\n",
        "  log['NiftiConverter_time'] = convert_dicom_to_nifti_time\n",
        "\n",
        "  # Update the global runtime statistics DataFrame.\n",
        "  global runtime_stats\n",
        "  runtime_stats = pd.concat([runtime_stats, log], ignore_index=True, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAQcXTtAGUfZ"
      },
      "outputs": [],
      "source": [
        "class MemoryMonitor:\n",
        "    def __init__(self):\n",
        "        # Flag to control the measurement loop\n",
        "        self.keep_measuring = True\n",
        "        # Get the path of the working disk\n",
        "        self.working_disk_path = self.get_working_disk_path()\n",
        "\n",
        "    def get_working_disk_path(self):\n",
        "        # This code is specific to Terra/SB-CGC as multiple disks are mounted on the platforms\n",
        "\n",
        "        # Get all disk partitions\n",
        "        partitions = psutil.disk_partitions()\n",
        "        for partition in partitions:\n",
        "            # If root partition, return root path\n",
        "            if partition.mountpoint == '/':\n",
        "                return '/'\n",
        "            # If cromwell_root is in mountpoint, return cromwell_root path\n",
        "            elif '/cromwell_root' in partition.mountpoint:\n",
        "                return '/cromwell_root'\n",
        "        # Default to root directory if no specific path is found\n",
        "        return '/'\n",
        "\n",
        "    def measure_usage(self):\n",
        "        # Initialize lists to store measurements\n",
        "        cpu_usage = []\n",
        "        ram_usage_mb = []\n",
        "        disk_usage_all = []\n",
        "        time_stamps = []\n",
        "\n",
        "        # Record start time\n",
        "        start_time = time.time()\n",
        "\n",
        "        while self.keep_measuring:\n",
        "            # Measure CPU usage\n",
        "            cpu = psutil.cpu_percent()\n",
        "\n",
        "            # Measure RAM usage\n",
        "            ram = psutil.virtual_memory()\n",
        "\n",
        "            # Measure disk usage\n",
        "            disk_usage = psutil.disk_usage(self.working_disk_path)\n",
        "\n",
        "            # Calculate used and total disk space in GB\n",
        "            disk_used = disk_usage.used / 1024 / 1024 / 1024\n",
        "            disk_total = disk_usage.total / 1024 / 1024 / 1024\n",
        "\n",
        "            # Calculate total and used RAM in MB\n",
        "            ram_total_mb = ram.total / 1024 / 1024\n",
        "            ram_mb = (ram.total - ram.available) / 1024 / 1024\n",
        "\n",
        "            # Append measurements to lists\n",
        "            cpu_usage.append(cpu)\n",
        "            ram_usage_mb.append(ram_mb)\n",
        "            disk_usage_all.append(disk_used)\n",
        "\n",
        "            # Record timestamp relative to start time\n",
        "            time_stamps.append(time.time() - start_time)\n",
        "\n",
        "            # Wait for a second before next measurement\n",
        "            sleep(1)\n",
        "\n",
        "        # Return all measurements and totals\n",
        "        return cpu_usage, ram_usage_mb, time_stamps, ram_total_mb, disk_usage_all, disk_total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYcb5F-2sFSO"
      },
      "source": [
        "###**Downloading and Converting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZTtOJ7CtQYkX",
        "outputId": "edcaa5f6-5471-41ad-ff2b-8d5d880938b3"
      },
      "outputs": [],
      "source": [
        "# Initialize a DataFrame to store runtime statistics\n",
        "runtime_stats = pd.DataFrame(columns=['SeriesInstanceUID','download_time',\n",
        "                                      'NiftiConverter_time', 'cpu_usage','ram_usage_mb', 'ram_total_mb', 'disk_usage_all', 'disk_total'\n",
        "                                      ])\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Loop over all series IDs\n",
        "    for series_id in SeriesInstanceUIDs:\n",
        "        # Create a ThreadPoolExecutor\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            # Initialize a MemoryMonitor instance\n",
        "            monitor = MemoryMonitor()\n",
        "            # Start a new thread to measure memory usage\n",
        "            mem_thread = executor.submit(monitor.measure_usage)\n",
        "            try:\n",
        "                # Start a new thread to download and process the series\n",
        "                proc_thread = executor.submit(download_and_process_series, series_id)\n",
        "                # Wait for the processing thread to finish\n",
        "                proc_thread.result()\n",
        "            finally:\n",
        "                # Stop the memory monitor thread\n",
        "                monitor.keep_measuring = False\n",
        "                # Get the results from the memory monitor thread\n",
        "                cpu_usage, ram_usage_mb, time_stamps, ram_total_mb, disk_usage_all, disk_total= mem_thread.result()\n",
        "\n",
        "                # Update the runtime statistics DataFrame with the results\n",
        "                cpu_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[cpu_idx, runtime_stats.columns.get_loc('cpu_usage')] = [[cpu_usage]]\n",
        "\n",
        "                ram_usage_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_usage_mb_idx, runtime_stats.columns.get_loc('ram_usage_mb')] = [[ram_usage_mb]]\n",
        "\n",
        "                ram_total_mb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[ram_total_mb_idx, runtime_stats.columns.get_loc('ram_total_mb')] = [[ram_total_mb]]\n",
        "\n",
        "                disk_usage_gb_idx = runtime_stats.index[runtime_stats['SeriesInstanceUID'] == series_id][0]\n",
        "                runtime_stats.iloc[disk_usage_gb_idx, runtime_stats.columns.get_loc('disk_usage_all')] = [[disk_usage_all]]\n",
        "\n",
        "                # Update total disk space for all rows (assuming it's the same for all series)\n",
        "                runtime_stats['disk_total']=disk_total\n",
        "\n",
        "                # Plot CPU usage, memory usage and disk usage over time\n",
        "                fig, ((ax1,ax2, ax3)) = plt.subplots(1,3, figsize=(12, 4))\n",
        "\n",
        "                ax1.plot(time_stamps, cpu_usage)\n",
        "                ax1.set_ylim(0, 100)\n",
        "                ax1.set_xlabel('Time (s)')\n",
        "                ax1.set_ylabel('CPU usage (%)')\n",
        "\n",
        "                ax2.plot(time_stamps, ram_usage_mb)\n",
        "                ax2.set_ylim(0, ram_total_mb)\n",
        "                ax2.set_xlabel('Time (s)')\n",
        "                ax2.set_ylabel('Memory usage (MB)')\n",
        "\n",
        "                ax3.plot(time_stamps, disk_usage_all)\n",
        "                ax3.set_ylim(0, disk_total)\n",
        "                ax3.set_xlabel('Time (s)')\n",
        "                ax3.set_ylabel('Disk usage (GB)')\n",
        "\n",
        "                plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su4z4ettsTaO"
      },
      "source": [
        "###**Monitoring for dcm2niix Errors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tba4tK6pvja9"
      },
      "outputs": [],
      "source": [
        "def check_dcm2niix_errors(path: str) -> None:\n",
        "    \"\"\"\n",
        "    Check for errors in the conversion of DICOM to NIfTI files.\n",
        "\n",
        "    Args:\n",
        "    path: The path to the directory containing the series directories.\n",
        "    \"\"\"\n",
        "    # Loop over all series directories in the path\n",
        "    for series_id in os.listdir(path):\n",
        "        series_id_path = os.path.join(path, series_id)\n",
        "\n",
        "        # Check if the path is a directory\n",
        "        if os.path.isdir(series_id_path):\n",
        "            # Count the number of files in the directory\n",
        "            num_files = len([f for f in os.listdir(series_id_path) if os.path.isfile(os.path.join(series_id_path, f))])\n",
        "\n",
        "            # If no files or more than one file found, log an error and remove the directory\n",
        "            if num_files == 0 or num_files > 1:\n",
        "                print(f'{\"No\" if num_files == 0 else \"More than one\"} NIfTI file{\"s\" if num_files > 1 else \"\"} found for {series_id}')\n",
        "\n",
        "                with open('dcm2niix_errors.csv', 'a') as csvfile:\n",
        "                    writer = csv.writer(csvfile)\n",
        "                    writer.writerow([series_id])\n",
        "\n",
        "                shutil.rmtree(os.path.join('dcm2niix', series_id))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP7_k4msvl2E"
      },
      "outputs": [],
      "source": [
        "check_dcm2niix_errors(f'/{curr_dir}/dcm2niix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-EDu3IFsxdq"
      },
      "source": [
        "###**Compressing Output Files**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q3J_d00v0gz",
        "outputId": "c3a9e42a-25d9-49b5-b922-75eb835039dc"
      },
      "outputs": [],
      "source": [
        "# Attempt to remove the archive file if it exists\n",
        "try:\n",
        "  os.remove('downloadDicomAndConvertNiftiFiles.tar.lz4')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "# Record the start time of the archiving process\n",
        "start_time = time.time()\n",
        "\n",
        "# Create a tar archive of the converterType directory, compress it with lz4, and save it as downloadDicomAndConvertNiftiFiles.tar.lz4\n",
        "!tar cvf - -C {curr_dir} dcm2niix | lz4 > downloadDicomAndConvertNiftiFiles.tar.lz4\n",
        "\n",
        "# Calculate and record the time taken for the archiving process\n",
        "archiving_time = time.time() - start_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVsFcPebsoro"
      },
      "source": [
        "###Utilization Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z9n17x4vhDQp",
        "outputId": "5b5c09f2-6ac1-4764-97a1-33f47d507418"
      },
      "outputs": [],
      "source": [
        "# Save the runtime statistics DataFrame to a CSV file\n",
        "runtime_stats.to_csv('runtime.csv')\n",
        "\n",
        "# Add the csv_read_time and archiving_time to the DataFrame as new columns\n",
        "runtime_stats['csv_read_time'] = read_time\n",
        "runtime_stats['archiving_time'] = archiving_time\n",
        "\n",
        "# Attempt to remove the lz4 file if it exists\n",
        "try:\n",
        "  os.remove('downloadDicomAndConvertUsageMetrics.lz4')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "# Compress the runtime.csv file using lz4 and save it as downloadDicomAndConvertUsageMetrics.lz4\n",
        "!lz4 {curr_dir}/runtime.csv downloadDicomAndConvertUsageMetrics.lz4\n",
        "\n",
        "# Print the runtime statistics DataFrame\n",
        "runtime_stats\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
