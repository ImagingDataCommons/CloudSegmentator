{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/Cloud-Resources-Workflows/blob/notebooks2/Notebooks/Totalsegmentator/PostProcessingTerra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6wtCVV3rGS7"
      },
      "source": [
        "##**This notebook can process the data generated by the twoVmWorkflowOnTerra and push to Google Cloud storage buckets.**\n",
        "\n",
        "The steps are:\n",
        "- The lz4 compressed DICOM SEG and SR files are downloaded from the Terra Workspace bucket\n",
        "- They are decompressed by lz4 and moved to a temporary directory\n",
        "- The uncompressed DICOM SEG and SR files are then pushed to Google Cloud Storage buckets, which can then be imported by DICOM store.\n",
        "- The process is iterated for each batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azyde9c3u5m6"
      },
      "source": [
        "##**Install lz4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QjxKglcr8Bd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt-get update \\\n",
        "  && apt-get install -y --no-install-recommends \\\n",
        "    lz4\\\n",
        "  && rm -rf /var/lib/apt/lists/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xqqgm_cuS46"
      },
      "source": [
        "##**Authenticate gcloud**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDVwFs1kujv-"
      },
      "outputs": [],
      "source": [
        "project_id='my-test-project'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftl5mKOSqe0N"
      },
      "outputs": [],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGRyvQDQqm6M"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project $project_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V06BwDUu_Wu"
      },
      "source": [
        "##**Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXuCtyZ3qTNA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pshMjPAvGUG"
      },
      "source": [
        "##**Load the terra table containing links to artifacts generated by the twoVmWorkflow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl6rWwzxrTZf"
      },
      "outputs": [],
      "source": [
        "data= pd.read_table('twoVM_2023_06_10_00_22.tsv')\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IA9rJllwkSJ"
      },
      "source": [
        "##**Decompress DICOM SEG files and push to Cloud Storage buckets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZXWK9jJ57WT",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dicom_seg_download_urls=data['dicomsegAndRadiomicsSR_CompressedFiles'].to_list()\n",
        "dicom_seg_download_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "796xToVj0hfQ"
      },
      "outputs": [],
      "source": [
        "destination_bucket_name='total_segmentator_nlst_sample_061023'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffGIRZ1V8wNO",
        "outputId": "2e371f41-ddb6-4158-ddfb-d34769b83ab7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for url in tqdm(dicom_seg_download_urls):\n",
        "    print('processing url:'+url)\n",
        "    try:\n",
        "      shutil.rmtree(f'dicom_seg_objects')\n",
        "      shutil.rmtree(f'itkimage2segimage')\n",
        "    except OSError:\n",
        "      pass\n",
        "    try:\n",
        "        os.mkdir(f'dicom_seg_objects')\n",
        "        !gsutil cp {url} . > /dev/null 2>&1\n",
        "        !lz4 -d --rm dicomsegAndRadiomicsSR_DICOMsegFiles.tar.lz4 -c | tar --strip-components=1  -xvf - > /dev/null 2>&1\n",
        "        !find ./itkimage2segimage -name '*.dcm.lz4' -exec mv -t dicom_seg_objects {} + > /dev/null 2>&1\n",
        "        !lz4 -d -m --rm \"dicom_seg_objects\"/*.lz4 > /dev/null 2>&1\n",
        "        !gsutil -m cp -r dicom_seg_objects/* gs://$destination_bucket_name/DICOM_SEGS/ > /dev/null 2>&1\n",
        "    except Exception as e:\n",
        "        print(f'Error processing {url}: {e}')\n",
        "        traceback.print_exc()\n",
        "shutil.rmtree(f'dicom_seg_objects')\n",
        "shutil.rmtree(f'itkimage2segimage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9BxxuL3y4pB"
      },
      "source": [
        "##**Decompress DICOM SR files and push to Cloud Storage buckets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-XxLXORrDya",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "sr_download_urls=data['structuredReportsDICOM'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV3a-LPbrDyb",
        "outputId": "7a32f37a-6abb-40c3-8b42-29efce91983f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "series_df = pd.DataFrame(columns=['series_id'])\n",
        "\n",
        "batch_count = 1  # Counter for batch folders\n",
        "\n",
        "for url in tqdm(sr_download_urls):\n",
        "    print('processing url:' + url)\n",
        "    try:\n",
        "        shutil.rmtree(f'structuredReportsDICOM')\n",
        "        shutil.rmtree(f'decompressedStructuredReportsDICOM')\n",
        "    except OSError:\n",
        "        pass\n",
        "    os.mkdir(f'decompressedStructuredReportsDICOM')\n",
        "    try:\n",
        "        !gsutil cp {url} . > /dev/null 2>&1\n",
        "        !lz4 -d --rm structuredReportsDICOM.tar.lz4 -c | tar --strip-components=1 -xvf - > /dev/null 2>&1\n",
        "        !find ./structuredReportsDICOM -name '*.dcm.lz4' -exec mv -t decompressedStructuredReportsDICOM {} + > /dev/null 2>&1\n",
        "        !lz4 -d -m --rm \"decompressedStructuredReportsDICOM\"/*.lz4 > /dev/null 2>&1\n",
        "        !gsutil -m cp -r decompressedStructuredReportsDICOM/* gs://$destination_bucket_name/decompressedStructuredReportsDICOM/batch_{batch_count}/ > /dev/null 2>&1\n",
        "\n",
        "        # Find all series IDs and add them to the DataFrame\n",
        "        series_ids = [filename.split('_')[0] for filename in os.listdir('decompressedStructuredReportsDICOM')]\n",
        "        url_series_df = pd.DataFrame({'series_id': series_ids})\n",
        "\n",
        "        # Append the current DataFrame to the main DataFrame\n",
        "        series_df = pd.concat([series_df, url_series_df], ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing {url}: {e}')\n",
        "        traceback.print_exc()\n",
        "\n",
        "#the below steps are for importing dicom files to dicom store from command line.\n",
        "    # try:\n",
        "    #     # Upload shape_sr.dcm files in the batch\n",
        "    #     gcs_uri = f\"gs://$destination_bucket_name/decompressedStructuredReportsDICOM/batch_{batch_count}/*.dcm\"\n",
        "    #     !gcloud healthcare dicom-stores import gcs 10k-series --dataset=total_segmentator_nlst_sample_061023 --location=us-central1 --gcs-uri={gcs_uri}\n",
        "    # except Exception as e:\n",
        "    #     print(f'Error processing {url}: {e}')\n",
        "    #     traceback.print_exc()\n",
        "\n",
        "    # Increment the batch counter\n",
        "    batch_count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYWOiib7rDyc"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(f'structuredReportsDICOM')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
